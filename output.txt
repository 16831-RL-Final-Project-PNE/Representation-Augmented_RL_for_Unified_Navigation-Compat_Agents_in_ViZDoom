Building RSSM...
Building Action Model...
Building Reward and Value...
Building Target Value Model...
Building Discount Model...
Building Observation Encoder and Decoder...


********** Iteration 0 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1278, -0.0881, -0.1327, -0.1156, -0.0976, -0.1256, -0.1161, -0.0892,
        -0.1413, -0.1346, -0.1326, -0.1226, -0.1468, -0.1427, -0.1261],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.1879, -0.1675, -0.2025, -0.1828, -0.1770, -0.2009, -0.1908, -0.1861,
        -0.2304, -0.2157, -0.2047, -0.1853, -0.1714, -0.1010], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.1879, -0.1675, -0.2025, -0.1828, -0.1770, -0.2009, -0.1908, -0.1861,
        -0.2304, -0.2157, -0.2047, -0.1853, -0.1714, -0.1010], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2018, -0.2014, -0.2295, -0.2269, -0.2163, -0.2440, -0.2457, -0.2469,
        -0.2228, -0.2454, -0.2251, -0.2497, -0.2182, -0.2233, -0.2425],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.6214, -0.6564, -0.6999, -0.6981, -0.7076, -0.7285, -0.7044, -0.6734,
        -0.6366, -0.6148, -0.5353, -0.4674, -0.3383, -0.1967], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.6214, -0.6564, -0.6999, -0.6981, -0.7076, -0.7285, -0.7044, -0.6734,
        -0.6366, -0.6148, -0.5353, -0.4674, -0.3383, -0.1967], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2700, -0.2728, -0.3024, -0.2840, -0.3008, -0.3062, -0.2899, -0.3211,
        -0.2755, -0.2679, -0.3065, -0.3146, -0.2966, -0.2990, -0.2799],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2123, -1.2375, -1.2777, -1.2544, -1.2461, -1.2088, -1.1610, -1.1108,
        -1.0007, -0.9218, -0.8493, -0.6968, -0.4927, -0.2587], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2123, -1.2375, -1.2777, -1.2544, -1.2461, -1.2088, -1.1610, -1.1108,
        -1.0007, -0.9218, -0.8493, -0.6968, -0.4927, -0.2587], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.0394, 0.0547, 0.0723, 0.0660, 0.0817, 0.0389, 0.0783, 0.0848, 0.0889,
        0.0390, 0.0683, 0.0450, 0.0642, 0.0773, 0.0858], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([0.4263, 0.4447, 0.4477, 0.4321, 0.4167, 0.3820, 0.3923, 0.3530, 0.3023,
        0.2363, 0.2201, 0.1645, 0.1308, 0.0686], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([0.4263, 0.4447, 0.4477, 0.4321, 0.4167, 0.3820, 0.3923, 0.3530, 0.3023,
        0.2363, 0.2201, 0.1645, 0.1308, 0.0686], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0749, 0.0807, 0.0868, 0.0822, 0.0863, 0.0831, 0.0854, 0.0804, 0.0826,
        0.0790, 0.0915, 0.0871], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([22, 29, 22, 32, 29, 27, 23, 23, 29, 15, 28, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_0.gif
Eval_AverageReturn : -159.10000610351562
Eval_StdReturn : 231.14300537109375
Eval_MaxReturn : 94.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 162.10000610351562
Train_AverageReturn : -235.5
Train_StdReturn : 211.8255157470703
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 4096.0
TimeSinceStart : 106.72561526298523
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 4.979601204395294
Loss_Value : 1.0238360464572906
Loss_Entropy : 2.483440101146698
Loss_Representation : 31.041366785764694
Loss_KL : 0.9418312311172485
Loss_Obs : 2.8641702942550182
Loss_Reward : 1.03863774985075
Loss_Discount : 0.419195543974638
Loss_RawKL : 0.557758167386055
mean_target : -0.43018903210759163
max_target : -0.06891448423266411
min_target : -0.6264747600071132
std_target : 0.16966876108199358
Done logging...

Current epsilon: 0.8061258102856943 at iteration 4096


********** Iteration 1 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.1829, 0.1460, 0.1407, 0.1847, 0.1431, 0.1804, 0.1660, 0.1590, 0.2173,
        0.2051, 0.1909, 0.1465, 0.2193, 0.1990, 0.1503], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([1.4303, 1.3705, 1.3475, 1.3267, 1.2531, 1.2202, 1.1425, 1.0704, 0.9953,
        0.8512, 0.7050, 0.5612, 0.4494, 0.2488], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([1.4303, 1.3705, 1.3475, 1.3267, 1.2531, 1.2202, 1.1425, 1.0704, 0.9953,
        0.8512, 0.7050, 0.5612, 0.4494, 0.2488], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.1822, 0.1872, 0.2512, 0.2160, 0.1949, 0.2130, 0.2262, 0.1855, 0.2505,
        0.2468, 0.2324, 0.1372, 0.2035, 0.1362, 0.1899], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([1.8895, 1.8362, 1.7793, 1.6470, 1.5358, 1.4422, 1.3218, 1.1769, 1.0640,
        0.8765, 0.6766, 0.4761, 0.3623, 0.1678], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([1.8895, 1.8362, 1.7793, 1.6470, 1.5358, 1.4422, 1.3218, 1.1769, 1.0640,
        0.8765, 0.6766, 0.4761, 0.3623, 0.1678], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.2253, 0.1793, 0.2905, 0.2062, 0.1836, 0.2139, 0.1899, 0.1592, 0.1404,
        0.1480, 0.1920, 0.1584, 0.1627, 0.1456, 0.2745], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([1.8476, 1.7283, 1.6548, 1.4568, 1.3341, 1.2280, 1.0784, 0.9474, 0.8413,
        0.7489, 0.6404, 0.4769, 0.3398, 0.1863], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([1.8476, 1.7283, 1.6548, 1.4568, 1.3341, 1.2280, 1.0784, 0.9474, 0.8413,
        0.7489, 0.6404, 0.4769, 0.3398, 0.1863], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.1593,  0.1342,  0.2044,  0.1808,  0.2076,  0.1038, -0.0286,  0.1752,
         0.1328,  0.2208,  0.1431,  0.1421,  0.1510,  0.1311,  0.1880],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([1.4467, 1.3694, 1.3114, 1.1761, 1.0570, 0.9023, 0.8477, 0.9334, 0.8080,
        0.7184, 0.5289, 0.4090, 0.2827, 0.1365], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([1.4467, 1.3694, 1.3114, 1.1761, 1.0570, 0.9023, 0.8477, 0.9334, 0.8080,
        0.7184, 0.5289, 0.4090, 0.2827, 0.1365], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0761, 0.0806, 0.0860, 0.0817, 0.0864, 0.0848, 0.0845, 0.0804, 0.0840,
        0.0787, 0.0911, 0.0857], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([25, 20, 26, 26, 22, 29, 26, 22, 32, 23, 24, 25]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_1.gif
Eval_AverageReturn : -57.70000076293945
Eval_StdReturn : 214.7305450439453
Eval_MaxReturn : 95.0
Eval_MinReturn : -385.0
Eval_AverageEpLen : 100.9000015258789
Train_AverageReturn : -274.27777099609375
Train_StdReturn : 193.63160705566406
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 8192.0
TimeSinceStart : 204.4992232322693
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : -14.106871366500854
Loss_Value : 1.9669502675533295
Loss_Entropy : 2.48364794254303
Loss_Representation : 75.20811340212822
Loss_KL : 1.13510200381279
Loss_Obs : 7.344873584806919
Loss_Reward : 0.5680477693676949
Loss_Discount : 0.05622611474245787
Loss_RawKL : 1.0681138336658478
mean_target : 0.9331242740154266
max_target : 1.5530688762664795
min_target : 0.12267665192484856
std_target : 0.45534686744213104
Done logging...

Current epsilon: 0.7226191070355269 at iteration 8192


********** Iteration 2 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.1441, 0.0922, 0.1349, 0.0826, 0.1356, 0.1318, 0.1326, 0.1304, 0.1714,
        0.1819, 0.0934, 0.1092, 0.1611, 0.1356, 0.0255], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([1.2725, 1.1961, 1.1730, 1.1017, 1.0822, 1.0080, 0.9302, 0.8470, 0.7618,
        0.6268, 0.4725, 0.4022, 0.3093, 0.1552], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([1.2725, 1.1961, 1.1730, 1.1017, 1.0822, 1.0080, 0.9302, 0.8470, 0.7618,
        0.6268, 0.4725, 0.4022, 0.3093, 0.1552], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.1089,  0.0449,  0.0654,  0.0692,  0.0711,  0.0742,  0.0068, -0.0035,
         0.0833,  0.0020,  0.0508,  0.0693,  0.1241,  0.0509,  0.0552],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([0.5969, 0.5143, 0.4964, 0.4560, 0.4106, 0.3592, 0.3006, 0.3106, 0.3329,
        0.2641, 0.2775, 0.2378, 0.1773, 0.0541], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([0.5969, 0.5143, 0.4964, 0.4560, 0.4106, 0.3592, 0.3006, 0.3106, 0.3329,
        0.2641, 0.2775, 0.2378, 0.1773, 0.0541], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0069,  0.0090, -0.0339,  0.0641,  0.0082,  0.0912, -0.0496,  0.0557,
         0.0568,  0.1043, -0.0144,  0.0480, -0.0214,  0.0033,  0.0045],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([0.2561, 0.2612, 0.2651, 0.3144, 0.2631, 0.2686, 0.1836, 0.2455, 0.2013,
        0.1507, 0.0478, 0.0658, 0.0170, 0.0398], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([0.2561, 0.2612, 0.2651, 0.3144, 0.2631, 0.2686, 0.1836, 0.2455, 0.2013,
        0.1507, 0.0478, 0.0658, 0.0170, 0.0398], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0557, -0.0610,  0.0056, -0.1220, -0.1018, -0.0302,  0.0078, -0.0821,
        -0.1295, -0.0677, -0.0184,  0.0268,  0.0083,  0.0148, -0.0435],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.4647, -0.4326, -0.3950, -0.4260, -0.3241, -0.2388, -0.2236, -0.2446,
        -0.1727, -0.0467,  0.0193,  0.0412,  0.0141,  0.0051], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.4647, -0.4326, -0.3950, -0.4260, -0.3241, -0.2388, -0.2236, -0.2446,
        -0.1727, -0.0467,  0.0193,  0.0412,  0.0141,  0.0051], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0770, 0.0820, 0.0877, 0.0811, 0.0863, 0.0842, 0.0820, 0.0814, 0.0841,
        0.0781, 0.0899, 0.0861], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([20, 28, 29, 31, 25, 38, 28, 25, 19, 23, 23, 11]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_2.gif
Eval_AverageReturn : -247.5
Eval_StdReturn : 217.80691528320312
Eval_MaxReturn : 94.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 214.3000030517578
Train_AverageReturn : -218.2857208251953
Train_StdReturn : 217.78497314453125
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 12288.0
TimeSinceStart : 318.58384680747986
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : -3.2832919359207153
Loss_Value : 0.7669854760169983
Loss_Entropy : 2.483777701854706
Loss_Representation : 1.9710439890623093
Loss_KL : 1.4317809045314789
Loss_Obs : -0.02693926077336073
Loss_Reward : 0.7742501199245453
Loss_Discount : 0.03440536977723241
Loss_RawKL : 1.4010542035102844
mean_target : 0.16000749729573727
max_target : 0.4215660057961941
min_target : -0.15355849592015147
std_target : 0.18703263625502586
Done logging...

Current epsilon: 0.6483349027304446 at iteration 12288


********** Iteration 3 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1527, -0.1771, -0.1777, -0.1455, -0.1661, -0.1163, -0.1487, -0.1960,
        -0.1247, -0.0927, -0.1424, -0.0802, -0.1678, -0.1356, -0.1074],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4456, -1.3693, -1.2626, -1.1472, -1.0605, -0.9478, -0.8801, -0.7753,
        -0.6143, -0.5192, -0.4547, -0.3297, -0.2636, -0.1022], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4456, -1.3693, -1.2626, -1.1472, -1.0605, -0.9478, -0.8801, -0.7753,
        -0.6143, -0.5192, -0.4547, -0.3297, -0.2636, -0.1022], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2206, -0.1822, -0.2456, -0.1664, -0.1247, -0.2124, -0.1579, -0.1684,
        -0.1873, -0.2304, -0.1909, -0.2403, -0.1508, -0.1891, -0.1496],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8798, -1.7551, -1.6654, -1.5036, -1.4144, -1.3660, -1.2222, -1.1264,
        -1.0154, -0.8751, -0.6821, -0.5199, -0.2980, -0.1561], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8798, -1.7551, -1.6654, -1.5036, -1.4144, -1.3660, -1.2222, -1.1264,
        -1.0154, -0.8751, -0.6821, -0.5199, -0.2980, -0.1561], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2316, -0.1934, -0.1968, -0.1763, -0.1547, -0.1722, -0.1794, -0.2037,
        -0.2068, -0.2646, -0.1822, -0.1784, -0.2380, -0.1389, -0.1467],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9187, -1.7851, -1.6862, -1.5769, -1.4819, -1.4057, -1.3081, -1.1943,
        -1.0498, -0.8923, -0.6646, -0.5123, -0.3563, -0.1270], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9187, -1.7851, -1.6862, -1.5769, -1.4819, -1.4057, -1.3081, -1.1943,
        -1.0498, -0.8923, -0.6646, -0.5123, -0.3563, -0.1270], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1521, -0.1127, -0.2465, -0.2005, -0.2165, -0.1560, -0.1111, -0.1814,
        -0.1414, -0.1878, -0.1543, -0.2348, -0.1820, -0.1938, -0.1681],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7298, -1.6685, -1.6457, -1.4791, -1.3530, -1.2039, -1.1101, -1.0592,
        -0.9304, -0.8348, -0.6865, -0.5632, -0.3475, -0.1759], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7298, -1.6685, -1.6457, -1.4791, -1.3530, -1.2039, -1.1101, -1.0592,
        -0.9304, -0.8348, -0.6865, -0.5632, -0.3475, -0.1759], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0767, 0.0827, 0.0905, 0.0805, 0.0866, 0.0830, 0.0806, 0.0829, 0.0823,
        0.0774, 0.0896, 0.0870], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([19, 32, 27, 32, 17, 21, 24, 23, 29, 21, 30, 25]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_3.gif
Eval_AverageReturn : -341.20001220703125
Eval_StdReturn : 144.77554321289062
Eval_MaxReturn : 93.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 270.79998779296875
Train_AverageReturn : -219.23809814453125
Train_StdReturn : 219.7899169921875
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 16384.0
TimeSinceStart : 441.32462978363037
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 15.22697401046753
Loss_Value : 2.2961860299110413
Loss_Entropy : 2.4834752082824707
Loss_Representation : 0.41172945499420166
Loss_KL : 2.0529058277606964
Loss_Obs : -0.24148060008883476
Loss_Reward : 0.7252028286457062
Loss_Discount : 0.048426754772663116
Loss_RawKL : 2.0529057532548904
mean_target : -1.1621453315019608
max_target : -0.24433834105730057
min_target : -1.8962211906909943
std_target : 0.5305217653512955
Done logging...

Current epsilon: 0.5822546625285524 at iteration 16384


********** Iteration 4 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2175, -0.1019, -0.2643, -0.2064, -0.1759, -0.2241, -0.1978, -0.1295,
        -0.2217, -0.1500, -0.1657, -0.1118, -0.2353, -0.0999, -0.1127],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8034, -1.6773, -1.6672, -1.4854, -1.3538, -1.2477, -1.0854, -0.9400,
        -0.8597, -0.6773, -0.5577, -0.4162, -0.3217, -0.0899], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8034, -1.6773, -1.6672, -1.4854, -1.3538, -1.2477, -1.0854, -0.9400,
        -0.8597, -0.6773, -0.5577, -0.4162, -0.3217, -0.0899], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0515, -0.1809, -0.0646, -0.1247, -0.0707, -0.0135, -0.0881, -0.0803,
        -0.0211,  0.0703, -0.1642, -0.0397, -0.1739, -0.1450, -0.1863],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7954, -0.7896, -0.6427, -0.6129, -0.5183, -0.4752, -0.4900, -0.4262,
        -0.3675, -0.3684, -0.4653, -0.3173, -0.2951, -0.1294], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7954, -0.7896, -0.6427, -0.6129, -0.5183, -0.4752, -0.4900, -0.4262,
        -0.3675, -0.3684, -0.4653, -0.3173, -0.2951, -0.1294], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0475,  0.0242, -0.0826,  0.0223,  0.0881, -0.0984,  0.0416, -0.0529,
         0.0313,  0.1081,  0.0348, -0.0030,  0.0452,  0.0364,  0.0353],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([0.0747, 0.1268, 0.1052, 0.1973, 0.1853, 0.1015, 0.2106, 0.1780, 0.2431,
        0.2231, 0.1205, 0.0893, 0.0975, 0.0532], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([0.0747, 0.1268, 0.1052, 0.1973, 0.1853, 0.1015, 0.2106, 0.1780, 0.2431,
        0.2231, 0.1205, 0.0893, 0.0975, 0.0532], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0284, -0.0599,  0.1053,  0.0356,  0.0212,  0.0973,  0.0421, -0.0349,
         0.0866,  0.0053,  0.1768,  0.0091, -0.0562,  0.0571,  0.0930],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([ 0.3544,  0.3437,  0.4270,  0.3388,  0.3198,  0.3152,  0.2290,  0.1981,
         0.2454,  0.1656,  0.1685, -0.0089, -0.0195,  0.0365], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([ 0.3544,  0.3437,  0.4270,  0.3388,  0.3198,  0.3152,  0.2290,  0.1981,
         0.2454,  0.1656,  0.1685, -0.0089, -0.0195,  0.0365], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0764, 0.0823, 0.0918, 0.0805, 0.0857, 0.0822, 0.0811, 0.0849, 0.0794,
        0.0765, 0.0915, 0.0878], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([22, 34, 32, 16, 24, 14, 28, 25, 30, 23, 31, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_4.gif
Eval_AverageReturn : -221.89999389648438
Eval_StdReturn : 203.56004333496094
Eval_MaxReturn : 55.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 203.3000030517578
Train_AverageReturn : -117.46666717529297
Train_StdReturn : 227.40928649902344
Train_MaxReturn : 95.0
Train_MinReturn : -390.0
Train_AverageEpLen : 136.53334045410156
Train_EnvstepsSoFar : 20480.0
TimeSinceStart : 554.5984621047974
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 3.759550556540489
Loss_Value : 0.9211611896753311
Loss_Entropy : 2.4829996824264526
Loss_Representation : -3.1467835307121277
Loss_KL : 0.942954957485199
Loss_Obs : -0.5185142457485199
Loss_Reward : 1.010670930147171
Loss_Discount : 0.0847330754622817
Loss_RawKL : 0.855907678604126
mean_target : -0.3430292713455856
max_target : -0.015433527936693281
min_target : -0.6305994167923927
std_target : 0.1977733364328742
Done logging...

Current epsilon: 0.523472338803224 at iteration 20480


********** Iteration 5 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0201,  0.0474, -0.0424, -0.0738,  0.0217,  0.0137, -0.0019, -0.0117,
        -0.0347, -0.0542, -0.0348, -0.0271,  0.0939, -0.0356,  0.0336],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.0911, -0.0746, -0.1310, -0.0964, -0.0238, -0.0494, -0.0687, -0.0731,
        -0.0664, -0.0358,  0.0184,  0.0555,  0.0867, -0.0092], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.0911, -0.0746, -0.1310, -0.0964, -0.0238, -0.0494, -0.0687, -0.0731,
        -0.0664, -0.0358,  0.0184,  0.0555,  0.0867, -0.0092], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1019,  0.0703, -0.0317, -0.0511, -0.0335,  0.0178, -0.0448, -0.0152,
         0.0918, -0.0315, -0.1235,  0.0242,  0.0720,  0.0172, -0.0722],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.1186, -0.0189, -0.0954, -0.0683, -0.0179,  0.0160, -0.0037,  0.0410,
         0.0575, -0.0380, -0.0091,  0.1197,  0.0983,  0.0267], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.1186, -0.0189, -0.0954, -0.0683, -0.0179,  0.0160, -0.0037,  0.0410,
         0.0575, -0.0380, -0.0091,  0.1197,  0.0983,  0.0267], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-2.9064e-02, -9.0866e-02,  4.2287e-02, -9.4473e-06,  1.1389e-02,
        -1.7349e-02, -6.9067e-02,  3.3074e-02,  8.5493e-02, -3.5355e-03,
         4.9259e-03, -4.6114e-02, -1.9347e-02, -6.2007e-02, -1.3735e-02],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.0897, -0.0653,  0.0264, -0.0155, -0.0160, -0.0309, -0.0139,  0.0597,
         0.0284, -0.0621, -0.0658, -0.0760, -0.0333, -0.0160], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.0897, -0.0653,  0.0264, -0.0155, -0.0160, -0.0309, -0.0139,  0.0597,
         0.0284, -0.0621, -0.0658, -0.0760, -0.0333, -0.0160], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1586, -0.0884, -0.0762, -0.1225, -0.0816, -0.1078, -0.1113, -0.1663,
        -0.1247, -0.1531, -0.1303, -0.1295, -0.2255, -0.0909, -0.0959],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2073, -1.1112, -1.0830, -1.0671, -1.0013, -0.9736, -0.9181, -0.8567,
        -0.7312, -0.6437, -0.5198, -0.4135, -0.3025, -0.0832], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2073, -1.1112, -1.0830, -1.0671, -1.0013, -0.9736, -0.9181, -0.8567,
        -0.7312, -0.6437, -0.5198, -0.4135, -0.3025, -0.0832], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0762, 0.0812, 0.0901, 0.0805, 0.0843, 0.0831, 0.0839, 0.0874, 0.0758,
        0.0760, 0.0938, 0.0878], device='cuda:0')
Count of actions: (array([ 0,  2,  3,  4,  5,  6,  7,  9, 11]), array([1, 3, 2, 1, 5, 1, 1, 4, 2]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_5.gif
Eval_AverageReturn : -116.69999694824219
Eval_StdReturn : 224.02992248535156
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 137.3000030517578
Train_AverageReturn : -271.77777099609375
Train_StdReturn : 189.47604370117188
Train_MaxReturn : 94.0
Train_MinReturn : -395.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 24576.0
TimeSinceStart : 651.7448608875275
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 3.9049467742443085
Loss_Value : 0.7407056540250778
Loss_Entropy : 2.482524037361145
Loss_Representation : 1.5939037799835205
Loss_KL : 1.0499985814094543
Loss_Obs : -0.009950384497642517
Loss_Reward : 0.5816648453474045
Loss_Discount : 0.06174417119473219
Loss_RawKL : 0.9426190704107285
mean_target : -0.3534004194661975
max_target : -0.09826138010248542
min_target : -0.5607875436544418
std_target : 0.14850696013309062
Done logging...

Current epsilon: 0.4711819480299418 at iteration 24576


********** Iteration 6 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1105, -0.2360, -0.2529, -0.1755, -0.2541, -0.1631, -0.2689, -0.2544,
        -0.1477, -0.2554, -0.2488, -0.2287, -0.2089, -0.2390, -0.0979],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.1111, -2.1177, -1.9923, -1.8434, -1.7659, -1.6019, -1.5233, -1.3294,
        -1.1380, -1.0476, -0.8390, -0.6259, -0.4213, -0.2244], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.1111, -2.1177, -1.9923, -1.8434, -1.7659, -1.6019, -1.5233, -1.3294,
        -1.1380, -1.0476, -0.8390, -0.6259, -0.4213, -0.2244], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0697, -0.2069, -0.1106, -0.0484, -0.1688, -0.0564, -0.0604,  0.0609,
        -0.1219, -0.0845, -0.2621, -0.0698, -0.1308, -0.1949, -0.1822],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.0523, -1.0425, -0.8839, -0.8186, -0.8164, -0.6856, -0.6668, -0.6441,
        -0.7449, -0.6612, -0.6120, -0.3709, -0.3200, -0.2030], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.0523, -1.0425, -0.8839, -0.8186, -0.8164, -0.6856, -0.6668, -0.6441,
        -0.7449, -0.6612, -0.6120, -0.3709, -0.3200, -0.2030], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0220, -0.2388, -0.1883, -0.1331, -0.0752, -0.1825, -0.1627, -0.1670,
        -0.0918, -0.1834, -0.0026,  0.0058,  0.0274, -0.1567, -0.1056],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.1774, -1.2237, -1.0421, -0.9046, -0.8173, -0.7857, -0.6401, -0.5070,
        -0.3630, -0.2890, -0.1121, -0.1167, -0.1321, -0.1716], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.1774, -1.2237, -1.0421, -0.9046, -0.8173, -0.7857, -0.6401, -0.5070,
        -0.3630, -0.2890, -0.1121, -0.1167, -0.1321, -0.1716], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0081, -0.0564, -0.0495, -0.1159, -0.0801, -0.0340, -0.0417, -0.0650,
        -0.1837, -0.1368, -0.1252, -0.0676, -0.0126, -0.0928, -0.0090],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7063, -0.7379, -0.7210, -0.7096, -0.6294, -0.5836, -0.5829, -0.5755,
        -0.5406, -0.3780, -0.2558, -0.1400, -0.0769, -0.0696], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7063, -0.7379, -0.7210, -0.7096, -0.6294, -0.5836, -0.5829, -0.5755,
        -0.5406, -0.3780, -0.2558, -0.1400, -0.0769, -0.0696], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0761, 0.0798, 0.0867, 0.0805, 0.0823, 0.0852, 0.0875, 0.0897, 0.0729,
        0.0761, 0.0962, 0.0869], device='cuda:0')
Count of actions: (array([1, 2, 4, 5, 6]), array([1, 1, 1, 1, 2]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_6.gif
Eval_AverageReturn : -198.5
Eval_StdReturn : 235.62989807128906
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 183.89999389648438
Train_AverageReturn : -253.2105255126953
Train_StdReturn : 206.21592712402344
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 28672.0
TimeSinceStart : 756.2141132354736
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 10.184205293655396
Loss_Value : 1.3022744208574295
Loss_Entropy : 2.481818199157715
Loss_Representation : -5.621915221214294
Loss_KL : 0.9045364260673523
Loss_Obs : -0.7009501159191132
Loss_Reward : 0.4332037605345249
Loss_Discount : 0.04984565172344446
Loss_RawKL : 0.6584611460566521
mean_target : -0.8018978983163834
max_target : -0.18387269601225853
min_target : -1.287765771150589
std_target : 0.35593414679169655
Done logging...

Current epsilon: 0.42466651968452584 at iteration 28672


********** Iteration 7 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1410, -0.1684, -0.1929, -0.1716, -0.1388, -0.1605, -0.1000, -0.1320,
        -0.2322, -0.1556, -0.1170, -0.1343, -0.1520, -0.1340, -0.1763],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5143, -1.4535, -1.3600, -1.2363, -1.1264, -1.0461, -0.9405, -0.8900,
        -0.8016, -0.6034, -0.4734, -0.3784, -0.2598, -0.1159], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5143, -1.4535, -1.3600, -1.2363, -1.1264, -1.0461, -0.9405, -0.8900,
        -0.8016, -0.6034, -0.4734, -0.3784, -0.2598, -0.1159], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0910, -0.1245, -0.0704, -0.1135, -0.0699, -0.1113, -0.0328, -0.0114,
        -0.0296, -0.0608, -0.0246, -0.1198, -0.0960, -0.0744, -0.1011],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7400, -0.6869, -0.5973, -0.5593, -0.4717, -0.4264, -0.3354, -0.3210,
        -0.3271, -0.3177, -0.2729, -0.2655, -0.1558, -0.0653], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7400, -0.6869, -0.5973, -0.5593, -0.4717, -0.4264, -0.3354, -0.3210,
        -0.3271, -0.3177, -0.2729, -0.2655, -0.1558, -0.0653], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.3798, -0.4696, -0.4566, -0.4232, -0.4569, -0.4787, -0.4370, -0.4633,
        -0.4646, -0.4971, -0.4523, -0.4583, -0.4186, -0.4710, -0.4171],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.4655, -4.3245, -4.0802, -3.8339, -3.6095, -3.3368, -3.0265, -2.7419,
        -2.4116, -2.0605, -1.6537, -1.2735, -0.8649, -0.4722], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.4655, -4.3245, -4.0802, -3.8339, -3.6095, -3.3368, -3.0265, -2.7419,
        -2.4116, -2.0605, -1.6537, -1.2735, -0.8649, -0.4722], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1407, -0.1371, -0.1281, -0.1773, -0.1850, -0.1872, -0.1410, -0.1450,
        -0.1651, -0.1651, -0.1767, -0.1343, -0.1712, -0.1512, -0.1898],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5429, -1.4833, -1.4260, -1.3758, -1.2675, -1.1474, -1.0168, -0.9270,
        -0.8296, -0.7043, -0.5718, -0.4187, -0.3017, -0.1383], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5429, -1.4833, -1.4260, -1.3758, -1.2675, -1.1474, -1.0168, -0.9270,
        -0.8296, -0.7043, -0.5718, -0.4187, -0.3017, -0.1383], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0761, 0.0781, 0.0830, 0.0808, 0.0801, 0.0880, 0.0910, 0.0918, 0.0706,
        0.0768, 0.0985, 0.0853], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([23, 14, 26, 34, 22, 25, 27, 27, 29, 28, 25, 20]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_7.gif
Eval_AverageReturn : -296.0
Eval_StdReturn : 190.55235290527344
Eval_MaxReturn : 94.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 242.6999969482422
Train_AverageReturn : -294.5882263183594
Train_StdReturn : 184.30093383789062
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 32768.0
TimeSinceStart : 874.5921852588654
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 17.93960201740265
Loss_Value : 3.890758454799652
Loss_Entropy : 2.480643928050995
Loss_Representation : -6.780556082725525
Loss_KL : 0.8999999761581421
Loss_Obs : -0.8162806034088135
Loss_Reward : 0.45489312056452036
Loss_Discount : 0.027356928680092096
Loss_RawKL : 0.4648096635937691
mean_target : -1.355819657444954
max_target : -0.279768418520689
min_target : -2.2157838344573975
std_target : 0.6218145601451397
Done logging...

Current epsilon: 0.38328826562750956 at iteration 32768


********** Iteration 8 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.1251, 0.0808, 0.0722, 0.1038, 0.0694, 0.0836, 0.1344, 0.0930, 0.0828,
        0.1074, 0.1058, 0.0938, 0.0875, 0.0845, 0.0858], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([0.9353, 0.8567, 0.8209, 0.7912, 0.7276, 0.6957, 0.6466, 0.5415, 0.4751,
        0.4148, 0.3249, 0.2316, 0.1465, 0.0630], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([0.9353, 0.8567, 0.8209, 0.7912, 0.7276, 0.6957, 0.6466, 0.5415, 0.4751,
        0.4148, 0.3249, 0.2316, 0.1465, 0.0630], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.0332, 0.0752, 0.0604, 0.0748, 0.0545, 0.0344, 0.0605, 0.0270, 0.0613,
        0.0460, 0.0738, 0.0726, 0.0783, 0.0522, 0.0361], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([0.5771, 0.5742, 0.5268, 0.4930, 0.4440, 0.4111, 0.3988, 0.3583, 0.3522,
        0.3046, 0.2742, 0.2093, 0.1458, 0.0710], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([0.5771, 0.5742, 0.5268, 0.4930, 0.4440, 0.4111, 0.3988, 0.3583, 0.3522,
        0.3046, 0.2742, 0.2093, 0.1458, 0.0710], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0234,  0.0145,  0.0076,  0.0021, -0.0200, -0.0164,  0.0187,  0.0260,
         0.0333, -0.0041, -0.0101, -0.0266,  0.0114,  0.0227, -0.0042],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([0.0704, 0.0506, 0.0366, 0.0323, 0.0324, 0.0556, 0.0766, 0.0617, 0.0367,
        0.0051, 0.0114, 0.0241, 0.0513, 0.0412], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([0.0704, 0.0506, 0.0366, 0.0323, 0.0324, 0.0556, 0.0766, 0.0617, 0.0367,
        0.0051, 0.0114, 0.0241, 0.0513, 0.0412], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0223, -0.0256, -0.0001, -0.0346, -0.0746, -0.0522, -0.0660, -0.0329,
        -0.0263, -0.0537, -0.0205, -0.0113, -0.0768, -0.0696, -0.0439],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.3648, -0.3643, -0.3591, -0.3816, -0.3687, -0.3126, -0.2756, -0.2216,
        -0.1993, -0.1817, -0.1347, -0.1209, -0.1156, -0.0412], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.3648, -0.3643, -0.3591, -0.3816, -0.3687, -0.3126, -0.2756, -0.2216,
        -0.1993, -0.1817, -0.1347, -0.1209, -0.1156, -0.0412], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0758, 0.0768, 0.0808, 0.0813, 0.0785, 0.0900, 0.0923, 0.0930, 0.0697,
        0.0779, 0.1001, 0.0837], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([21, 18, 21, 18, 31, 22, 29, 28, 20, 26, 33, 33]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_8.gif
Eval_AverageReturn : -206.0
Eval_StdReturn : 222.216552734375
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 191.89999389648438
Train_AverageReturn : -251.36842346191406
Train_StdReturn : 203.92367553710938
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 36864.0
TimeSinceStart : 985.5627868175507
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : -2.2063064724206924
Loss_Value : 0.6907448917627335
Loss_Entropy : 2.4796955585479736
Loss_Representation : -6.4905513525009155
Loss_KL : 1.0166726410388947
Loss_Obs : -0.8254933208227158
Loss_Reward : 0.7072085589170456
Loss_Discount : 0.0405005794018507
Loss_RawKL : 0.898100882768631
mean_target : 0.08320220652967691
max_target : 0.2978602049406618
min_target : -0.15758837317116559
std_target : 0.14608723483979702
Done logging...

Current epsilon: 0.34647983518389924 at iteration 36864


********** Iteration 9 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1065, -0.0434, -0.0946, -0.0607, -0.0828, -0.1403, -0.1220, -0.1538,
        -0.0888, -0.0952, -0.0794, -0.1003, -0.1066, -0.0963, -0.1194],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9606, -0.9056, -0.9122, -0.8665, -0.8541, -0.8186, -0.7175, -0.6326,
        -0.5067, -0.4434, -0.3659, -0.3041, -0.2179, -0.1181], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9606, -0.9056, -0.9122, -0.8665, -0.8541, -0.8186, -0.7175, -0.6326,
        -0.5067, -0.4434, -0.3659, -0.3041, -0.2179, -0.1181], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1689, -0.1419, -0.1283, -0.0884, -0.1297, -0.1354, -0.0813, -0.1528,
        -0.0963, -0.1332, -0.1267, -0.1546, -0.1229, -0.0987, -0.0712],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2478, -1.1446, -1.0640, -0.9928, -0.9589, -0.8786, -0.7900, -0.7524,
        -0.6369, -0.5747, -0.4673, -0.3595, -0.2171, -0.1013], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2478, -1.1446, -1.0640, -0.9928, -0.9589, -0.8786, -0.7900, -0.7524,
        -0.6369, -0.5747, -0.4673, -0.3595, -0.2171, -0.1013], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0823, -0.1738, -0.0555, -0.0899, -0.1460, -0.2289, -0.1325, -0.0971,
        -0.1679, -0.1286, -0.1123, -0.1400, -0.1894, -0.2037, -0.1087],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.3149, -1.3080, -1.2032, -1.2172, -1.1944, -1.1135, -0.9402, -0.8568,
        -0.8068, -0.6801, -0.5839, -0.5009, -0.3858, -0.2088], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.3149, -1.3080, -1.2032, -1.2172, -1.1944, -1.1135, -0.9402, -0.8568,
        -0.8068, -0.6801, -0.5839, -0.5009, -0.3858, -0.2088], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1496, -0.1060, -0.1331, -0.2048, -0.1829, -0.1595, -0.1679, -0.1968,
        -0.1834, -0.1775, -0.1818, -0.1237, -0.1163, -0.1779, -0.1847],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5499, -1.4908, -1.4736, -1.4213, -1.2944, -1.1803, -1.0846, -0.9726,
        -0.8216, -0.6778, -0.5300, -0.3670, -0.2585, -0.1509], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5499, -1.4908, -1.4736, -1.4213, -1.2944, -1.1803, -1.0846, -0.9726,
        -0.8216, -0.6778, -0.5300, -0.3670, -0.2585, -0.1509], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0758, 0.0759, 0.0794, 0.0814, 0.0767, 0.0912, 0.0932, 0.0945, 0.0689,
        0.0791, 0.1013, 0.0826], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([22, 17, 24, 19, 19, 35, 21, 27, 28, 28, 35, 25]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_9.gif
Eval_AverageReturn : -152.6999969482422
Eval_StdReturn : 233.61679077148438
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 158.6999969482422
Train_AverageReturn : -273.1666564941406
Train_StdReturn : 198.037109375
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 40960.0
TimeSinceStart : 1092.0589458942413
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 11.09398901462555
Loss_Value : 1.3173637241125107
Loss_Entropy : 2.4787323474884033
Loss_Representation : -7.127296090126038
Loss_KL : 0.9056338220834732
Loss_Obs : -0.8572975099086761
Loss_Reward : 0.5140252187848091
Loss_Discount : 0.026020069606602192
Loss_RawKL : 0.7555678337812424
mean_target : -0.8667901158332825
max_target : -0.21898507326841354
min_target : -1.3816723823547363
std_target : 0.3729320913553238
Done logging...

Current epsilon: 0.3137365360138764 at iteration 40960


********** Iteration 10 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0916, -0.2210, -0.1805, -0.1765, -0.2538, -0.1259, -0.1592, -0.1201,
        -0.1580, -0.1378, -0.1827, -0.1098, -0.2355, -0.1732, -0.2486],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6082, -1.6161, -1.4852, -1.3887, -1.2902, -1.1027, -1.0380, -0.9354,
        -0.8673, -0.7557, -0.6562, -0.5043, -0.4179, -0.1953], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6082, -1.6161, -1.4852, -1.3887, -1.2902, -1.1027, -1.0380, -0.9354,
        -0.8673, -0.7557, -0.6562, -0.5043, -0.4179, -0.1953], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0984, -0.2376, -0.1574, -0.1184, -0.2721, -0.1995, -0.1509, -0.1993,
        -0.1660, -0.1405, -0.0499, -0.1526, -0.2458, -0.0637, -0.1831],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5762, -1.5741, -1.4241, -1.3494, -1.3094, -1.1061, -0.9656, -0.8688,
        -0.7134, -0.5837, -0.4720, -0.4481, -0.3147, -0.0725], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5762, -1.5741, -1.4241, -1.3494, -1.3094, -1.1061, -0.9656, -0.8688,
        -0.7134, -0.5837, -0.4720, -0.4481, -0.3147, -0.0725], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0610, -0.2143, -0.1863, -0.1277, -0.0771, -0.1024, -0.1813, -0.0968,
        -0.1775, -0.1145, -0.2313, -0.1658, -0.1717, -0.0971, -0.0333],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.3539, -1.3776, -1.2434, -1.1251, -1.0620, -1.0496, -1.0113, -0.8860,
        -0.8416, -0.7090, -0.6322, -0.4283, -0.2809, -0.1168], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.3539, -1.3776, -1.2434, -1.1251, -1.0620, -1.0496, -1.0113, -0.8860,
        -0.8416, -0.7090, -0.6322, -0.4283, -0.2809, -0.1168], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0849, -0.1302, -0.1230, -0.0506, -0.1676, -0.0271, -0.1639, -0.1421,
        -0.0463, -0.1313, -0.0629, -0.1889, -0.1127, -0.1131, -0.1138],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.0421, -1.0204, -0.9495, -0.8826, -0.8882, -0.7702, -0.7935, -0.6735,
        -0.5669, -0.5545, -0.4518, -0.4159, -0.2400, -0.1339], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.0421, -1.0204, -0.9495, -0.8826, -0.8882, -0.7702, -0.7935, -0.6735,
        -0.5669, -0.5545, -0.4518, -0.4159, -0.2400, -0.1339], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0766, 0.0751, 0.0791, 0.0808, 0.0748, 0.0909, 0.0938, 0.0957, 0.0679,
        0.0805, 0.1023, 0.0826], device='cuda:0')
Count of actions: (array([ 0,  2,  3,  4,  5,  6,  9, 10]), array([1, 1, 1, 1, 1, 1, 1, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_10.gif
Eval_AverageReturn : -151.5
Eval_StdReturn : 236.63990783691406
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 157.0
Train_AverageReturn : -234.75
Train_StdReturn : 214.99183654785156
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 45056.0
TimeSinceStart : 1192.093508720398
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 11.81315541267395
Loss_Value : 1.3871557116508484
Loss_Entropy : 2.47797828912735
Loss_Representation : -7.219082593917847
Loss_KL : 0.9018402993679047
Loss_Obs : -0.8778285533189774
Loss_Reward : 0.6283131092786789
Loss_Discount : 0.029049488715827465
Loss_RawKL : 0.7547660320997238
mean_target : -0.9181364476680756
max_target : -0.23758850991725922
min_target : -1.4490180909633636
std_target : 0.38844651728868484
Done logging...

Current epsilon: 0.2846094141122759 at iteration 45056


********** Iteration 11 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1666, -0.1010, -0.0929,  0.0144, -0.0030, -0.0773,  0.0295, -0.0688,
        -0.0414, -0.1687, -0.1472, -0.1243, -0.0451, -0.1588, -0.0144],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7619, -0.6348, -0.5707, -0.5081, -0.5584, -0.5925, -0.5511, -0.6203,
        -0.5908, -0.5865, -0.4455, -0.3191, -0.2089, -0.1741], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7619, -0.6348, -0.5707, -0.5081, -0.5584, -0.5925, -0.5511, -0.6203,
        -0.5908, -0.5865, -0.4455, -0.3191, -0.2089, -0.1741], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0909, -0.1752, -0.0948, -0.0899, -0.0891, -0.0400, -0.0145, -0.1341,
        -0.0415, -0.1109, -0.0662,  0.0107, -0.1853, -0.2035,  0.0526],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8751, -0.8399, -0.7108, -0.6599, -0.6102, -0.5553, -0.5506, -0.5735,
        -0.4689, -0.4552, -0.3684, -0.3241, -0.3582, -0.1830], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8751, -0.8399, -0.7108, -0.6599, -0.6102, -0.5553, -0.5506, -0.5735,
        -0.4689, -0.4552, -0.3684, -0.3241, -0.3582, -0.1830], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0058, -0.0786,  0.0376, -0.1225, -0.1206, -0.0311, -0.1077, -0.0007,
        -0.0444,  0.0442, -0.1732, -0.1120, -0.0630, -0.0198,  0.0093],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.5220, -0.5634, -0.5173, -0.5927, -0.5043, -0.4083, -0.3995, -0.3133,
        -0.3333, -0.3084, -0.3734, -0.2119, -0.1050, -0.0453], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.5220, -0.5634, -0.5173, -0.5927, -0.5043, -0.4083, -0.3995, -0.3133,
        -0.3333, -0.3084, -0.3734, -0.2119, -0.1050, -0.0453], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0693, -0.0293, -0.0756, -0.0454, -0.0206, -0.0886,  0.0340, -0.2013,
         0.0141, -0.0621,  0.0243, -0.0235, -0.0239,  0.0548,  0.0045],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.1257e-01, -3.6536e-01, -3.5835e-01, -3.0183e-01, -2.7345e-01,
        -2.7156e-01, -1.9475e-01, -2.4365e-01, -4.5708e-02, -6.3150e-02,
         1.4491e-04, -2.3808e-02,  1.5959e-03,  2.9137e-02], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.1257e-01, -3.6536e-01, -3.5835e-01, -3.0183e-01, -2.7345e-01,
        -2.7156e-01, -1.9475e-01, -2.4365e-01, -4.5708e-02, -6.3150e-02,
         1.4491e-04, -2.3808e-02,  1.5959e-03,  2.9137e-02], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0783, 0.0748, 0.0812, 0.0801, 0.0741, 0.0888, 0.0926, 0.0951, 0.0681,
        0.0816, 0.1016, 0.0839], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([30, 25, 22, 23, 24, 29, 25, 28, 13, 23, 36, 22]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_11.gif
Eval_AverageReturn : -161.3000030517578
Eval_StdReturn : 228.34141540527344
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 164.3000030517578
Train_AverageReturn : -292.5294189453125
Train_StdReturn : 179.07412719726562
Train_MaxReturn : 95.0
Train_MinReturn : -390.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 49152.0
TimeSinceStart : 1299.1167528629303
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 4.505824506282806
Loss_Value : 0.6448848843574524
Loss_Entropy : 2.478632628917694
Loss_Representation : -7.492889046669006
Loss_KL : 0.9086293131113052
Loss_Obs : -0.8998759686946869
Loss_Reward : 0.5703542232513428
Loss_Discount : 0.026887063402682543
Loss_RawKL : 0.7005219161510468
mean_target : -0.3962039574980736
max_target : -0.155838742852211
min_target : -0.584058441221714
std_target : 0.1370287360623479
Done logging...

Current epsilon: 0.2586990980544666 at iteration 49152


********** Iteration 12 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1161, -0.0206,  0.0718, -0.0781,  0.0533,  0.0181,  0.0348, -0.0341,
        -0.0724, -0.1344,  0.1210,  0.0535,  0.0398,  0.0928, -0.0296],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.0914,  0.0257,  0.0490, -0.0219,  0.0614,  0.0086, -0.0092, -0.0474,
        -0.0126,  0.0655,  0.2146,  0.1004,  0.0518,  0.0143], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.0914,  0.0257,  0.0490, -0.0219,  0.0614,  0.0086, -0.0092, -0.0474,
        -0.0126,  0.0655,  0.2146,  0.1004,  0.0518,  0.0143], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0622, -0.0288, -0.1544, -0.0572, -0.0759, -0.0269, -0.1081,  0.0766,
        -0.0816,  0.0627, -0.0281,  0.0058, -0.0039, -0.0432, -0.0758],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.3394, -0.4283, -0.4227, -0.2829, -0.2377, -0.1700, -0.1519, -0.0459,
        -0.1281, -0.0469, -0.1172, -0.0925, -0.1036, -0.1052], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.3394, -0.4283, -0.4227, -0.2829, -0.2377, -0.1700, -0.1519, -0.0459,
        -0.1281, -0.0469, -0.1172, -0.0925, -0.1036, -0.1052], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0925, -0.0316, -0.1106,  0.1279, -0.0668, -0.0889,  0.0079,  0.0579,
        -0.0951, -0.0643,  0.0369, -0.0202, -0.0160, -0.0668,  0.0107],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.3345, -0.2583, -0.2416, -0.1375, -0.2828, -0.2307, -0.1483, -0.1653,
        -0.2364, -0.1487, -0.0888, -0.1317, -0.1170, -0.1066], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.3345, -0.2583, -0.2416, -0.1375, -0.2828, -0.2307, -0.1483, -0.1653,
        -0.2364, -0.1487, -0.0888, -0.1317, -0.1170, -0.1066], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0832, -0.1161, -0.1198, -0.1229, -0.0140, -0.0383, -0.0789, -0.0053,
        -0.0150, -0.0522, -0.0899, -0.0799, -0.0927, -0.1048,  0.0561],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7678, -0.7285, -0.6495, -0.5618, -0.4652, -0.4779, -0.4676, -0.4126,
        -0.4312, -0.4419, -0.4120, -0.3390, -0.2712, -0.1868], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7678, -0.7285, -0.6495, -0.5618, -0.4652, -0.4779, -0.4676, -0.4126,
        -0.4312, -0.4419, -0.4120, -0.3390, -0.2712, -0.1868], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0804, 0.0745, 0.0843, 0.0796, 0.0742, 0.0863, 0.0910, 0.0931, 0.0687,
        0.0827, 0.0995, 0.0855], device='cuda:0')
Count of actions: (array([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([3, 3, 1, 2, 1, 4, 1, 1, 2, 1, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_12.gif
Eval_AverageReturn : -157.60000610351562
Eval_StdReturn : 234.06417846679688
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 160.60000610351562
Train_AverageReturn : -247.63157653808594
Train_StdReturn : 203.57546997070312
Train_MaxReturn : 93.0
Train_MinReturn : -395.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 53248.0
TimeSinceStart : 1399.9108619689941
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 2.494577929377556
Loss_Value : 0.5587908029556274
Loss_Entropy : 2.4797324538230896
Loss_Representation : -7.415703058242798
Loss_KL : 1.187391221523285
Loss_Obs : -0.9182584136724472
Loss_Reward : 0.5510612577199936
Loss_Discount : 0.028428923338651657
Loss_RawKL : 1.1873912066221237
mean_target : -0.25257623568177223
max_target : -0.14017673581838608
min_target : -0.3425026684999466
std_target : 0.06500667985528708
Done logging...

Current epsilon: 0.23565032308509076 at iteration 53248


********** Iteration 13 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0453, -0.0548, -0.1496, -0.1045, -0.1218, -0.1039, -0.1408, -0.0491,
        -0.0456, -0.0778, -0.0114, -0.1676, -0.0822, -0.0163, -0.0888],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8630, -0.8689, -0.8655, -0.7595, -0.6971, -0.6094, -0.5354, -0.4189,
        -0.3937, -0.3688, -0.3067, -0.3116, -0.1499, -0.0701], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8630, -0.8689, -0.8655, -0.7595, -0.6971, -0.6094, -0.5354, -0.4189,
        -0.3937, -0.3688, -0.3067, -0.3116, -0.1499, -0.0701], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1360, -0.1711, -0.1734, -0.1653, -0.1527, -0.1086, -0.1901, -0.0833,
        -0.1711, -0.0910, -0.0852, -0.0761, -0.1797, -0.1416, -0.1520],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.3996, -1.3436, -1.2432, -1.1375, -1.0315, -0.9339, -0.8745, -0.7265,
        -0.6835, -0.5449, -0.4789, -0.4147, -0.3565, -0.1861], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.3996, -1.3436, -1.2432, -1.1375, -1.0315, -0.9339, -0.8745, -0.7265,
        -0.6835, -0.5449, -0.4789, -0.4147, -0.3565, -0.1861], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1874, -0.1571, -0.2157, -0.1187, -0.1603, -0.1726, -0.1940, -0.1566,
        -0.2388, -0.1809, -0.1758, -0.1515, -0.1817, -0.1424, -0.1623],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7516, -1.6587, -1.5917, -1.4596, -1.4232, -1.3378, -1.2369, -1.1072,
        -1.0085, -0.8159, -0.6717, -0.5245, -0.3937, -0.2211], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7516, -1.6587, -1.5917, -1.4596, -1.4232, -1.3378, -1.2369, -1.1072,
        -1.0085, -0.8159, -0.6717, -0.5245, -0.3937, -0.2211], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2614, -0.1002, -0.1776, -0.2172, -0.1925, -0.1906, -0.1505, -0.1585,
        -0.2237, -0.1574, -0.1371, -0.1772, -0.2025, -0.1306, -0.1661],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8042, -1.6411, -1.6323, -1.5417, -1.4060, -1.2860, -1.1610, -1.0705,
        -0.9643, -0.7846, -0.6625, -0.5548, -0.3983, -0.2052], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8042, -1.6411, -1.6323, -1.5417, -1.4060, -1.2860, -1.1610, -1.0705,
        -0.9643, -0.7846, -0.6625, -0.5548, -0.3983, -0.2052], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0827, 0.0742, 0.0874, 0.0791, 0.0746, 0.0842, 0.0897, 0.0908, 0.0694,
        0.0837, 0.0972, 0.0870], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  5,  6,  7,  8,  9, 10, 11]), array([6, 2, 3, 2, 3, 1, 1, 2, 3, 4, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_13.gif
Eval_AverageReturn : -74.19999694824219
Eval_StdReturn : 208.05325317382812
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 112.9000015258789
Train_AverageReturn : -162.75999450683594
Train_StdReturn : 226.19200134277344
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 163.83999633789062
Train_EnvstepsSoFar : 57344.0
TimeSinceStart : 1494.5452268123627
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 12.55023717880249
Loss_Value : 1.4278781414031982
Loss_Entropy : 2.4805341958999634
Loss_Representation : -7.320288062095642
Loss_KL : 0.9603857696056366
Loss_Obs : -0.9284562021493912
Loss_Reward : 0.9614813178777695
Loss_Discount : 0.042406815104186535
Loss_RawKL : 0.8303236365318298
mean_target : -0.9708615690469742
max_target : -0.27166585624217987
min_target : -1.5174776017665863
std_target : 0.40051987767219543
Done logging...

Current epsilon: 0.21514705996766492 at iteration 57344


********** Iteration 14 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1687, -0.1759, -0.2012, -0.2228, -0.1931, -0.1134, -0.1916, -0.2146,
        -0.1770, -0.1422, -0.1427, -0.1968, -0.1432, -0.1905, -0.2098],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7986, -1.7288, -1.6445, -1.5285, -1.3822, -1.2590, -1.2145, -1.0835,
        -0.9206, -0.7892, -0.6837, -0.5735, -0.3985, -0.2682], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7986, -1.7288, -1.6445, -1.5285, -1.3822, -1.2590, -1.2145, -1.0835,
        -0.9206, -0.7892, -0.6837, -0.5735, -0.3985, -0.2682], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2149, -0.1788, -0.1837, -0.2126, -0.1879, -0.1548, -0.1836, -0.1546,
        -0.1556, -0.2051, -0.2033, -0.0917, -0.1513, -0.1605, -0.1648],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7867, -1.6682, -1.5810, -1.4818, -1.3459, -1.2279, -1.1345, -1.0093,
        -0.9061, -0.7962, -0.6256, -0.4461, -0.3748, -0.2339], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7867, -1.6682, -1.5810, -1.4818, -1.3459, -1.2279, -1.1345, -1.0093,
        -0.9061, -0.7962, -0.6256, -0.4461, -0.3748, -0.2339], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1728, -0.1620, -0.2082, -0.1710, -0.1304, -0.1645, -0.1516, -0.1487,
        -0.1299, -0.1925, -0.1493, -0.1918, -0.1691, -0.1455, -0.1558],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6628, -1.5801, -1.5034, -1.3737, -1.2729, -1.2074, -1.1041, -1.0073,
        -0.9079, -0.8233, -0.6679, -0.5469, -0.3752, -0.2163], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6628, -1.5801, -1.5034, -1.3737, -1.2729, -1.2074, -1.1041, -1.0073,
        -0.9079, -0.8233, -0.6679, -0.5469, -0.3752, -0.2163], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0974, -0.0692, -0.0932, -0.0943, -0.1894, -0.1357, -0.1479, -0.2193,
        -0.1431, -0.1642, -0.1513, -0.1288, -0.1141, -0.1283, -0.1596],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.3339, -1.3101, -1.3154, -1.2968, -1.2722, -1.1434, -1.0652, -0.9697,
        -0.7967, -0.6921, -0.5590, -0.4314, -0.3197, -0.2154], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.3339, -1.3101, -1.3154, -1.2968, -1.2722, -1.1434, -1.0652, -0.9697,
        -0.7967, -0.6921, -0.5590, -0.4314, -0.3197, -0.2154], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0850, 0.0739, 0.0901, 0.0785, 0.0750, 0.0823, 0.0886, 0.0884, 0.0699,
        0.0849, 0.0951, 0.0885], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([2, 6, 1, 3, 4, 6, 2, 2, 2, 5, 7]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_14.gif
Eval_AverageReturn : -199.5
Eval_StdReturn : 234.4616241455078
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 184.89999389648438
Train_AverageReturn : -234.75
Train_StdReturn : 211.75123596191406
Train_MaxReturn : 94.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 61440.0
TimeSinceStart : 1599.6884455680847
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 14.71660327911377
Loss_Value : 1.7158765196800232
Loss_Entropy : 2.4808732867240906
Loss_Representation : -7.761705279350281
Loss_KL : 1.270969808101654
Loss_Obs : -0.9463701695203781
Loss_Reward : 0.39694856107234955
Loss_Discount : 0.03407809417694807
Loss_RawKL : 1.270969808101654
mean_target : -1.1256123185157776
max_target : -0.3070322275161743
min_target : -1.7737211883068085
std_target : 0.4703398197889328
Done logging...

Current epsilon: 0.1969081818051186 at iteration 61440


********** Iteration 15 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1287, -0.1003, -0.1547, -0.0714, -0.0608, -0.1349, -0.0906, -0.0416,
        -0.0366, -0.0661, -0.0798, -0.1101, -0.0582, -0.0721, -0.0876],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9489, -0.8673, -0.8092, -0.6903, -0.6525, -0.6256, -0.5165, -0.4472,
        -0.4285, -0.4133, -0.3679, -0.3045, -0.2032, -0.1530], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9489, -0.8673, -0.8092, -0.6903, -0.6525, -0.6256, -0.5165, -0.4472,
        -0.4285, -0.4133, -0.3679, -0.3045, -0.2032, -0.1530], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1266, -0.0974, -0.0750, -0.0683, -0.1132, -0.0919, -0.1551, -0.0933,
        -0.0490, -0.1192, -0.0919, -0.0848, -0.0292, -0.0579, -0.1275],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9673, -0.8888, -0.8387, -0.8092, -0.7827, -0.7082, -0.6505, -0.5245,
        -0.4562, -0.4281, -0.3240, -0.2423, -0.1649, -0.1407], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9673, -0.8888, -0.8387, -0.8092, -0.7827, -0.7082, -0.6505, -0.5245,
        -0.4562, -0.4281, -0.3240, -0.2423, -0.1649, -0.1407], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2128, -0.1898, -0.2087, -0.0773, -0.1790, -0.1528, -0.1767, -0.2101,
        -0.1420, -0.1266, -0.1811, -0.1615, -0.1776, -0.1902, -0.1634],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7280, -1.6067, -1.5046, -1.3746, -1.3721, -1.2635, -1.1770, -1.0597,
        -0.9008, -0.8009, -0.7124, -0.5611, -0.4189, -0.2529], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7280, -1.6067, -1.5046, -1.3746, -1.3721, -1.2635, -1.1770, -1.0597,
        -0.9008, -0.8009, -0.7124, -0.5611, -0.4189, -0.2529], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1382, -0.1564, -0.1518, -0.1182, -0.1186, -0.1051, -0.1579, -0.1373,
        -0.1164, -0.1118, -0.1220, -0.1082, -0.1197, -0.1318, -0.1463],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.3257, -1.2595, -1.1691, -1.0767, -1.0141, -0.9459, -0.8901, -0.7738,
        -0.6716, -0.5846, -0.4975, -0.3943, -0.2994, -0.1886], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.3257, -1.2595, -1.1691, -1.0767, -1.0141, -0.9459, -0.8901, -0.7738,
        -0.6716, -0.5846, -0.4975, -0.3943, -0.2994, -0.1886], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0872, 0.0736, 0.0926, 0.0772, 0.0754, 0.0805, 0.0876, 0.0860, 0.0701,
        0.0864, 0.0932, 0.0902], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([26, 26, 27, 24, 18, 22, 16, 29, 19, 29, 31, 33]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_15.gif
Eval_AverageReturn : -158.10000610351562
Eval_StdReturn : 232.44244384765625
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 161.10000610351562
Train_AverageReturn : -179.0
Train_StdReturn : 231.9247283935547
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 170.6666717529297
Train_EnvstepsSoFar : 65536.0
TimeSinceStart : 1706.5660345554352
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 11.25438928604126
Loss_Value : 1.1294640451669693
Loss_Entropy : 2.4806450605392456
Loss_Representation : -8.924154281616211
Loss_KL : 0.9357962012290955
Loss_Obs : -1.0049120783805847
Loss_Reward : 0.16035022935830057
Loss_Discount : 0.02881993819028139
Loss_RawKL : 0.8681694269180298
mean_target : -0.8783046752214432
max_target : -0.2796326279640198
min_target : -1.3590478599071503
std_target : 0.3456052541732788
Done logging...

Current epsilon: 0.18068360941763928 at iteration 65536


********** Iteration 16 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1398, -0.1231, -0.1500, -0.1047, -0.1274, -0.1016, -0.1280, -0.0974,
        -0.1148, -0.0829, -0.1219, -0.0828, -0.1511, -0.1290, -0.1114],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2287, -1.1554, -1.0926, -0.9988, -0.9444, -0.8634, -0.8051, -0.7147,
        -0.6489, -0.5615, -0.5044, -0.4012, -0.3338, -0.1888], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2287, -1.1554, -1.0926, -0.9988, -0.9444, -0.8634, -0.8051, -0.7147,
        -0.6489, -0.5615, -0.5044, -0.4012, -0.3338, -0.1888], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2183, -0.1658, -0.1874, -0.1538, -0.1495, -0.1105, -0.0779, -0.0223,
         0.0335,  0.0753,  0.0849,  0.0301,  0.0617,  0.0359,  0.0937],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8604, -0.6799, -0.5433, -0.3733, -0.2302, -0.0831,  0.0333,  0.1232,
         0.1581,  0.1376,  0.0701, -0.0100, -0.0357, -0.0970], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8604, -0.6799, -0.5433, -0.3733, -0.2302, -0.0831,  0.0333,  0.1232,
         0.1581,  0.1376,  0.0701, -0.0100, -0.0357, -0.0970], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0909, -0.0841, -0.0616, -0.0782, -0.0751, -0.0994, -0.1113, -0.1026,
        -0.1109, -0.1336, -0.1275, -0.1632, -0.0911, -0.1163, -0.1302],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.0836, -1.0510, -1.0209, -1.0152, -0.9915, -0.9711, -0.9225, -0.8588,
        -0.7965, -0.7249, -0.6226, -0.5186, -0.3697, -0.2903], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.0836, -1.0510, -1.0209, -1.0152, -0.9915, -0.9711, -0.9225, -0.8588,
        -0.7965, -0.7249, -0.6226, -0.5186, -0.3697, -0.2903], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0211,  0.0186, -0.0071,  0.0193, -0.0251, -0.0376, -0.0394, -0.0696,
        -0.1198, -0.1738, -0.1945, -0.1920, -0.2250, -0.2476, -0.2629],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7868, -0.8524, -0.9173, -0.9638, -1.0395, -1.0724, -1.0935, -1.1147,
        -1.1045, -1.0415, -0.9164, -0.7583, -0.5951, -0.3862], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7868, -0.8524, -0.9173, -0.9638, -1.0395, -1.0724, -1.0935, -1.1147,
        -1.1045, -1.0415, -0.9164, -0.7583, -0.5951, -0.3862], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0892, 0.0729, 0.0945, 0.0759, 0.0752, 0.0790, 0.0877, 0.0841, 0.0693,
        0.0886, 0.0922, 0.0915], device='cuda:0')
Count of actions: (array([ 0,  4,  6, 10, 11]), array([1, 1, 2, 1, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_16.gif
Eval_AverageReturn : -196.60000610351562
Eval_StdReturn : 233.86585998535156
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 184.0
Train_AverageReturn : -253.73684692382812
Train_StdReturn : 208.7294464111328
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 69632.0
TimeSinceStart : 1810.7290079593658
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 13.240877389907837
Loss_Value : 2.0925512313842773
Loss_Entropy : 2.4796722531318665
Loss_Representation : -8.241790533065796
Loss_KL : 0.907136544585228
Loss_Obs : -1.0130338668823242
Loss_Reward : 0.9553368240594864
Loss_Discount : 0.02607457572594285
Loss_RawKL : 0.7820103466510773
mean_target : -1.0201665461063385
max_target : -0.3438170701265335
min_target : -1.5295689105987549
std_target : 0.37818530946969986
Done logging...

Current epsilon: 0.16625088242584904 at iteration 69632


********** Iteration 17 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0719, -0.0944, -0.1045, -0.1251, -0.1327, -0.1201, -0.1176, -0.1277,
        -0.1069, -0.1038, -0.1239, -0.1424, -0.1588, -0.1284, -0.1321],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2106, -1.2045, -1.1733, -1.1320, -1.0668, -0.9873, -0.9146, -0.8392,
        -0.7485, -0.6752, -0.5992, -0.4982, -0.3692, -0.2153], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2106, -1.2045, -1.1733, -1.1320, -1.0668, -0.9873, -0.9146, -0.8392,
        -0.7485, -0.6752, -0.5992, -0.4982, -0.3692, -0.2153], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1923, -0.1976, -0.1306, -0.0931, -0.1464, -0.0972, -0.1204, -0.0938,
        -0.0476, -0.0818, -0.0498, -0.0626, -0.0418, -0.0833, -0.0059],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2111, -1.0780, -0.9285, -0.8404, -0.7877, -0.6752, -0.6085, -0.5132,
        -0.4382, -0.4072, -0.3397, -0.3018, -0.2489, -0.2117], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2111, -1.0780, -0.9285, -0.8404, -0.7877, -0.6752, -0.6085, -0.5132,
        -0.4382, -0.4072, -0.3397, -0.3018, -0.2489, -0.2117], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2788, -0.2792, -0.2477, -0.2513, -0.2636, -0.2655, -0.3074, -0.2926,
        -0.2719, -0.2832, -0.2917, -0.2724, -0.2884, -0.2375, -0.2789],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.7290, -2.6014, -2.4672, -2.3594, -2.2367, -2.0956, -1.9406, -1.7311,
        -1.5262, -1.3291, -1.1087, -0.8632, -0.6225, -0.3493], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.7290, -2.6014, -2.4672, -2.3594, -2.2367, -2.0956, -1.9406, -1.7311,
        -1.5262, -1.3291, -1.1087, -0.8632, -0.6225, -0.3493], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1874, -0.2622, -0.2726, -0.2738, -0.2750, -0.2853, -0.2531, -0.3184,
        -0.3097, -0.2696, -0.3159, -0.3151, -0.2774, -0.2524, -0.2834],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.7293, -2.6989, -2.5832, -2.4558, -2.3127, -2.1590, -1.9849, -1.8349,
        -1.6061, -1.3712, -1.1663, -0.9029, -0.6194, -0.3599], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.7293, -2.6989, -2.5832, -2.4558, -2.3127, -2.1590, -1.9849, -1.8349,
        -1.6061, -1.3712, -1.1663, -0.9029, -0.6194, -0.3599], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0912, 0.0724, 0.0942, 0.0751, 0.0744, 0.0786, 0.0888, 0.0832, 0.0681,
        0.0906, 0.0917, 0.0917], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([19, 24, 29, 23, 16, 26, 31, 30, 26, 21, 29, 26]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_17.gif
Eval_AverageReturn : -211.3000030517578
Eval_StdReturn : 217.31959533691406
Eval_MaxReturn : 76.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 195.1999969482422
Train_AverageReturn : -213.7142791748047
Train_StdReturn : 221.4852752685547
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 73728.0
TimeSinceStart : 1922.3599224090576
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 15.891630411148071
Loss_Value : 2.066853493452072
Loss_Entropy : 2.4789891839027405
Loss_Representation : -8.926121473312378
Loss_KL : 1.0566636621952057
Loss_Obs : -1.0197745263576508
Loss_Reward : 0.18099087849259377
Loss_Discount : 0.033969477750360966
Loss_RawKL : 1.0092829763889313
mean_target : -1.209485650062561
max_target : -0.37604065239429474
min_target : -1.8865883350372314
std_target : 0.48791012167930603
Done logging...

Current epsilon: 0.15341210902432012 at iteration 73728


********** Iteration 18 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.3030, -0.3013, -0.2347, -0.2868, -0.2963, -0.2778, -0.2860, -0.2420,
        -0.2715, -0.2876, -0.2660, -0.2915, -0.2851, -0.2947, -0.2647],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.8245, -2.6773, -2.5214, -2.4277, -2.2719, -2.0954, -1.9278, -1.7398,
        -1.5901, -1.3970, -1.1745, -0.9642, -0.7103, -0.4450], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.8245, -2.6773, -2.5214, -2.4277, -2.2719, -2.0954, -1.9278, -1.7398,
        -1.5901, -1.3970, -1.1745, -0.9642, -0.7103, -0.4450], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1395, -0.1314, -0.1304, -0.1374, -0.1402, -0.1506, -0.1446, -0.1463,
        -0.1378, -0.1555, -0.1446, -0.1372, -0.1266, -0.1337, -0.1430],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4687, -1.4095, -1.3548, -1.2923, -1.2206, -1.1403, -1.0477, -0.9539,
        -0.8542, -0.7595, -0.6375, -0.5197, -0.4019, -0.2876], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4687, -1.4095, -1.3548, -1.2923, -1.2206, -1.1403, -1.0477, -0.9539,
        -0.8542, -0.7595, -0.6375, -0.5197, -0.4019, -0.2876], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0542, -0.0341, -0.0313, -0.0081, -0.0363, -0.0387,  0.0003,  0.0034,
        -0.0038,  0.0057,  0.0013, -0.0099, -0.0568, -0.0444, -0.0309],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.3556, -0.3131, -0.2888, -0.2633, -0.2640, -0.2340, -0.2002, -0.2065,
        -0.2160, -0.2212, -0.2372, -0.2492, -0.2488, -0.1996], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.3556, -0.3131, -0.2888, -0.2633, -0.2640, -0.2340, -0.2002, -0.2065,
        -0.2160, -0.2212, -0.2372, -0.2492, -0.2488, -0.1996], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2048, -0.1934, -0.1900, -0.2153, -0.2384, -0.2720, -0.3191, -0.2917,
        -0.3285, -0.3304, -0.3015, -0.2823, -0.3154, -0.3054, -0.2865],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.6669, -2.6076, -2.5605, -2.5102, -2.4358, -2.3287, -2.1762, -1.9691,
        -1.7781, -1.5316, -1.2725, -1.0245, -0.7840, -0.4889], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.6669, -2.6076, -2.5605, -2.5102, -2.4358, -2.3287, -2.1762, -1.9691,
        -1.7781, -1.5316, -1.2725, -1.0245, -0.7840, -0.4889], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0916, 0.0729, 0.0949, 0.0749, 0.0750, 0.0777, 0.0870, 0.0817, 0.0696,
        0.0913, 0.0907, 0.0926], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([19, 23, 18, 21, 24, 29, 30, 33, 23, 24, 26, 30]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_18.gif
Eval_AverageReturn : -198.39999389648438
Eval_StdReturn : 230.91131591796875
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 185.8000030517578
Train_AverageReturn : -219.7142791748047
Train_StdReturn : 219.4604034423828
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 77824.0
TimeSinceStart : 2032.3979535102844
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 11.854949831962585
Loss_Value : 1.847093477845192
Loss_Entropy : 2.479295015335083
Loss_Representation : -9.059280157089233
Loss_KL : 1.140883445739746
Loss_Obs : -1.0480356514453888
Loss_Reward : 0.24721705168485641
Loss_Discount : 0.03297614539042115
Loss_RawKL : 1.140883445739746
mean_target : -0.9211611151695251
max_target : -0.3352689892053604
min_target : -1.3620612174272537
std_target : 0.3316282518208027
Done logging...

Current epsilon: 0.14199125262278423 at iteration 77824


********** Iteration 19 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([0.0896, 0.1215, 0.1188, 0.1351, 0.1048, 0.1135, 0.1522, 0.1506, 0.1388,
        0.1789, 0.1737, 0.1270, 0.1408, 0.1076, 0.0925], device='cuda:0',
       grad_fn=<SelectBackward0>)
Loss total:  tensor([ 1.1262,  1.1071,  1.0539,  1.0026,  0.9288,  0.8808,  0.8242,  0.7203,
         0.6119,  0.5074,  0.3560,  0.2018,  0.0861, -0.0517], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([ 1.1262,  1.1071,  1.0539,  1.0026,  0.9288,  0.8808,  0.8242,  0.7203,
         0.6119,  0.5074,  0.3560,  0.2018,  0.0861, -0.0517], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0476, -0.0199, -0.0546, -0.0420, -0.0297, -0.0675, -0.1289, -0.1247,
        -0.1778, -0.0943, -0.0745, -0.0613, -0.0737, -0.0311,  0.0275],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8198, -0.8119, -0.8316, -0.8179, -0.8156, -0.8283, -0.8010, -0.7070,
        -0.6089, -0.4502, -0.3697, -0.3075, -0.2539, -0.1824], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8198, -0.8119, -0.8316, -0.8179, -0.8156, -0.8283, -0.8010, -0.7070,
        -0.6089, -0.4502, -0.3697, -0.3075, -0.2539, -0.1824], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1267, -0.1187, -0.1303, -0.1186, -0.1076, -0.0771, -0.0139, -0.0609,
        -0.1043, -0.0257, -0.0296, -0.0415,  0.0111, -0.0110, -0.0150],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9336, -0.8467, -0.7656, -0.6637, -0.5678, -0.4761, -0.4144, -0.4159,
        -0.3700, -0.2739, -0.2556, -0.2318, -0.1960, -0.2094], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9336, -0.8467, -0.7656, -0.6637, -0.5678, -0.4761, -0.4144, -0.4159,
        -0.3700, -0.2739, -0.2556, -0.2318, -0.1960, -0.2094], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1082, -0.0783, -0.0854, -0.0870, -0.0695, -0.0669, -0.0710, -0.0694,
        -0.0608, -0.0438, -0.0520, -0.0668, -0.0679, -0.0680, -0.0639],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8612, -0.7916, -0.7506, -0.6962, -0.6372, -0.5970, -0.5540, -0.5050,
        -0.4543, -0.4115, -0.3816, -0.3440, -0.2871, -0.2238], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8612, -0.7916, -0.7506, -0.6962, -0.6372, -0.5970, -0.5540, -0.5050,
        -0.4543, -0.4115, -0.3816, -0.3440, -0.2871, -0.2238], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0920, 0.0729, 0.0947, 0.0754, 0.0753, 0.0778, 0.0875, 0.0805, 0.0701,
        0.0918, 0.0900, 0.0922], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([33, 18, 29, 18, 25, 29, 31, 41, 20, 14, 20, 22]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_19.gif
Eval_AverageReturn : -116.69999694824219
Eval_StdReturn : 223.33116149902344
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 137.8000030517578
Train_AverageReturn : -251.36842346191406
Train_StdReturn : 204.6774444580078
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 81920.0
TimeSinceStart : 2135.8103001117706
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 4.362419903278351
Loss_Value : 1.2300862222909927
Loss_Entropy : 2.4794313311576843
Loss_Representation : -9.219121932983398
Loss_KL : 1.07360278069973
Loss_Obs : -1.070804625749588
Loss_Reward : 0.3859652616083622
Loss_Discount : 0.029356482904404402
Loss_RawKL : 1.07360278069973
mean_target : -0.38598454743623734
max_target : -0.2621454894542694
min_target : -0.4978101886808872
std_target : 0.07548525941092521
Done logging...

Current epsilon: 0.13183171815129263 at iteration 81920


********** Iteration 20 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1590, -0.1369, -0.1323, -0.1309, -0.1253, -0.1496, -0.1264, -0.1549,
        -0.1246, -0.1336, -0.1342, -0.1328, -0.1414, -0.1154, -0.1573],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4835, -1.3994, -1.3306, -1.2655, -1.1960, -1.1301, -1.0333, -0.9532,
        -0.8408, -0.7512, -0.6485, -0.5380, -0.4256, -0.2962], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4835, -1.3994, -1.3306, -1.2655, -1.1960, -1.1301, -1.0333, -0.9532,
        -0.8408, -0.7512, -0.6485, -0.5380, -0.4256, -0.2962], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2170, -0.2073, -0.2109, -0.1920, -0.1977, -0.1973, -0.2094, -0.2113,
        -0.1994, -0.2120, -0.1880, -0.2209, -0.1987, -0.2149, -0.1888],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.1510, -2.0471, -1.9470, -1.8371, -1.7407, -1.6303, -1.5172, -1.3801,
        -1.2379, -1.0940, -0.9307, -0.7837, -0.5877, -0.4041], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.1510, -2.0471, -1.9470, -1.8371, -1.7407, -1.6303, -1.5172, -1.3801,
        -1.2379, -1.0940, -0.9307, -0.7837, -0.5877, -0.4041], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2388, -0.2390, -0.2400, -0.2635, -0.2157, -0.2204, -0.2519, -0.2572,
        -0.2121, -0.2134, -0.2465, -0.2396, -0.2273, -0.2368, -0.2098],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.4457, -2.3380, -2.2207, -2.0936, -1.9348, -1.8158, -1.6827, -1.5093,
        -1.3196, -1.1678, -1.0070, -0.8003, -0.5865, -0.3734], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.4457, -2.3380, -2.2207, -2.0936, -1.9348, -1.8158, -1.6827, -1.5093,
        -1.3196, -1.1678, -1.0070, -0.8003, -0.5865, -0.3734], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0085, -0.0486, -0.0533, -0.0528, -0.0475, -0.0977, -0.1091, -0.1201,
        -0.1194, -0.1360, -0.1209, -0.0306, -0.0921, -0.1782, -0.0749],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9993, -1.0399, -1.0420, -1.0399, -1.0366, -1.0412, -0.9925, -0.9285,
        -0.8494, -0.7649, -0.6561, -0.5564, -0.5449, -0.4697], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9993, -1.0399, -1.0420, -1.0399, -1.0366, -1.0412, -0.9925, -0.9285,
        -0.8494, -0.7649, -0.6561, -0.5564, -0.5449, -0.4697], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0941, 0.0738, 0.0944, 0.0750, 0.0761, 0.0781, 0.0882, 0.0791, 0.0705,
        0.0906, 0.0889, 0.0913], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([24, 22, 30, 22, 19, 27, 32, 29, 22, 22, 27, 24]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_20.gif
Eval_AverageReturn : -249.5
Eval_StdReturn : 214.70362854003906
Eval_MaxReturn : 92.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 215.8000030517578
Train_AverageReturn : -218.2857208251953
Train_StdReturn : 219.0812225341797
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 86016.0
TimeSinceStart : 2250.3518481254578
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 18.0383083820343
Loss_Value : 1.7941937744617462
Loss_Entropy : 2.479684829711914
Loss_Representation : -9.890660762786865
Loss_KL : 0.9723756313323975
Loss_Obs : -1.0991363525390625
Loss_Reward : 0.09644377324730158
Loss_Discount : 0.03188339248299599
Loss_RawKL : 0.9275137037038803
mean_target : -1.362841159105301
max_target : -0.4558057636022568
min_target : -2.060580223798752
std_target : 0.5161720812320709
Done logging...

Current epsilon: 0.12279420493437261 at iteration 86016


********** Iteration 21 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1694, -0.1773, -0.1630, -0.1732, -0.1505, -0.1744, -0.1622, -0.1273,
        -0.1745, -0.1611, -0.1512, -0.1415, -0.1720, -0.1635, -0.1531],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7839, -1.7074, -1.6132, -1.5349, -1.4344, -1.3551, -1.2433, -1.1387,
        -1.0629, -0.9329, -0.8106, -0.6903, -0.5682, -0.4110], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7839, -1.7074, -1.6132, -1.5349, -1.4344, -1.3551, -1.2433, -1.1387,
        -1.0629, -0.9329, -0.8106, -0.6903, -0.5682, -0.4110], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0782, -0.0860, -0.0771, -0.0890, -0.0946, -0.1035, -0.1010, -0.0936,
        -0.0966, -0.0943, -0.0909, -0.0945, -0.0886, -0.0832, -0.1007],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.0533, -1.0261, -0.9880, -0.9537, -0.9083, -0.8555, -0.7887, -0.7197,
        -0.6562, -0.5851, -0.5088, -0.4355, -0.3522, -0.2705], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.0533, -1.0261, -0.9880, -0.9537, -0.9083, -0.8555, -0.7887, -0.7197,
        -0.6562, -0.5851, -0.5088, -0.4355, -0.3522, -0.2705], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1060, -0.1058, -0.0988, -0.1076, -0.1191, -0.1025, -0.1106, -0.1008,
        -0.1017, -0.1045, -0.1049, -0.0908, -0.0969, -0.1024, -0.1020],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2001, -1.1511, -1.1002, -1.0521, -0.9905, -0.9164, -0.8527, -0.7774,
        -0.7092, -0.6353, -0.5529, -0.4682, -0.3894, -0.3011], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2001, -1.1511, -1.1002, -1.0521, -0.9905, -0.9164, -0.8527, -0.7774,
        -0.7092, -0.6353, -0.5529, -0.4682, -0.3894, -0.3011], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1437, -0.1558, -0.1442, -0.1613, -0.1554, -0.1663, -0.1493, -0.1553,
        -0.1515, -0.1519, -0.1448, -0.1536, -0.1592, -0.1452, -0.1467],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7126, -1.6535, -1.5804, -1.5126, -1.4239, -1.3360, -1.2319, -1.1340,
        -1.0271, -0.9169, -0.8017, -0.6862, -0.5556, -0.4072], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7126, -1.6535, -1.5804, -1.5126, -1.4239, -1.3360, -1.2319, -1.1340,
        -1.0271, -0.9169, -0.8017, -0.6862, -0.5556, -0.4072], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0948, 0.0741, 0.0940, 0.0752, 0.0764, 0.0788, 0.0890, 0.0781, 0.0709,
        0.0902, 0.0876, 0.0909], device='cuda:0')
Count of actions: (array([ 0,  1,  3,  4,  5,  7,  8,  9, 11]), array([1, 1, 2, 4, 5, 2, 2, 1, 2]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_21.gif
Eval_AverageReturn : -208.60000610351562
Eval_StdReturn : 221.70034790039062
Eval_MaxReturn : 93.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 192.5
Train_AverageReturn : -191.17391967773438
Train_StdReturn : 227.58749389648438
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 178.0869598388672
Train_EnvstepsSoFar : 90112.0
TimeSinceStart : 2356.2331852912903
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 11.83219313621521
Loss_Value : 0.9361463487148285
Loss_Entropy : 2.4798147082328796
Loss_Representation : -10.31917142868042
Loss_KL : 0.9300682991743088
Loss_Obs : -1.1236457824707031
Loss_Reward : -0.04486688785254955
Loss_Discount : 0.03208515699952841
Loss_RawKL : 0.9300682991743088
mean_target : -0.9195509850978851
max_target : -0.4160844162106514
min_target : -1.3212059438228607
std_target : 0.2900153324007988
Done logging...

Current epsilon: 0.11475479669424161 at iteration 90112


********** Iteration 22 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1412, -0.1387, -0.1572, -0.1502, -0.1437, -0.1249, -0.0970, -0.1569,
        -0.1077, -0.0895, -0.1179, -0.0754, -0.0461, -0.0438, -0.0795],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4047, -1.3315, -1.2532, -1.1534, -1.0511, -0.9508, -0.8633, -0.8033,
        -0.6786, -0.5952, -0.5236, -0.4195, -0.3510, -0.3094], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4047, -1.3315, -1.2532, -1.1534, -1.0511, -0.9508, -0.8633, -0.8033,
        -0.6786, -0.5952, -0.5236, -0.4195, -0.3510, -0.3094], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0655, -0.0648, -0.0619, -0.0789, -0.0712, -0.0788, -0.0595, -0.0742,
        -0.0807, -0.0718, -0.0653, -0.0841, -0.0744, -0.0676, -0.0724],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9012, -0.8760, -0.8544, -0.8282, -0.7832, -0.7421, -0.6930, -0.6610,
        -0.6108, -0.5497, -0.4986, -0.4484, -0.3753, -0.3081], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9012, -0.8760, -0.8544, -0.8282, -0.7832, -0.7421, -0.6930, -0.6610,
        -0.6108, -0.5497, -0.4986, -0.4484, -0.3753, -0.3081], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0714, -0.0739, -0.0726, -0.0683, -0.0741, -0.0805, -0.0776, -0.0689,
        -0.0661, -0.0726, -0.0775, -0.0806, -0.0836, -0.0814, -0.0794],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9624, -0.9287, -0.8950, -0.8602, -0.8276, -0.7871, -0.7376, -0.6883,
        -0.6457, -0.6046, -0.5504, -0.4899, -0.4211, -0.3441], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9624, -0.9287, -0.8950, -0.8602, -0.8276, -0.7871, -0.7376, -0.6883,
        -0.6457, -0.6046, -0.5504, -0.4899, -0.4211, -0.3441], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1697, -0.1600, -0.1551, -0.1404, -0.1516, -0.1436, -0.1619, -0.1588,
        -0.1498, -0.1322, -0.1333, -0.1259, -0.1375, -0.1413, -0.1334],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7067, -1.6208, -1.5380, -1.4543, -1.3871, -1.3000, -1.2136, -1.1059,
        -0.9932, -0.8832, -0.7837, -0.6802, -0.5738, -0.4472], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7067, -1.6208, -1.5380, -1.4543, -1.3871, -1.3000, -1.2136, -1.1059,
        -0.9932, -0.8832, -0.7837, -0.6802, -0.5738, -0.4472], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0946, 0.0750, 0.0935, 0.0751, 0.0769, 0.0794, 0.0885, 0.0777, 0.0715,
        0.0905, 0.0866, 0.0907], device='cuda:0')
Count of actions: (array([ 1,  3,  4,  5,  6,  7, 11]), array([1, 1, 1, 1, 3, 1, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_22.gif
Eval_AverageReturn : -244.10000610351562
Eval_StdReturn : 219.60302734375
Eval_MaxReturn : 92.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 212.89999389648438
Train_AverageReturn : -199.40908813476562
Train_StdReturn : 220.7935333251953
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 186.18182373046875
Train_EnvstepsSoFar : 94208.0
TimeSinceStart : 2464.4408354759216
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 11.111029863357544
Loss_Value : 0.9110650420188904
Loss_Entropy : 2.4801857471466064
Loss_Representation : -9.916370868682861
Loss_KL : 1.2375681102275848
Loss_Obs : -1.136953204870224
Loss_Reward : 0.18145150132477283
Loss_Discount : 0.03414203692227602
Loss_RawKL : 1.1767123937606812
mean_target : -0.8680507242679596
max_target : -0.43621599674224854
min_target : -1.1979308873414993
std_target : 0.24575341306626797
Done logging...

Current epsilon: 0.10760326249449276 at iteration 94208


********** Iteration 23 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1724, -0.1671, -0.1743, -0.1875, -0.1802, -0.1873, -0.1827, -0.1836,
        -0.1798, -0.1813, -0.1829, -0.1865, -0.1746, -0.1805, -0.1707],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9931, -1.9233, -1.8492, -1.7627, -1.6619, -1.5584, -1.4434, -1.3270,
        -1.2012, -1.0674, -0.9265, -0.7767, -0.6155, -0.4553], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9931, -1.9233, -1.8492, -1.7627, -1.6619, -1.5584, -1.4434, -1.3270,
        -1.2012, -1.0674, -0.9265, -0.7767, -0.6155, -0.4553], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1509, -0.1574, -0.1583, -0.1586, -0.1565, -0.1508, -0.1612, -0.1561,
        -0.1617, -0.1674, -0.1631, -0.1623, -0.1608, -0.1547, -0.1593],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8093, -1.7494, -1.6774, -1.5976, -1.5143, -1.4297, -1.3432, -1.2443,
        -1.1390, -1.0246, -0.8943, -0.7634, -0.6226, -0.4794], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8093, -1.7494, -1.6774, -1.5976, -1.5143, -1.4297, -1.3432, -1.2443,
        -1.1390, -1.0246, -0.8943, -0.7634, -0.6226, -0.4794], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1143, -0.1170, -0.1165, -0.1207, -0.1223, -0.1182, -0.1228, -0.1195,
        -0.1187, -0.1178, -0.1246, -0.1159, -0.1135, -0.1213, -0.1166],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4353, -1.3905, -1.3342, -1.2771, -1.2146, -1.1434, -1.0741, -0.9946,
        -0.9100, -0.8257, -0.7369, -0.6348, -0.5383, -0.4384], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4353, -1.3905, -1.3342, -1.2771, -1.2146, -1.1434, -1.0741, -0.9946,
        -0.9100, -0.8257, -0.7369, -0.6348, -0.5383, -0.4384], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0909, -0.0936, -0.0861, -0.0921, -0.0879, -0.0867, -0.0870, -0.0827,
        -0.0946, -0.0854, -0.0851, -0.0804, -0.0823, -0.0802, -0.0949],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.1385, -1.0962, -1.0487, -1.0093, -0.9577, -0.9072, -0.8591, -0.8037,
        -0.7505, -0.6800, -0.6144, -0.5466, -0.4775, -0.4043], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.1385, -1.0962, -1.0487, -1.0093, -0.9577, -0.9072, -0.8591, -0.8037,
        -0.7505, -0.6800, -0.6144, -0.5466, -0.4775, -0.4043], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0941, 0.0755, 0.0918, 0.0751, 0.0766, 0.0805, 0.0893, 0.0782, 0.0714,
        0.0911, 0.0861, 0.0904], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([31, 22, 26, 27, 23, 23, 35, 17, 22, 25, 22, 27]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_23.gif
Eval_AverageReturn : -246.89999389648438
Eval_StdReturn : 215.5548553466797
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 214.6999969482422
Train_AverageReturn : -185.9130401611328
Train_StdReturn : 223.20314025878906
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 178.0869598388672
Train_EnvstepsSoFar : 98304.0
TimeSinceStart : 2578.5319123268127
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 8.758180260658264
Loss_Value : 1.215661108493805
Loss_Entropy : 2.480117380619049
Loss_Representation : -9.791853427886963
Loss_KL : 1.6666614711284637
Loss_Obs : -1.157024085521698
Loss_Reward : 0.07664368092082441
Loss_Discount : 0.03508180985227227
Loss_RawKL : 1.6666613817214966
mean_target : -0.6999874413013458
max_target : -0.4398857206106186
min_target : -0.9230960011482239
std_target : 0.15361292101442814
Done logging...

Current epsilon: 0.10124154532793868 at iteration 98304


********** Iteration 24 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0887, -0.0843, -0.0791, -0.0879, -0.0825, -0.0825, -0.0897, -0.0912,
        -0.0892, -0.0864, -0.0857, -0.0817, -0.0838, -0.0830, -0.0919],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.1214, -1.0759, -1.0324, -0.9967, -0.9498, -0.9033, -0.8575, -0.7992,
        -0.7367, -0.6732, -0.6106, -0.5392, -0.4670, -0.3935], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.1214, -1.0759, -1.0324, -0.9967, -0.9498, -0.9033, -0.8575, -0.7992,
        -0.7367, -0.6732, -0.6106, -0.5392, -0.4670, -0.3935], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1232, -0.1219, -0.1199, -0.1176, -0.1087, -0.1186, -0.1264, -0.1218,
        -0.1204, -0.1087, -0.1173, -0.1170, -0.1189, -0.1200, -0.1177],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4559, -1.3990, -1.3420, -1.2838, -1.2242, -1.1679, -1.0980, -1.0127,
        -0.9262, -0.8374, -0.7578, -0.6655, -0.5654, -0.4574], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4559, -1.3990, -1.3420, -1.2838, -1.2242, -1.1679, -1.0980, -1.0127,
        -0.9262, -0.8374, -0.7578, -0.6655, -0.5654, -0.4574], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1488, -0.1418, -0.1596, -0.1512, -0.1517, -0.1611, -0.1536, -0.1537,
        -0.1598, -0.1584, -0.1556, -0.1601, -0.1518, -0.1564, -0.1636],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8207, -1.7622, -1.7060, -1.6277, -1.5508, -1.4662, -1.3688, -1.2728,
        -1.1689, -1.0549, -0.9341, -0.8060, -0.6714, -0.5316], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8207, -1.7622, -1.7060, -1.6277, -1.5508, -1.4662, -1.3688, -1.2728,
        -1.1689, -1.0549, -0.9341, -0.8060, -0.6714, -0.5316], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0406, -0.0173, -0.0169, -0.0932, -0.0711, -0.1005, -0.1156, -0.1364,
        -0.1324, -0.1446, -0.1347, -0.1309, -0.1342, -0.1473, -0.1633],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.1144, -1.2124, -1.2499, -1.2914, -1.2557, -1.2364, -1.1880, -1.1192,
        -1.0273, -0.9315, -0.8169, -0.7062, -0.5921, -0.4699], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.1144, -1.2124, -1.2499, -1.2914, -1.2557, -1.2364, -1.1880, -1.1192,
        -1.0273, -0.9315, -0.8169, -0.7062, -0.5921, -0.4699], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0931, 0.0773, 0.0928, 0.0749, 0.0775, 0.0800, 0.0861, 0.0783, 0.0732,
        0.0905, 0.0850, 0.0914], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([34, 18, 24, 35, 23, 19, 19, 26, 21, 22, 31, 28]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_24.gif
Eval_AverageReturn : -339.5
Eval_StdReturn : 144.86976623535156
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 270.6000061035156
Train_AverageReturn : -185.9130401611328
Train_StdReturn : 222.83877563476562
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 178.0869598388672
Train_EnvstepsSoFar : 102400.0
TimeSinceStart : 2701.2577097415924
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 14.607743263244629
Loss_Value : 0.9840507507324219
Loss_Entropy : 2.4811431765556335
Loss_Representation : -9.419531106948853
Loss_KL : 1.7496360540390015
Loss_Obs : -1.1239097714424133
Loss_Reward : 0.04209310282021761
Loss_Discount : 0.027837506029754877
Loss_RawKL : 1.7496360540390015
mean_target : -1.1178446561098099
max_target : -0.5379499420523643
min_target : -1.544399231672287
std_target : 0.32574812695384026
Done logging...

Current epsilon: 0.09558241762515149 at iteration 102400


********** Iteration 25 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0529,  0.0308, -0.0319, -0.0760, -0.0702, -0.0557, -0.0478, -0.0279,
        -0.0451, -0.0925, -0.0588, -0.0903, -0.0660, -0.0711, -0.0445],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7481, -0.8281, -0.8932, -0.8930, -0.8444, -0.8048, -0.7742, -0.7508,
        -0.7450, -0.7220, -0.6517, -0.6088, -0.5301, -0.4704], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7481, -0.8281, -0.8932, -0.8930, -0.8444, -0.8048, -0.7742, -0.7508,
        -0.7450, -0.7220, -0.6517, -0.6088, -0.5301, -0.4704], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0172, -0.0195,  0.0138,  0.0243,  0.0211,  0.0144, -0.0424, -0.0335,
        -0.0369, -0.0317,  0.0032,  0.0198,  0.0297, -0.0199,  0.0039],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.4302, -0.4169, -0.4016, -0.4205, -0.4485, -0.4781, -0.5010, -0.4641,
        -0.4349, -0.4004, -0.3698, -0.3739, -0.3955, -0.4275], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.4302, -0.4169, -0.4016, -0.4205, -0.4485, -0.4781, -0.5010, -0.4641,
        -0.4349, -0.4004, -0.3698, -0.3739, -0.3955, -0.4275], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0747, -0.0779, -0.0779, -0.0840, -0.0754, -0.0707, -0.0656, -0.0621,
        -0.0658, -0.0710, -0.0652, -0.0676, -0.0643, -0.0655, -0.0677],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.0147, -0.9831, -0.9430, -0.9017, -0.8556, -0.8080, -0.7676, -0.7274,
        -0.6871, -0.6438, -0.5909, -0.5443, -0.4879, -0.4351], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.0147, -0.9831, -0.9430, -0.9017, -0.8556, -0.8080, -0.7676, -0.7274,
        -0.6871, -0.6438, -0.5909, -0.5443, -0.4879, -0.4351], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0866, -0.0832, -0.0854, -0.0820, -0.0840, -0.0870, -0.0786, -0.0833,
        -0.0811, -0.0831, -0.0858, -0.0848, -0.0815, -0.0818, -0.0845],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.1428, -1.1019, -1.0642, -1.0228, -0.9835, -0.9393, -0.8824, -0.8349,
        -0.7810, -0.7271, -0.6702, -0.6019, -0.5324, -0.4612], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.1428, -1.1019, -1.0642, -1.0228, -0.9835, -0.9393, -0.8824, -0.8349,
        -0.7810, -0.7271, -0.6702, -0.6019, -0.5324, -0.4612], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0920, 0.0778, 0.0918, 0.0752, 0.0775, 0.0804, 0.0853, 0.0798, 0.0740,
        0.0905, 0.0840, 0.0917], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([31, 16, 30, 26, 20, 26, 32, 28, 24, 18, 26, 23]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_25.gif
Eval_AverageReturn : -300.29998779296875
Eval_StdReturn : 175.8909149169922
Eval_MaxReturn : 71.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 248.0
Train_AverageReturn : -174.5833282470703
Train_StdReturn : 226.17652893066406
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 170.6666717529297
Train_EnvstepsSoFar : 106496.0
TimeSinceStart : 2820.520205259323
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 11.360169172286987
Loss_Value : 0.807531401515007
Loss_Entropy : 2.481602728366852
Loss_Representation : -9.61743688583374
Loss_KL : 1.797167420387268
Loss_Obs : -1.1424799859523773
Loss_Reward : -0.02781637106090784
Loss_Discount : 0.03801187686622143
Loss_RawKL : 1.7971673607826233
mean_target : -0.8858887553215027
max_target : -0.5418541133403778
min_target : -1.1545129269361496
std_target : 0.19714098423719406
Done logging...

Current epsilon: 0.09054828524893951 at iteration 106496


********** Iteration 26 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1503, -0.1279, -0.1560, -0.1509, -0.1525, -0.1366, -0.1482, -0.1626,
        -0.1401, -0.1487, -0.1532, -0.1504, -0.1559, -0.1558, -0.1538],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7819, -1.7208, -1.6751, -1.5964, -1.5182, -1.4324, -1.3613, -1.2683,
        -1.1559, -1.0618, -0.9508, -0.8312, -0.7053, -0.5620], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7819, -1.7208, -1.6751, -1.5964, -1.5182, -1.4324, -1.3613, -1.2683,
        -1.1559, -1.0618, -0.9508, -0.8312, -0.7053, -0.5620], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1817, -0.1979, -0.1935, -0.2083, -0.2053, -0.2023, -0.2047, -0.2017,
        -0.1977, -0.1967, -0.2044, -0.1821, -0.1941, -0.1857, -0.2024],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.2636, -2.2003, -2.1117, -2.0205, -1.9082, -1.7967, -1.6793, -1.5520,
        -1.4175, -1.2786, -1.1314, -0.9670, -0.8117, -0.6330], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.2636, -2.2003, -2.1117, -2.0205, -1.9082, -1.7967, -1.6793, -1.5520,
        -1.4175, -1.2786, -1.1314, -0.9670, -0.8117, -0.6330], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1802, -0.1791, -0.1752, -0.1736, -0.1909, -0.1832, -0.1764, -0.1876,
        -0.1854, -0.1822, -0.1926, -0.1800, -0.1712, -0.1679, -0.1847],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0481, -1.9707, -1.8917, -1.8099, -1.7238, -1.6134, -1.5049, -1.3938,
        -1.2673, -1.1324, -0.9913, -0.8302, -0.6718, -0.5109], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0481, -1.9707, -1.8917, -1.8099, -1.7238, -1.6134, -1.5049, -1.3938,
        -1.2673, -1.1324, -0.9913, -0.8302, -0.6718, -0.5109], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1645, -0.1529, -0.1530, -0.1365, -0.1457, -0.0906, -0.1363, -0.1362,
        -0.1201, -0.1505, -0.1388, -0.1403, -0.1338, -0.1319, -0.1235],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6803, -1.5968, -1.5199, -1.4360, -1.3637, -1.2820, -1.2502, -1.1677,
        -1.0791, -1.0017, -0.8833, -0.7700, -0.6532, -0.5356], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6803, -1.5968, -1.5199, -1.4360, -1.3637, -1.2820, -1.2502, -1.1677,
        -1.0791, -1.0017, -0.8833, -0.7700, -0.6532, -0.5356], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0913, 0.0785, 0.0913, 0.0751, 0.0779, 0.0803, 0.0838, 0.0815, 0.0751,
        0.0904, 0.0831, 0.0919], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([23, 23, 28, 23, 28, 24, 24, 23, 29, 26, 16, 33]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_26.gif
Eval_AverageReturn : -203.0
Eval_StdReturn : 227.4629669189453
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 188.39999389648438
Train_AverageReturn : -185.69564819335938
Train_StdReturn : 216.8390350341797
Train_MaxReturn : 94.0
Train_MinReturn : -395.0
Train_AverageEpLen : 178.0869598388672
Train_EnvstepsSoFar : 110592.0
TimeSinceStart : 2931.3738236427307
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 15.733437776565552
Loss_Value : 1.1094843000173569
Loss_Entropy : 2.481990694999695
Loss_Representation : -9.186370849609375
Loss_KL : 1.6732213199138641
Loss_Obs : -1.1213851571083069
Loss_Reward : 0.31691463850438595
Loss_Discount : 0.03734483430162072
Loss_RawKL : 1.673221230506897
mean_target : -1.1982766389846802
max_target : -0.6294052600860596
min_target : -1.66094571352005
std_target : 0.32932858169078827
Done logging...

Current epsilon: 0.086070123575941 at iteration 110592


********** Iteration 27 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1283, -0.1228, -0.1251, -0.1292, -0.1102, -0.1323, -0.1341, -0.1301,
        -0.1318, -0.1343, -0.1321, -0.1347, -0.1308, -0.1343, -0.1301],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6121, -1.5632, -1.5091, -1.4491, -1.3874, -1.3422, -1.2653, -1.1806,
        -1.1006, -1.0117, -0.9121, -0.8098, -0.6999, -0.5841], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6121, -1.5632, -1.5091, -1.4491, -1.3874, -1.3422, -1.2653, -1.1806,
        -1.1006, -1.0117, -0.9121, -0.8098, -0.6999, -0.5841], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1044, -0.1000, -0.0976, -0.0988, -0.1067, -0.0995, -0.0994, -0.0927,
        -0.0952, -0.0972, -0.1044, -0.0998, -0.1040, -0.1007, -0.0965],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.3477, -1.3030, -1.2603, -1.2197, -1.1736, -1.1147, -1.0613, -1.0010,
        -0.9461, -0.8859, -0.8208, -0.7421, -0.6597, -0.5679], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.3477, -1.3030, -1.2603, -1.2197, -1.1736, -1.1147, -1.0613, -1.0010,
        -0.9461, -0.8859, -0.8208, -0.7421, -0.6597, -0.5679], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0879, -0.0868, -0.0904, -0.0858, -0.0922, -0.0867, -0.0850, -0.0894,
        -0.0857, -0.0884, -0.0896, -0.0857, -0.0926, -0.0892, -0.0868],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.2421, -1.2065, -1.1649, -1.1203, -1.0741, -1.0217, -0.9734, -0.9203,
        -0.8633, -0.8051, -0.7384, -0.6613, -0.5880, -0.5033], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.2421, -1.2065, -1.1649, -1.1203, -1.0741, -1.0217, -0.9734, -0.9203,
        -0.8633, -0.8051, -0.7384, -0.6613, -0.5880, -0.5033], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0315, -0.0146,  0.0101, -0.0050, -0.0232, -0.0146, -0.0183, -0.0287,
        -0.0515, -0.0155, -0.0292, -0.0304, -0.0232, -0.0412, -0.0553],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.7052, -0.6880, -0.6854, -0.7119, -0.7208, -0.7116, -0.7104, -0.7075,
        -0.6916, -0.6498, -0.6464, -0.6272, -0.6074, -0.5949], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.7052, -0.6880, -0.6854, -0.7119, -0.7208, -0.7116, -0.7104, -0.7075,
        -0.6916, -0.6498, -0.6464, -0.6272, -0.6074, -0.5949], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0914, 0.0789, 0.0915, 0.0751, 0.0788, 0.0797, 0.0817, 0.0827, 0.0765,
        0.0896, 0.0825, 0.0918], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([34, 12, 34, 25, 25, 28, 20, 17, 26, 34, 21, 24]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_27.gif
Eval_AverageReturn : -206.5
Eval_StdReturn : 223.46107482910156
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 191.39999389648438
Train_AverageReturn : -294.29412841796875
Train_StdReturn : 176.40719604492188
Train_MaxReturn : 94.0
Train_MinReturn : -395.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 114688.0
TimeSinceStart : 3041.9154732227325
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 14.214112043380737
Loss_Value : 0.789848193526268
Loss_Entropy : 2.4822241067886353
Loss_Representation : -10.277745246887207
Loss_KL : 1.5733652710914612
Loss_Obs : -1.1665547788143158
Loss_Reward : -0.21072909235954285
Loss_Discount : 0.025166839826852083
Loss_RawKL : 1.5733652710914612
mean_target : -1.0897604376077652
max_target : -0.6256258934736252
min_target : -1.4357921779155731
std_target : 0.2608208656311035
Done logging...

Current epsilon: 0.08208653107760414 at iteration 114688


********** Iteration 28 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1404, -0.1241, -0.1468, -0.1380, -0.1431, -0.1501, -0.1529, -0.1602,
        -0.1564, -0.1588, -0.1640, -0.1587, -0.1582, -0.1568, -0.1579],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8786, -1.8241, -1.7815, -1.7139, -1.6485, -1.5754, -1.4909, -1.3962,
        -1.2894, -1.1786, -1.0595, -0.9302, -0.7938, -0.6495], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8786, -1.8241, -1.7815, -1.7139, -1.6485, -1.5754, -1.4909, -1.3962,
        -1.2894, -1.1786, -1.0595, -0.9302, -0.7938, -0.6495], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0113, -0.0092,  0.0003, -0.0070,  0.0023, -0.0044,  0.0044, -0.0076,
         0.0197, -0.0407,  0.0117, -0.0312,  0.0055,  0.0302,  0.0310],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.5337, -0.5502, -0.5471, -0.5547, -0.5509, -0.5607, -0.5616, -0.5742,
        -0.5736, -0.6007, -0.5637, -0.5815, -0.5587, -0.5703], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.5337, -0.5502, -0.5471, -0.5547, -0.5509, -0.5607, -0.5616, -0.5742,
        -0.5736, -0.6007, -0.5637, -0.5815, -0.5587, -0.5703], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1057, -0.1057, -0.1014, -0.1020, -0.1017, -0.1006, -0.1073, -0.1038,
        -0.1036, -0.1030, -0.0995, -0.1036, -0.1054, -0.1057, -0.1020],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4714, -1.4278, -1.3787, -1.3314, -1.2801, -1.2214, -1.1656, -1.0986,
        -1.0307, -0.9602, -0.8846, -0.8072, -0.7277, -0.6344], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4714, -1.4278, -1.3787, -1.3314, -1.2801, -1.2214, -1.1656, -1.0986,
        -1.0307, -0.9602, -0.8846, -0.8072, -0.7277, -0.6344], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0817, -0.0577, -0.0290, -0.0591, -0.0391, -0.0088, -0.0121, -0.0160,
        -0.0283, -0.0154, -0.0032, -0.0035, -0.0428, -0.0265, -0.0133],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8498, -0.7914, -0.7558, -0.7410, -0.6958, -0.6650, -0.6684, -0.6674,
        -0.6641, -0.6453, -0.6368, -0.6451, -0.6537, -0.6201], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8498, -0.7914, -0.7558, -0.7410, -0.6958, -0.6650, -0.6684, -0.6674,
        -0.6641, -0.6453, -0.6368, -0.6451, -0.6537, -0.6201], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0903, 0.0784, 0.0918, 0.0765, 0.0800, 0.0789, 0.0792, 0.0842, 0.0795,
        0.0897, 0.0803, 0.0912], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([24, 20, 27, 23, 23, 19, 26, 26, 26, 26, 29, 31]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_28.gif
Eval_AverageReturn : -199.1999969482422
Eval_StdReturn : 234.83348083496094
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 184.60000610351562
Train_AverageReturn : -104.4375
Train_StdReturn : 221.49363708496094
Train_MaxReturn : 95.0
Train_MinReturn : -390.0
Train_AverageEpLen : 128.0
Train_EnvstepsSoFar : 118784.0
TimeSinceStart : 3152.2998609542847
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 14.101972818374634
Loss_Value : 0.8272359520196915
Loss_Entropy : 2.4824604392051697
Loss_Representation : -10.46034288406372
Loss_KL : 1.6126876771450043
Loss_Obs : -1.2247489094734192
Loss_Reward : 0.12541760283056647
Loss_Discount : 0.04904056526720524
Loss_RawKL : 1.6126876175403595
mean_target : -1.0817576199769974
max_target : -0.6786404699087143
min_target : -1.3987483382225037
std_target : 0.23112690821290016
Done logging...

Current epsilon: 0.07854288742389476 at iteration 118784


********** Iteration 29 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1485, -0.1412, -0.1417, -0.1539, -0.1444, -0.1548, -0.1483, -0.1499,
        -0.1489, -0.1520, -0.1522, -0.1553, -0.1527, -0.1539, -0.1496],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9184, -1.8511, -1.7884, -1.7219, -1.6408, -1.5639, -1.4720, -1.3783,
        -1.2748, -1.1660, -1.0502, -0.9246, -0.7851, -0.6448], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9184, -1.8511, -1.7884, -1.7219, -1.6408, -1.5639, -1.4720, -1.3783,
        -1.2748, -1.1660, -1.0502, -0.9246, -0.7851, -0.6448], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1525, -0.1539, -0.1505, -0.1544, -0.1597, -0.1615, -0.1563, -0.1574,
        -0.1577, -0.1597, -0.1594, -0.1477, -0.1564, -0.1632, -0.1602],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0007, -1.9390, -1.8710, -1.7965, -1.7166, -1.6251, -1.5256, -1.4275,
        -1.3224, -1.2129, -1.0936, -0.9663, -0.8460, -0.7016], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0007, -1.9390, -1.8710, -1.7965, -1.7166, -1.6251, -1.5256, -1.4275,
        -1.3224, -1.2129, -1.0936, -0.9663, -0.8460, -0.7016], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0540, -0.0590, -0.0659, -0.0531, -0.0555, -0.0450, -0.0561, -0.0676,
        -0.0580, -0.0561, -0.0659, -0.0424, -0.0526, -0.0513, -0.0592],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.0646, -1.0424, -1.0171, -0.9819, -0.9581, -0.9290, -0.9076, -0.8771,
        -0.8268, -0.7851, -0.7468, -0.7009, -0.6711, -0.6242], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.0646, -1.0424, -1.0171, -0.9819, -0.9581, -0.9290, -0.9076, -0.8771,
        -0.8268, -0.7851, -0.7468, -0.7009, -0.6711, -0.6242], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1400, -0.1354, -0.1368, -0.1410, -0.1420, -0.1354, -0.1443, -0.1401,
        -0.1392, -0.1396, -0.1440, -0.1424, -0.1409, -0.1455, -0.1425],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8477, -1.7889, -1.7250, -1.6590, -1.5855, -1.5048, -1.4238, -1.3367,
        -1.2439, -1.1450, -1.0404, -0.9220, -0.7986, -0.6687], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8477, -1.7889, -1.7250, -1.6590, -1.5855, -1.5048, -1.4238, -1.3367,
        -1.2439, -1.1450, -1.0404, -0.9220, -0.7986, -0.6687], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0897, 0.0778, 0.0917, 0.0777, 0.0811, 0.0787, 0.0775, 0.0848, 0.0810,
        0.0902, 0.0798, 0.0899], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([35, 17, 26, 24, 27, 20, 18, 33, 34, 18, 23, 25]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_29.gif
Eval_AverageReturn : -202.8000030517578
Eval_StdReturn : 224.56883239746094
Eval_MaxReturn : 93.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 189.6999969482422
Train_AverageReturn : -202.40908813476562
Train_StdReturn : 222.57473754882812
Train_MaxReturn : 95.0
Train_MinReturn : -390.0
Train_AverageEpLen : 186.18182373046875
Train_EnvstepsSoFar : 122880.0
TimeSinceStart : 3263.3704884052277
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 16.668256759643555
Loss_Value : 0.8598218560218811
Loss_Entropy : 2.4826276898384094
Loss_Representation : -10.917387008666992
Loss_KL : 1.249462366104126
Loss_Obs : -1.211403340101242
Loss_Reward : -0.08417871780693531
Loss_Discount : 0.031362975016236305
Loss_RawKL : 1.2494623363018036
mean_target : -1.2650687098503113
max_target : -0.7500994950532913
min_target : -1.650556892156601
std_target : 0.2903173640370369
Done logging...

Current epsilon: 0.07539060456621857 at iteration 122880
Saved model checkpoint to ./checkpoints/dreamerv2_basic_run1_iter_30.pt


********** Iteration 30 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1275, -0.1741, -0.1450, -0.1589, -0.1642, -0.1374, -0.1513, -0.1510,
        -0.1672, -0.1652, -0.1621, -0.1690, -0.1781, -0.1866, -0.1793],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0377, -2.0082, -1.9223, -1.8605, -1.7765, -1.6837, -1.6127, -1.5248,
        -1.4332, -1.3116, -1.1913, -1.0626, -0.9218, -0.7632], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0377, -2.0082, -1.9223, -1.8605, -1.7765, -1.6837, -1.6127, -1.5248,
        -1.4332, -1.3116, -1.1913, -1.0626, -0.9218, -0.7632], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1448, -0.1413, -0.1494, -0.1324, -0.1594, -0.1515, -0.1508, -0.1551,
        -0.1251, -0.1445, -0.1310, -0.1073, -0.1367, -0.1450, -0.1387],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9106, -1.8484, -1.7820, -1.7032, -1.6445, -1.5482, -1.4560, -1.3608,
        -1.2514, -1.1668, -1.0539, -0.9529, -0.8702, -0.7516], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9106, -1.8484, -1.7820, -1.7032, -1.6445, -1.5482, -1.4560, -1.3608,
        -1.2514, -1.1668, -1.0539, -0.9529, -0.8702, -0.7516], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0988, -0.0972, -0.1171, -0.0968, -0.1167, -0.0838, -0.0943, -0.0804,
        -0.0838, -0.0817, -0.0781, -0.0578, -0.0834, -0.0851, -0.0499],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4174, -1.3670, -1.3212, -1.2474, -1.1948, -1.1169, -1.0690, -1.0056,
        -0.9498, -0.8905, -0.8255, -0.7656, -0.7255, -0.6491], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4174, -1.3670, -1.3212, -1.2474, -1.1948, -1.1169, -1.0690, -1.0056,
        -0.9498, -0.8905, -0.8255, -0.7656, -0.7255, -0.6491], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1034, -0.1052, -0.1003, -0.1037, -0.1007, -0.0852, -0.0659, -0.0661,
        -0.0876, -0.0920, -0.0826, -0.0979, -0.0866, -0.1044, -0.1054],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4477, -1.3976, -1.3377, -1.2823, -1.2149, -1.1535, -1.1062, -1.0730,
        -1.0414, -0.9859, -0.9190, -0.8611, -0.7790, -0.7071], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4477, -1.3976, -1.3377, -1.2823, -1.2149, -1.1535, -1.1062, -1.0730,
        -1.0414, -0.9859, -0.9190, -0.8611, -0.7790, -0.7071], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0883, 0.0762, 0.0909, 0.0806, 0.0827, 0.0771, 0.0747, 0.0865, 0.0847,
        0.0923, 0.0774, 0.0886], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([28, 28, 23, 19, 21, 29, 19, 22, 28, 36, 22, 25]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_30.gif
Eval_AverageReturn : -107.5999984741211
Eval_StdReturn : 231.1476593017578
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 130.6999969482422
Train_AverageReturn : -295.76470947265625
Train_StdReturn : 177.4113006591797
Train_MaxReturn : 76.0
Train_MinReturn : -395.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 126976.0
TimeSinceStart : 3366.240402698517
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 17.880834102630615
Loss_Value : 0.8882398903369904
Loss_Entropy : 2.4821377396583557
Loss_Representation : -11.226930618286133
Loss_KL : 1.0989102125167847
Loss_Obs : -1.2045683562755585
Loss_Reward : -0.3034811057150364
Loss_Discount : 0.023323948495090008
Loss_RawKL : 1.0989102125167847
mean_target : -1.3516663908958435
max_target : -0.7960751950740814
min_target : -1.7884133160114288
std_target : 0.3193327710032463
Done logging...

Current epsilon: 0.07258646053091254 at iteration 126976


********** Iteration 31 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0920, -0.0851, -0.0961, -0.0863, -0.0885, -0.0915, -0.0909, -0.0953,
        -0.0888, -0.0932, -0.0871, -0.0928, -0.0917, -0.0803, -0.0731],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4321, -1.3974, -1.3664, -1.3194, -1.2807, -1.2310, -1.1766, -1.1235,
        -1.0631, -1.0042, -0.9385, -0.8675, -0.7885, -0.7065], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4321, -1.3974, -1.3664, -1.3194, -1.2807, -1.2310, -1.1766, -1.1235,
        -1.0631, -1.0042, -0.9385, -0.8675, -0.7885, -0.7065], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0937, -0.0759, -0.0710, -0.0737, -0.0939, -0.0757, -0.0772, -0.0879,
        -0.0896, -0.1011, -0.0971, -0.0911, -0.0838, -0.0784, -0.0897],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4051, -1.3604, -1.3326, -1.3109, -1.2869, -1.2346, -1.2013, -1.1571,
        -1.1034, -1.0483, -0.9769, -0.9065, -0.8392, -0.7702], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4051, -1.3604, -1.3326, -1.3109, -1.2869, -1.2346, -1.2013, -1.1571,
        -1.1034, -1.0483, -0.9769, -0.9065, -0.8392, -0.7702], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1186, -0.1142, -0.1089, -0.1132, -0.1119, -0.1106, -0.1147, -0.1117,
        -0.1192, -0.1121, -0.1158, -0.1073, -0.1186, -0.1140, -0.1231],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6254, -1.5722, -1.5252, -1.4711, -1.4179, -1.3516, -1.2835, -1.2115,
        -1.1362, -1.0533, -0.9611, -0.8624, -0.7650, -0.6506], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6254, -1.5722, -1.5252, -1.4711, -1.4179, -1.3516, -1.2835, -1.2115,
        -1.1362, -1.0533, -0.9611, -0.8624, -0.7650, -0.6506], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1448, -0.1435, -0.1393, -0.1404, -0.1453, -0.1411, -0.1447, -0.1367,
        -0.1420, -0.1400, -0.1398, -0.1440, -0.1368, -0.1427, -0.1415],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0281, -1.9596, -1.8904, -1.8270, -1.7542, -1.6720, -1.5900, -1.5004,
        -1.4117, -1.3087, -1.2090, -1.0969, -0.9761, -0.8529], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0281, -1.9596, -1.8904, -1.8270, -1.7542, -1.6720, -1.5900, -1.5004,
        -1.4117, -1.3087, -1.2090, -1.0969, -0.9761, -0.8529], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0868, 0.0746, 0.0897, 0.0834, 0.0842, 0.0759, 0.0728, 0.0874, 0.0871,
        0.0943, 0.0760, 0.0877], device='cuda:0')
Count of actions: (array([ 0,  1,  3,  5,  6,  7,  8,  9, 10, 11]), array([4, 4, 3, 3, 1, 5, 3, 2, 4, 5]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_31.gif
Eval_AverageReturn : -256.1000061035156
Eval_StdReturn : 203.73731994628906
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 221.89999389648438
Train_AverageReturn : -118.46666717529297
Train_StdReturn : 223.3317108154297
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 136.53334045410156
Train_EnvstepsSoFar : 131072.0
TimeSinceStart : 3476.980701684952
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 17.04833173751831
Loss_Value : 0.729177713394165
Loss_Entropy : 2.481365382671356
Loss_Representation : -11.086129665374756
Loss_KL : 0.9627029895782471
Loss_Obs : -1.2184293270111084
Loss_Reward : 0.09531060233712196
Loss_Discount : 0.040149988140910864
Loss_RawKL : 0.9618426710367203
mean_target : -1.2921794056892395
max_target : -0.8314159214496613
min_target : -1.6426555216312408
std_target : 0.26098671555519104
Done logging...

Current epsilon: 0.07009200678873188 at iteration 131072


********** Iteration 32 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1425, -0.1468, -0.1457, -0.1509, -0.1447, -0.1459, -0.1461, -0.1470,
        -0.1490, -0.1449, -0.1521, -0.1525, -0.1420, -0.1504, -0.1537],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0484, -1.9914, -1.9266, -1.8561, -1.7726, -1.6910, -1.6039, -1.5089,
        -1.4096, -1.3033, -1.1937, -1.0706, -0.9394, -0.8067], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0484, -1.9914, -1.9266, -1.8561, -1.7726, -1.6910, -1.6039, -1.5089,
        -1.4096, -1.3033, -1.1937, -1.0706, -0.9394, -0.8067], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1269, -0.1309, -0.1314, -0.1302, -0.1316, -0.1333, -0.1298, -0.1325,
        -0.1332, -0.1289, -0.1348, -0.1292, -0.1320, -0.1353, -0.1406],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9205, -1.8615, -1.7961, -1.7300, -1.6549, -1.5823, -1.4986, -1.4132,
        -1.3205, -1.2245, -1.1285, -1.0187, -0.9140, -0.7942], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9205, -1.8615, -1.7961, -1.7300, -1.6549, -1.5823, -1.4986, -1.4132,
        -1.3205, -1.2245, -1.1285, -1.0187, -0.9140, -0.7942], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1073, -0.0879, -0.1043, -0.0803, -0.0848, -0.0874, -0.0970, -0.0936,
        -0.1030, -0.1041, -0.0997, -0.0961, -0.0915, -0.1016, -0.0819],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5747, -1.5221, -1.4837, -1.4245, -1.3923, -1.3533, -1.3060, -1.2453,
        -1.1887, -1.1138, -1.0324, -0.9538, -0.8748, -0.7945], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5747, -1.5221, -1.4837, -1.4245, -1.3923, -1.3533, -1.3060, -1.2453,
        -1.1887, -1.1138, -1.0324, -0.9538, -0.8748, -0.7945], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0951, -0.0821, -0.0880, -0.0816, -0.0933, -0.0927, -0.0905, -0.1008,
        -0.0972, -0.0870, -0.0804, -0.1022, -0.1033, -0.0887, -0.0839],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5457, -1.4994, -1.4654, -1.4257, -1.3906, -1.3378, -1.2914, -1.2353,
        -1.1682, -1.1000, -1.0423, -0.9838, -0.8977, -0.8028], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5457, -1.4994, -1.4654, -1.4257, -1.3906, -1.3378, -1.2914, -1.2353,
        -1.1682, -1.1000, -1.0423, -0.9838, -0.8977, -0.8028], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0849, 0.0730, 0.0885, 0.0864, 0.0858, 0.0743, 0.0709, 0.0886, 0.0893,
        0.0968, 0.0745, 0.0871], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([33, 26, 18, 29, 19, 24, 23, 26, 29, 21, 26, 26]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_32.gif
Eval_AverageReturn : -199.5
Eval_StdReturn : 232.3864288330078
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 185.39999389648438
Train_AverageReturn : -231.1999969482422
Train_StdReturn : 216.6376190185547
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 135168.0
TimeSinceStart : 3587.601927280426
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 18.010260105133057
Loss_Value : 0.734791025519371
Loss_Entropy : 2.4795949459075928
Loss_Representation : -11.003493309020996
Loss_KL : 1.1778094470500946
Loss_Obs : -1.195464849472046
Loss_Reward : -0.25209418684244156
Loss_Discount : 0.025439962279051542
Loss_RawKL : 1.1778094470500946
mean_target : -1.360836237668991
max_target : -0.8939545452594757
min_target : -1.7103325724601746
std_target : 0.2628982476890087
Done logging...

Current epsilon: 0.06787304107458302 at iteration 135168


********** Iteration 33 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1052, -0.0832, -0.0104, -0.0166, -0.0689, -0.0771, -0.1000, -0.1004,
        -0.0957, -0.1280, -0.1109, -0.0906, -0.0980, -0.0992, -0.0694],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.4005, -1.3429, -1.3099, -1.3459, -1.3776, -1.3662, -1.3433, -1.2883,
        -1.2250, -1.1673, -1.0731, -0.9882, -0.9253, -0.8462], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.4005, -1.3429, -1.3099, -1.3459, -1.3776, -1.3662, -1.3433, -1.2883,
        -1.2250, -1.1673, -1.0731, -0.9882, -0.9253, -0.8462], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1324, -0.1409, -0.1421, -0.1419, -0.1469, -0.1362, -0.1442, -0.1474,
        -0.1384, -0.1401, -0.1328, -0.1335, -0.1388, -0.1509, -0.1329],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0774, -2.0259, -1.9656, -1.9004, -1.8336, -1.7485, -1.6753, -1.5908,
        -1.4916, -1.4022, -1.2984, -1.1947, -1.0861, -0.9609], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0774, -2.0259, -1.9656, -1.9004, -1.8336, -1.7485, -1.6753, -1.5908,
        -1.4916, -1.4022, -1.2984, -1.1947, -1.0861, -0.9609], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0409, -0.0603, -0.0484, -0.0325, -0.0741, -0.0347, -0.0385, -0.0320,
        -0.0200, -0.0227, -0.0637, -0.0260, -0.0434, -0.0098, -0.0853],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9881, -0.9720, -0.9345, -0.9064, -0.8934, -0.8329, -0.8155, -0.7903,
        -0.7694, -0.7639, -0.7533, -0.6964, -0.6813, -0.6427], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9881, -0.9720, -0.9345, -0.9064, -0.8934, -0.8329, -0.8155, -0.7903,
        -0.7694, -0.7639, -0.7533, -0.6964, -0.6813, -0.6427], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1122, -0.0975, -0.1057, -0.1131, -0.1098, -0.1132, -0.1133, -0.1129,
        -0.1067, -0.1052, -0.0902, -0.1169, -0.1177, -0.0961, -0.0977],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8093, -1.7643, -1.7298, -1.6830, -1.6221, -1.5618, -1.4917, -1.4224,
        -1.3497, -1.2773, -1.2007, -1.1337, -1.0369, -0.9351], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8093, -1.7643, -1.7298, -1.6830, -1.6221, -1.5618, -1.4917, -1.4224,
        -1.3497, -1.2773, -1.2007, -1.1337, -1.0369, -0.9351], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0822, 0.0701, 0.0869, 0.0915, 0.0881, 0.0696, 0.0667, 0.0917, 0.0935,
        0.1017, 0.0706, 0.0874], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([17, 24, 24, 27, 28, 19, 20, 36, 23, 35, 20, 27]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_33.gif
Eval_AverageReturn : -297.6000061035156
Eval_StdReturn : 183.9506378173828
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 245.3000030517578
Train_AverageReturn : -235.0
Train_StdReturn : 212.554931640625
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 139264.0
TimeSinceStart : 3706.39240527153
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 18.380309104919434
Loss_Value : 0.7064844369888306
Loss_Entropy : 2.4745916724205017
Loss_Representation : -11.591288089752197
Loss_KL : 1.006521463394165
Loss_Obs : -1.2377778589725494
Loss_Reward : -0.2466384582221508
Loss_Discount : 0.02660790178924799
Loss_RawKL : 1.0065214484930038
mean_target : -1.387117087841034
max_target : -0.9369129836559296
min_target : -1.737986981868744
std_target : 0.25747235864400864
Done logging...

Current epsilon: 0.06589913842916303 at iteration 139264


********** Iteration 34 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1144, -0.1360, -0.1142, -0.1202, -0.1362, -0.1194, -0.1208, -0.1162,
        -0.1279, -0.1194, -0.1075, -0.1277, -0.1200, -0.1094, -0.1189],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9359, -1.8904, -1.8239, -1.7755, -1.7136, -1.6318, -1.5649, -1.4922,
        -1.4230, -1.3338, -1.2505, -1.1651, -1.0608, -0.9530], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9359, -1.8904, -1.8239, -1.7755, -1.7136, -1.6318, -1.5649, -1.4922,
        -1.4230, -1.3338, -1.2505, -1.1651, -1.0608, -0.9530], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1340, -0.1358, -0.1355, -0.1476, -0.1531, -0.1433, -0.1406, -0.1357,
        -0.1354, -0.1494, -0.1337, -0.1351, -0.1392, -0.1442, -0.1329],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.1200, -2.0653, -2.0079, -1.9426, -1.8624, -1.7719, -1.6816, -1.5909,
        -1.5032, -1.4076, -1.2944, -1.1860, -1.0706, -0.9468], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.1200, -2.0653, -2.0079, -1.9426, -1.8624, -1.7719, -1.6816, -1.5909,
        -1.5032, -1.4076, -1.2944, -1.1860, -1.0706, -0.9468], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0805, -0.1309, -0.1293, -0.1272, -0.1431, -0.1407, -0.1134, -0.1320,
        -0.1115, -0.0989, -0.1308, -0.1321, -0.1220, -0.1390, -0.1328],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9640, -1.9567, -1.8946, -1.8367, -1.7722, -1.6926, -1.6126, -1.5552,
        -1.4727, -1.4021, -1.3436, -1.2522, -1.1537, -1.0588], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9640, -1.9567, -1.8946, -1.8367, -1.7722, -1.6926, -1.6126, -1.5552,
        -1.4727, -1.4021, -1.3436, -1.2522, -1.1537, -1.0588], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0895, -0.0906, -0.0429, -0.0118, -0.0662, -0.0737, -0.0687, -0.0885,
        -0.1389, -0.1412, -0.1363, -0.1331, -0.1297, -0.1384, -0.1264],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6076, -1.5706, -1.5319, -1.5410, -1.5853, -1.5914, -1.5829, -1.5765,
        -1.5518, -1.4736, -1.3798, -1.2772, -1.1740, -1.0717], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6076, -1.5706, -1.5319, -1.5410, -1.5853, -1.5914, -1.5829, -1.5765,
        -1.5518, -1.4736, -1.3798, -1.2772, -1.1740, -1.0717], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0797, 0.0680, 0.0854, 0.0946, 0.0893, 0.0663, 0.0642, 0.0947, 0.0941,
        0.1059, 0.0685, 0.0892], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([25, 15, 24, 30, 34, 16, 21, 28, 30, 35, 22, 20]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_34.gif
Eval_AverageReturn : -256.20001220703125
Eval_StdReturn : 208.52760314941406
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 219.5
Train_AverageReturn : -192.04347229003906
Train_StdReturn : 227.8245849609375
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 178.0869598388672
Train_EnvstepsSoFar : 143360.0
TimeSinceStart : 3821.1037652492523
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 20.970001697540283
Loss_Value : 0.7382432669401169
Loss_Entropy : 2.4703069925308228
Loss_Representation : -12.056175708770752
Loss_KL : 1.0257126837968826
Loss_Obs : -1.2778401672840118
Loss_Reward : -0.34293967112898827
Loss_Discount : 0.03945306595414877
Loss_RawKL : 1.0257126688957214
mean_target : -1.571968287229538
max_target : -1.0298892259597778
min_target : -1.9910852909088135
std_target : 0.30896634608507156
Done logging...

Current epsilon: 0.06414323403246508 at iteration 143360


********** Iteration 35 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0522, -0.0773, -0.0226, -0.0330, -0.0166, -0.0025, -0.0580,  0.0047,
         0.0026, -0.0201, -0.0633, -0.0017, -0.0315, -0.0080, -0.0213],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.9879, -0.9493, -0.8871, -0.8785, -0.8612, -0.8572, -0.8757, -0.8294,
        -0.8446, -0.8581, -0.8512, -0.8008, -0.8111, -0.7885], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.9879, -0.9493, -0.8871, -0.8785, -0.8612, -0.8572, -0.8757, -0.8294,
        -0.8446, -0.8581, -0.8512, -0.8008, -0.8111, -0.7885], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0676, -0.0790, -0.0652, -0.0545, -0.0826, -0.0618, -0.0771, -0.0664,
        -0.0865, -0.0829, -0.0891, -0.0734, -0.0657, -0.0622, -0.0888],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5126, -1.4876, -1.4450, -1.4192, -1.4002, -1.3503, -1.3224, -1.2774,
        -1.2404, -1.1714, -1.1037, -1.0285, -0.9665, -0.9108], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5126, -1.4876, -1.4450, -1.4192, -1.4002, -1.3503, -1.3224, -1.2774,
        -1.2404, -1.1714, -1.1037, -1.0285, -0.9665, -0.9108], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1643, -0.1530, -0.1688, -0.1602, -0.1729, -0.1688, -0.1584, -0.1604,
        -0.1693, -0.1676, -0.1578, -0.1534, -0.1665, -0.1545, -0.1469],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.4351, -2.3592, -2.2947, -2.2066, -2.1247, -2.0268, -1.9259, -1.8338,
        -1.7257, -1.6043, -1.4783, -1.3578, -1.2287, -1.0805], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.4351, -2.3592, -2.2947, -2.2066, -2.1247, -2.0268, -1.9259, -1.8338,
        -1.7257, -1.6043, -1.4783, -1.3578, -1.2287, -1.0805], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1544, -0.1347, -0.1228, -0.1257, -0.1352, -0.1222, -0.1144, -0.1497,
        -0.1407, -0.1440, -0.1266, -0.1362, -0.1271, -0.1243, -0.1299],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.1850, -2.1223, -2.0652, -2.0149, -1.9563, -1.8827, -1.8185, -1.7560,
        -1.6512, -1.5557, -1.4562, -1.3601, -1.2455, -1.1374], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.1850, -2.1223, -2.0652, -2.0149, -1.9563, -1.8827, -1.8185, -1.7560,
        -1.6512, -1.5557, -1.4562, -1.3601, -1.2455, -1.1374], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0759, 0.0649, 0.0831, 0.0993, 0.0902, 0.0614, 0.0604, 0.0994, 0.0959,
        0.1119, 0.0651, 0.0925], device='cuda:0')
Count of actions: (array([ 0,  1,  4,  9, 11]), array([1, 1, 1, 1, 2]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_35.gif
Eval_AverageReturn : -164.89999389648438
Eval_StdReturn : 229.1032257080078
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 165.39999389648438
Train_AverageReturn : -214.6666717529297
Train_StdReturn : 212.56134033203125
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 147456.0
TimeSinceStart : 3922.4025218486786
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 21.836819648742676
Loss_Value : 0.7267212867736816
Loss_Entropy : 2.4604673981666565
Loss_Representation : -11.34613299369812
Loss_KL : 1.5935770571231842
Loss_Obs : -1.2685171067714691
Loss_Reward : -0.2837742045521736
Loss_Discount : 0.029234728775918484
Loss_RawKL : 1.5935769975185394
mean_target : -1.6335901319980621
max_target : -1.0965849161148071
min_target : -2.0413935780525208
std_target : 0.3042069040238857
Done logging...

Current epsilon: 0.06258125210924455 at iteration 147456


********** Iteration 36 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0573, -0.0609, -0.0609, -0.0853, -0.0727, -0.0905, -0.0397, -0.0676,
        -0.0903, -0.1096, -0.0736, -0.0892, -0.0819, -0.0672, -0.0544],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6039, -1.5919, -1.5703, -1.5504, -1.5097, -1.4748, -1.4167, -1.4135,
        -1.3840, -1.3199, -1.2351, -1.1820, -1.1127, -1.0403], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6039, -1.5919, -1.5703, -1.5504, -1.5097, -1.4748, -1.4167, -1.4135,
        -1.3840, -1.3199, -1.2351, -1.1820, -1.1127, -1.0403], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0317, -0.0782, -0.0837, -0.1084, -0.1448, -0.1561, -0.1621, -0.1686,
        -0.1897, -0.1532, -0.1624, -0.1696, -0.1434, -0.1531, -0.1305],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0102, -2.0565, -2.0628, -2.0642, -2.0550, -1.9950, -1.9190, -1.8273,
        -1.7148, -1.5751, -1.4663, -1.3394, -1.2006, -1.0752], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0102, -2.0565, -2.0628, -2.0642, -2.0550, -1.9950, -1.9190, -1.8273,
        -1.7148, -1.5751, -1.4663, -1.3394, -1.2006, -1.0752], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1694, -0.1660, -0.1615, -0.1711, -0.1647, -0.1643, -0.1669, -0.1639,
        -0.1597, -0.1582, -0.1593, -0.1612, -0.1617, -0.1620, -0.1685],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.4912, -2.4197, -2.3443, -2.2667, -2.1758, -2.0802, -1.9827, -1.8761,
        -1.7686, -1.6610, -1.5449, -1.4219, -1.2850, -1.1398], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.4912, -2.4197, -2.3443, -2.2667, -2.1758, -2.0802, -1.9827, -1.8761,
        -1.7686, -1.6610, -1.5449, -1.4219, -1.2850, -1.1398], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0843, -0.0460, -0.0653, -0.0485, -0.0289, -0.0133, -0.0264, -0.0491,
        -0.0281, -0.0220, -0.0444, -0.0016, -0.0155, -0.0565, -0.0354],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.3355, -1.2754, -1.2500, -1.2059, -1.1750, -1.1602, -1.1694, -1.1609,
        -1.1258, -1.1034, -1.0908, -1.0560, -1.0625, -1.0510], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.3355, -1.2754, -1.2500, -1.2059, -1.1750, -1.1602, -1.1694, -1.1609,
        -1.1258, -1.1034, -1.0908, -1.0560, -1.0625, -1.0510], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0728, 0.0629, 0.0806, 0.1012, 0.0889, 0.0587, 0.0586, 0.1041, 0.0948,
        0.1166, 0.0636, 0.0971], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([16, 20, 32, 28, 29, 15, 15, 36, 25, 30, 14, 40]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_36.gif
Eval_AverageReturn : -21.299999237060547
Eval_StdReturn : 185.31976318359375
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 80.0999984741211
Train_AverageReturn : -153.57691955566406
Train_StdReturn : 222.88841247558594
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 157.53846740722656
Train_EnvstepsSoFar : 151552.0
TimeSinceStart : 4017.5169155597687
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 22.25429916381836
Loss_Value : 0.7375987768173218
Loss_Entropy : 2.451036810874939
Loss_Representation : -11.068405389785767
Loss_KL : 1.9285410940647125
Loss_Obs : -1.2865382730960846
Loss_Reward : -0.16847325954586267
Loss_Discount : 0.03690948337316513
Loss_RawKL : 1.9285410642623901
mean_target : -1.6631283164024353
max_target : -1.1699038445949554
min_target : -2.0451948046684265
std_target : 0.28118768334388733
Done logging...

Current epsilon: 0.061191775818248396 at iteration 151552


********** Iteration 37 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1003, -0.1025, -0.0884, -0.0855, -0.0861, -0.1002, -0.0822, -0.0903,
        -0.0916, -0.0910, -0.0838, -0.1023, -0.0897, -0.0942, -0.0797],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7204, -1.6739, -1.6305, -1.5940, -1.5536, -1.5165, -1.4588, -1.4152,
        -1.3651, -1.3026, -1.2417, -1.1880, -1.1147, -1.0485], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7204, -1.6739, -1.6305, -1.5940, -1.5536, -1.5165, -1.4588, -1.4152,
        -1.3651, -1.3026, -1.2417, -1.1880, -1.1147, -1.0485], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1089, -0.1054, -0.1093, -0.1141, -0.1213, -0.1187, -0.1089, -0.1162,
        -0.1113, -0.0955, -0.1156, -0.1136, -0.1145, -0.1173, -0.1103],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9817, -1.9434, -1.9047, -1.8604, -1.8068, -1.7455, -1.6870, -1.6259,
        -1.5592, -1.4911, -1.4369, -1.3533, -1.2661, -1.1688], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9817, -1.9434, -1.9047, -1.8604, -1.8068, -1.7455, -1.6870, -1.6259,
        -1.5592, -1.4911, -1.4369, -1.3533, -1.2661, -1.1688], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1360, -0.1353, -0.1057, -0.1393, -0.1121, -0.1296, -0.1164, -0.1221,
        -0.0550, -0.0663, -0.1356, -0.1028, -0.1223, -0.0641, -0.1099],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0033, -1.9323, -1.8581, -1.8122, -1.7292, -1.6720, -1.5927, -1.5184,
        -1.4315, -1.4131, -1.3839, -1.2841, -1.2086, -1.1071], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0033, -1.9323, -1.8581, -1.8122, -1.7292, -1.6720, -1.5927, -1.5184,
        -1.4315, -1.4131, -1.3839, -1.2841, -1.2086, -1.1071], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1575, -0.1618, -0.1604, -0.1546, -0.1580, -0.1541, -0.1578, -0.1561,
        -0.1576, -0.1640, -0.1580, -0.1576, -0.1565, -0.1599, -0.1587],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.5078, -2.4427, -2.3702, -2.3004, -2.2194, -2.1322, -2.0429, -1.9538,
        -1.8506, -1.7399, -1.6212, -1.4944, -1.3678, -1.2222], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.5078, -2.4427, -2.3702, -2.3004, -2.2194, -2.1322, -2.0429, -1.9538,
        -1.8506, -1.7399, -1.6212, -1.4944, -1.3678, -1.2222], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0684, 0.0598, 0.0767, 0.1042, 0.0868, 0.0542, 0.0554, 0.1112, 0.0939,
        0.1234, 0.0607, 0.1051], device='cuda:0')
Count of actions: (array([ 2,  3,  4,  5,  6,  7,  9, 10, 11]), array([3, 4, 2, 1, 2, 2, 7, 2, 4]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_37.gif
Eval_AverageReturn : -252.10000610351562
Eval_StdReturn : 213.01194763183594
Eval_MaxReturn : 93.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 216.89999389648438
Train_AverageReturn : -140.9629669189453
Train_StdReturn : 216.53184509277344
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 151.70370483398438
Train_EnvstepsSoFar : 155648.0
TimeSinceStart : 4126.733515024185
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 22.445223808288574
Loss_Value : 0.6062149405479431
Loss_Entropy : 2.4342066049575806
Loss_Representation : -11.11696481704712
Loss_KL : 2.011982649564743
Loss_Obs : -1.3116492629051208
Loss_Reward : -0.05581886973232031
Loss_Discount : 0.04336422495543957
Loss_RawKL : 2.0119826197624207
mean_target : -1.6762596666812897
max_target : -1.2337502241134644
min_target : -2.014616698026657
std_target : 0.25111135840415955
Done logging...

Current epsilon: 0.05995575359895166 at iteration 155648


********** Iteration 38 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0972, -0.1342, -0.0977, -0.1278, -0.0335, -0.1036, -0.1358, -0.1211,
        -0.1311, -0.0831, -0.1138, -0.1212, -0.1429, -0.1315, -0.1269],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9799, -1.9526, -1.8935, -1.8678, -1.8071, -1.8412, -1.8060, -1.7282,
        -1.6591, -1.5744, -1.5342, -1.4633, -1.3753, -1.2552], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9799, -1.9526, -1.8935, -1.8678, -1.8071, -1.8412, -1.8060, -1.7282,
        -1.6591, -1.5744, -1.5342, -1.4633, -1.3753, -1.2552], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0484, -0.1054, -0.0498, -0.0910, -0.0683, -0.0977, -0.0950, -0.0874,
        -0.0956, -0.0938, -0.0911, -0.0927, -0.0409, -0.0956, -0.0912],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7224, -1.7261, -1.6781, -1.6760, -1.6411, -1.6250, -1.5680, -1.5189,
        -1.4643, -1.4033, -1.3378, -1.2731, -1.2072, -1.1829], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7224, -1.7261, -1.6781, -1.6760, -1.6411, -1.6250, -1.5680, -1.5189,
        -1.4643, -1.4033, -1.3378, -1.2731, -1.2072, -1.1829], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0066, -0.0509, -0.0556, -0.0645, -0.0699, -0.0734, -0.0975, -0.0907,
        -0.0983, -0.0963, -0.0918, -0.0930, -0.0998, -0.0782, -0.1000],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6247, -1.6680, -1.6753, -1.6952, -1.6878, -1.6712, -1.6510, -1.6020,
        -1.5550, -1.5115, -1.4479, -1.3922, -1.3269, -1.2440], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6247, -1.6680, -1.6753, -1.6952, -1.6878, -1.6712, -1.6510, -1.6020,
        -1.5550, -1.5115, -1.4479, -1.3922, -1.3269, -1.2440], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1373, -0.1329, -0.1325, -0.1285, -0.1332, -0.1324, -0.1322, -0.1319,
        -0.1349, -0.1348, -0.1317, -0.1324, -0.1343, -0.1310, -0.1348],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.3567, -2.3042, -2.2486, -2.1827, -2.1258, -2.0585, -1.9926, -1.9102,
        -1.8264, -1.7353, -1.6401, -1.5465, -1.4414, -1.3238], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.3567, -2.3042, -2.2486, -2.1827, -2.1258, -2.0585, -1.9926, -1.9102,
        -1.8264, -1.7353, -1.6401, -1.5465, -1.4414, -1.3238], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0625, 0.0558, 0.0714, 0.1088, 0.0842, 0.0479, 0.0506, 0.1208, 0.0917,
        0.1321, 0.0563, 0.1179], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([28, 24, 20, 21, 20, 15, 18, 32, 24, 47, 18, 33]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_38.gif
Eval_AverageReturn : -111.80000305175781
Eval_StdReturn : 230.9397430419922
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 132.39999389648438
Train_AverageReturn : -102.84375
Train_StdReturn : 213.1459503173828
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 128.0
Train_EnvstepsSoFar : 159744.0
TimeSinceStart : 4230.048871517181
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 23.264068603515625
Loss_Value : 0.514931783080101
Loss_Entropy : 2.4091196060180664
Loss_Representation : -12.079309225082397
Loss_KL : 1.444163978099823
Loss_Obs : -1.3312409222126007
Loss_Reward : -0.24994082003831863
Loss_Discount : 0.038876754231750965
Loss_RawKL : 1.4441639184951782
mean_target : -1.7339884638786316
max_target : -1.3131778836250305
min_target : -2.053660273551941
std_target : 0.23823412135243416
Done logging...

Current epsilon: 0.05885623794942593 at iteration 159744


********** Iteration 39 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0946, -0.0886, -0.0966, -0.0920, -0.0957, -0.0889, -0.0985, -0.0997,
        -0.0963, -0.0990, -0.0972, -0.0915, -0.1061, -0.1001, -0.0920],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.0193, -1.9775, -1.9461, -1.8979, -1.8496, -1.8018, -1.7559, -1.7025,
        -1.6434, -1.5775, -1.5022, -1.4265, -1.3533, -1.2567], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.0193, -1.9775, -1.9461, -1.8979, -1.8496, -1.8018, -1.7559, -1.7025,
        -1.6434, -1.5775, -1.5022, -1.4265, -1.3533, -1.2567], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1159, -0.1151, -0.1090, -0.1209, -0.1125, -0.1085, -0.1093, -0.1168,
        -0.1196, -0.1185, -0.1206, -0.1154, -0.1136, -0.1153, -0.1158],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.2702, -2.2300, -2.1847, -2.1458, -2.0842, -2.0224, -1.9633, -1.8963,
        -1.8153, -1.7311, -1.6495, -1.5562, -1.4607, -1.3639], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.2702, -2.2300, -2.1847, -2.1458, -2.0842, -2.0224, -1.9633, -1.8963,
        -1.8153, -1.7311, -1.6495, -1.5562, -1.4607, -1.3639], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1147, -0.1162, -0.1163, -0.1264, -0.1097, -0.1069, -0.1107, -0.1113,
        -0.1151, -0.1095, -0.1108, -0.1130, -0.1135, -0.1229, -0.1170],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.3549, -2.3184, -2.2741, -2.2258, -2.1659, -2.1097, -2.0567, -1.9913,
        -1.9261, -1.8532, -1.7766, -1.7029, -1.6140, -1.5251], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.3549, -2.3184, -2.2741, -2.2258, -2.1659, -2.1097, -2.0567, -1.9913,
        -1.9261, -1.8532, -1.7766, -1.7029, -1.6140, -1.5251], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0525, -0.0110, -0.0874, -0.0884, -0.0202, -0.0844, -0.0569, -0.0294,
        -0.0540, -0.0296, -0.0365, -0.0810, -0.0911, -0.0828, -0.0057],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.5678, -1.5618, -1.5918, -1.5436, -1.4953, -1.5037, -1.4578, -1.4347,
        -1.4411, -1.4212, -1.4300, -1.4322, -1.3852, -1.3281], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.5678, -1.5618, -1.5918, -1.5436, -1.4953, -1.5037, -1.4578, -1.4347,
        -1.4411, -1.4212, -1.4300, -1.4322, -1.3852, -1.3281], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0608, 0.0558, 0.0692, 0.1076, 0.0814, 0.0493, 0.0517, 0.1236, 0.0851,
        0.1316, 0.0573, 0.1267], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([15, 16, 21, 27, 25, 13, 13, 37, 36, 43, 17, 37]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_39.gif
Eval_AverageReturn : -157.6999969482422
Eval_StdReturn : 236.57432556152344
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 159.1999969482422
Train_AverageReturn : -188.0869598388672
Train_StdReturn : 226.14443969726562
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 178.0869598388672
Train_EnvstepsSoFar : 163840.0
TimeSinceStart : 4336.034016132355
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 24.711912155151367
Loss_Value : 0.45745907723903656
Loss_Entropy : 2.3937928676605225
Loss_Representation : -12.51419711112976
Loss_KL : 1.3786401748657227
Loss_Obs : -1.3300322890281677
Loss_Reward : -0.6220132261514664
Loss_Discount : 0.02949880389496684
Loss_RawKL : 1.3786401450634003
mean_target : -1.8369586765766144
max_target : -1.436815470457077
min_target : -2.1405866146087646
std_target : 0.22690391913056374
Done logging...

Current epsilon: 0.057878153053638345 at iteration 163840


********** Iteration 40 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0471, -0.0969, -0.0936, -0.0900, -0.0653, -0.0878, -0.0704, -0.0939,
        -0.1156, -0.0485, -0.1079, -0.0946, -0.1126, -0.1217, -0.1059],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8941, -1.8957, -1.8688, -1.8331, -1.7934, -1.7816, -1.7372, -1.7093,
        -1.6632, -1.5936, -1.5779, -1.5018, -1.4313, -1.3417], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8941, -1.8957, -1.8688, -1.8331, -1.7934, -1.7816, -1.7372, -1.7093,
        -1.6632, -1.5936, -1.5779, -1.5018, -1.4313, -1.3417], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0981, -0.1048, -0.1068, -0.1144, -0.1048, -0.0964, -0.1014, -0.1066,
        -0.0988, -0.1105, -0.1097, -0.1025, -0.0934, -0.0970, -0.0922],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.2604, -2.2249, -2.1782, -2.1324, -2.0708, -2.0166, -1.9603, -1.8923,
        -1.8274, -1.7603, -1.6738, -1.5949, -1.5146, -1.4373], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.2604, -2.2249, -2.1782, -2.1324, -2.0708, -2.0166, -1.9603, -1.8923,
        -1.8274, -1.7603, -1.6738, -1.5949, -1.5146, -1.4373], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([ 0.0188,  0.0393,  0.0401,  0.0074,  0.0840,  0.0072,  0.0419,  0.0617,
         0.0096,  0.0159, -0.0152, -0.0094, -0.0360, -0.0647, -0.0016],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8059, -0.8154, -0.8506, -0.8934, -0.9057, -0.9934, -1.0110, -1.0722,
        -1.1456, -1.1846, -1.2205, -1.2306, -1.2421, -1.2339], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8059, -0.8154, -0.8506, -0.8934, -0.9057, -0.9934, -1.0110, -1.0722,
        -1.1456, -1.1846, -1.2205, -1.2306, -1.2421, -1.2339], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1961, -0.1950, -0.1927, -0.1919, -0.1929, -0.1962, -0.1949, -0.1920,
        -0.1906, -0.1878, -0.1933, -0.1908, -0.1898, -0.1906, -0.1877],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.2458, -3.1546, -3.0651, -2.9680, -2.8659, -2.7645, -2.6501, -2.5300,
        -2.4018, -2.2722, -2.1379, -1.9912, -1.8380, -1.6704], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.2458, -3.1546, -3.0651, -2.9680, -2.8659, -2.7645, -2.6501, -2.5300,
        -2.4018, -2.2722, -2.1379, -1.9912, -1.8380, -1.6704], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0529, 0.0493, 0.0607, 0.1124, 0.0789, 0.0411, 0.0452, 0.1400, 0.0788,
        0.1422, 0.0507, 0.1477], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([13, 20, 16, 39, 23, 16, 16, 37, 20, 33, 26, 41]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_40.gif
Eval_AverageReturn : -155.60000610351562
Eval_StdReturn : 235.62945556640625
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 158.60000610351562
Train_AverageReturn : -113.48387145996094
Train_StdReturn : 232.69509887695312
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 132.1290283203125
Train_EnvstepsSoFar : 167936.0
TimeSinceStart : 4442.4448800086975
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 25.5232572555542
Loss_Value : 0.8023246079683304
Loss_Entropy : 2.344704270362854
Loss_Representation : -10.992780685424805
Loss_KL : 2.563343644142151
Loss_Obs : -1.3268980979919434
Loss_Reward : -0.3123770635575056
Loss_Discount : 0.025233006104826927
Loss_RawKL : 2.563343644142151
mean_target : -1.8934138119220734
max_target : -1.4932437241077423
min_target : -2.207532823085785
std_target : 0.2304893434047699
Done logging...

Current epsilon: 0.05700808807204354 at iteration 167936


********** Iteration 41 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1602, -0.1557, -0.1479, -0.1488, -0.1328, -0.1133, -0.1534, -0.1527,
        -0.1328, -0.1564, -0.1560, -0.1454, -0.1363, -0.1485, -0.1444],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.6707, -2.5938, -2.5150, -2.4394, -2.3600, -2.2914, -2.2391, -2.1544,
        -2.0555, -1.9729, -1.8557, -1.7326, -1.6030, -1.4844], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.6707, -2.5938, -2.5150, -2.4394, -2.3600, -2.2914, -2.2391, -2.1544,
        -2.0555, -1.9729, -1.8557, -1.7326, -1.6030, -1.4844], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0205, -0.0341, -0.0146, -0.0322, -0.0248, -0.0443, -0.0468, -0.0274,
        -0.0606, -0.0160, -0.0358, -0.0561, -0.0410, -0.0372, -0.0350],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.6898, -1.6946, -1.6795, -1.6947, -1.6884, -1.6877, -1.6723, -1.6500,
        -1.6493, -1.6069, -1.6122, -1.5875, -1.5569, -1.5344], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.6898, -1.6946, -1.6795, -1.6947, -1.6884, -1.6877, -1.6723, -1.6500,
        -1.6493, -1.6069, -1.6122, -1.5875, -1.5569, -1.5344], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0568, -0.0338, -0.0711, -0.0547, -0.0611, -0.0372, -0.0864, -0.0865,
        -0.0833, -0.0991, -0.0850, -0.1011, -0.0985, -0.0473, -0.0522],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.8581, -1.8404, -1.8376, -1.7998, -1.7781, -1.7434, -1.7390, -1.6766,
        -1.6140, -1.5547, -1.4729, -1.3983, -1.3118, -1.2263], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.8581, -1.8404, -1.8376, -1.7998, -1.7781, -1.7434, -1.7390, -1.6766,
        -1.6140, -1.5547, -1.4729, -1.3983, -1.3118, -1.2263], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0065,  0.0291,  0.0740,  0.0876,  0.1053,  0.1064,  0.1116,  0.1184,
         0.0983,  0.1262,  0.1492,  0.1613,  0.0901,  0.1602,  0.1890],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.1165, -0.0527, -0.0254, -0.0431, -0.0783, -0.1371, -0.2014, -0.2711,
        -0.3587, -0.4275, -0.5301, -0.6668, -0.8225, -1.0080], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.1165, -0.0527, -0.0254, -0.0431, -0.0783, -0.1371, -0.2014, -0.2711,
        -0.3587, -0.4275, -0.5301, -0.6668, -0.8225, -1.0080], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0462, 0.0430, 0.0521, 0.1144, 0.0785, 0.0358, 0.0407, 0.1608, 0.0689,
        0.1479, 0.0449, 0.1669], device='cuda:0')
Count of actions: (array([ 3,  8,  9, 11]), array([1, 2, 3, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_41.gif
Eval_AverageReturn : -253.1999969482422
Eval_StdReturn : 219.00265502929688
Eval_MaxReturn : 94.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 215.0
Train_AverageReturn : -165.16000366210938
Train_StdReturn : 229.9578399658203
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 163.83999633789062
Train_EnvstepsSoFar : 172032.0
TimeSinceStart : 4550.968257665634
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 20.85665798187256
Loss_Value : 1.0153845697641373
Loss_Entropy : 2.278770625591278
Loss_Representation : -9.226089715957642
Loss_KL : 2.688277304172516
Loss_Obs : -1.2954188883304596
Loss_Reward : 1.0016240943223238
Loss_Discount : 0.03819748014211655
Loss_RawKL : 2.688277304172516
mean_target : -1.5580229759216309
max_target : -1.41703462600708
min_target : -1.7141270339488983
std_target : 0.09818565100431442
Done logging...

Current epsilon: 0.05623411326120875 at iteration 172032


********** Iteration 42 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1752, -0.1800, -0.1836, -0.1799, -0.1679, -0.1822, -0.1778, -0.1624,
        -0.1324, -0.1179, -0.1169, -0.1498, -0.1178, -0.1200, -0.1100],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.8442, -2.7526, -2.6580, -2.5449, -2.4336, -2.3260, -2.2008, -2.0746,
        -1.9517, -1.8563, -1.7762, -1.6838, -1.5571, -1.4546], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.8442, -2.7526, -2.6580, -2.5449, -2.4336, -2.3260, -2.2008, -2.0746,
        -1.9517, -1.8563, -1.7762, -1.6838, -1.5571, -1.4546], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2283, -0.2129, -0.2035, -0.2155, -0.2135, -0.2168, -0.2201, -0.2132,
        -0.2165, -0.2141, -0.1894, -0.1960, -0.2173, -0.2038, -0.1814],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4129, -3.2969, -3.1916, -3.0789, -2.9569, -2.8368, -2.6998, -2.5520,
        -2.4031, -2.2392, -2.0699, -1.9206, -1.7500, -1.5572], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4129, -3.2969, -3.1916, -3.0789, -2.9569, -2.8368, -2.6998, -2.5520,
        -2.4031, -2.2392, -2.0699, -1.9206, -1.7500, -1.5572], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0148,  0.0990,  0.0995,  0.1335,  0.1215,  0.0870,  0.0416, -0.0513,
         0.0088,  0.0070, -0.0086, -0.1315, -0.1512, -0.1015, -0.1401],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-0.8901, -0.8642, -0.9524, -1.0567, -1.1982, -1.3407, -1.4490, -1.5146,
        -1.4938, -1.5317, -1.5721, -1.5825, -1.4751, -1.3399], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-0.8901, -0.8642, -0.9524, -1.0567, -1.1982, -1.3407, -1.4490, -1.5146,
        -1.4938, -1.5317, -1.5721, -1.5825, -1.4751, -1.3399], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.2013, -0.1925, -0.2018, -0.2200, -0.2199, -0.2139, -0.2204, -0.2201,
        -0.2063, -0.2237, -0.2168, -0.2142, -0.2087, -0.2171, -0.2234],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4922, -3.4129, -3.3350, -3.2489, -3.1260, -2.9979, -2.8740, -2.7391,
        -2.5863, -2.4392, -2.2709, -2.0995, -1.9250, -1.7319], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4922, -3.4129, -3.3350, -3.2489, -3.1260, -2.9979, -2.8740, -2.7391,
        -2.5863, -2.4392, -2.2709, -2.0995, -1.9250, -1.7319], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0479, 0.0442, 0.0521, 0.1118, 0.0877, 0.0385, 0.0437, 0.1683, 0.0665,
        0.1409, 0.0468, 0.1517], device='cuda:0')
Count of actions: (array([ 3,  7, 11]), array([1, 4, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_42.gif
Eval_AverageReturn : -59.70000076293945
Eval_StdReturn : 218.95159912109375
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 100.4000015258789
Train_AverageReturn : -202.59091186523438
Train_StdReturn : 228.4564208984375
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 186.18182373046875
Train_EnvstepsSoFar : 176128.0
TimeSinceStart : 4643.427981138229
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 32.61300086975098
Loss_Value : 1.9597707688808441
Loss_Entropy : 2.306999385356903
Loss_Representation : -11.021491765975952
Loss_KL : 1.8626676201820374
Loss_Obs : -1.2819030284881592
Loss_Reward : -0.09358447257545777
Loss_Discount : 0.028455461841076612
Loss_RawKL : 1.862667590379715
mean_target : -2.398748755455017
max_target : -1.672761619091034
min_target : -2.9163307547569275
std_target : 0.40130724012851715
Done logging...

Current epsilon: 0.05554561640122854 at iteration 176128


********** Iteration 43 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1512, -0.1831, -0.1795, -0.1818, -0.1901, -0.1855, -0.1921, -0.1867,
        -0.1788, -0.1870, -0.1874, -0.1871, -0.1874, -0.1892, -0.1803],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1831, -3.1395, -3.0545, -2.9589, -2.8647, -2.7581, -2.6459, -2.5166,
        -2.3832, -2.2578, -2.1146, -1.9625, -1.8042, -1.6348], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1831, -3.1395, -3.0545, -2.9589, -2.8647, -2.7581, -2.6459, -2.5166,
        -2.3832, -2.2578, -2.1146, -1.9625, -1.8042, -1.6348], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1619, -0.1636, -0.1616, -0.1722, -0.1650, -0.1572, -0.1646, -0.1748,
        -0.1656, -0.1733, -0.1707, -0.1685, -0.1607, -0.1656, -0.1657],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1047, -3.0323, -2.9541, -2.8739, -2.7810, -2.6954, -2.6083, -2.5071,
        -2.3832, -2.2704, -2.1361, -2.0000, -1.8571, -1.7184], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1047, -3.0323, -2.9541, -2.8739, -2.7810, -2.6954, -2.6083, -2.5071,
        -2.3832, -2.2704, -2.1361, -2.0000, -1.8571, -1.7184], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1343, -0.1401, -0.1523, -0.1511, -0.1565, -0.1379, -0.1431, -0.1493,
        -0.1390, -0.1372, -0.1427, -0.1379, -0.1433, -0.1426, -0.1485],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.8778, -2.8262, -2.7606, -2.6885, -2.6105, -2.5187, -2.4375, -2.3522,
        -2.2478, -2.1563, -2.0558, -1.9454, -1.8353, -1.7066], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.8778, -2.8262, -2.7606, -2.6885, -2.6105, -2.5187, -2.4375, -2.3522,
        -2.2478, -2.1563, -2.0558, -1.9454, -1.8353, -1.7066], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1220, -0.1168, -0.1158, -0.1234, -0.1070, -0.1290, -0.1099, -0.1050,
        -0.1089, -0.1305, -0.1018, -0.1116, -0.1233, -0.1173, -0.1230],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.7234, -2.6656, -2.6105, -2.5521, -2.4890, -2.4511, -2.3716, -2.3111,
        -2.2493, -2.1848, -2.0909, -2.0223, -1.9374, -1.8438], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.7234, -2.6656, -2.6105, -2.5521, -2.4890, -2.4511, -2.3716, -2.3111,
        -2.2493, -2.1848, -2.0909, -2.0223, -1.9374, -1.8438], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0472, 0.0428, 0.0499, 0.1082, 0.0951, 0.0391, 0.0439, 0.1901, 0.0632,
        0.1415, 0.0454, 0.1337], device='cuda:0')
Count of actions: (array([ 1,  3,  7, 11]), array([1, 1, 3, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_43.gif
Eval_AverageReturn : -120.69999694824219
Eval_StdReturn : 227.11099243164062
Eval_MaxReturn : 75.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 137.3000030517578
Train_AverageReturn : -219.42857360839844
Train_StdReturn : 224.37205505371094
Train_MaxReturn : 93.0
Train_MinReturn : -400.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 180224.0
TimeSinceStart : 4741.290266036987
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 31.878169059753418
Loss_Value : 0.8960552513599396
Loss_Entropy : 2.291162133216858
Loss_Representation : -11.16364574432373
Loss_KL : 1.3378658294677734
Loss_Obs : -1.2449897229671478
Loss_Reward : -0.07928075082600117
Loss_Discount : 0.027666416484862566
Loss_RawKL : 1.3378658294677734
mean_target : -2.345767319202423
max_target : -1.727902889251709
min_target : -2.8180994391441345
std_target : 0.35035086423158646
Done logging...

Current epsilon: 0.05493315728813243 at iteration 180224


********** Iteration 44 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1060, -0.1168, -0.1209, -0.1022, -0.1085, -0.1002, -0.1129, -0.0997,
        -0.1136, -0.1197, -0.1098, -0.1072, -0.1230, -0.1207, -0.0924],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.6092, -2.5765, -2.5234, -2.4631, -2.4151, -2.3558, -2.3022, -2.2291,
        -2.1758, -2.1015, -2.0128, -1.9357, -1.8520, -1.7460], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.6092, -2.5765, -2.5234, -2.4631, -2.4151, -2.3558, -2.3022, -2.2291,
        -2.1758, -2.1015, -2.0128, -1.9357, -1.8520, -1.7460], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0995, -0.1050, -0.1286, -0.1205, -0.1236, -0.0969, -0.0922, -0.1006,
        -0.1022, -0.0831, -0.0850, -0.0851, -0.1009, -0.0783, -0.1051],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.5478, -2.5116, -2.4712, -2.3989, -2.3445, -2.2608, -2.2132, -2.1667,
        -2.1101, -2.0459, -1.9916, -1.9352, -1.8815, -1.8052], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.5478, -2.5116, -2.4712, -2.3989, -2.3445, -2.2608, -2.2132, -2.1667,
        -2.1101, -2.0459, -1.9916, -1.9352, -1.8815, -1.8052], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0908, -0.0821, -0.0881, -0.0826, -0.0909, -0.0883, -0.0877, -0.0830,
        -0.0920, -0.0846, -0.0755, -0.0712, -0.0757, -0.0862, -0.0926],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9427, -1.8932, -1.8573, -1.8086, -1.7722, -1.7221, -1.6708, -1.6091,
        -1.5556, -1.4951, -1.4357, -1.3860, -1.3402, -1.2783], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9427, -1.8932, -1.8573, -1.8086, -1.7722, -1.7221, -1.6708, -1.6091,
        -1.5556, -1.4951, -1.4357, -1.3860, -1.3402, -1.2783], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1082, -0.1210, -0.1107, -0.1153, -0.1109, -0.0997, -0.1240, -0.1100,
        -0.1109, -0.1205, -0.1188, -0.1088, -0.0993, -0.1050, -0.1124],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.7213, -2.6754, -2.6236, -2.5875, -2.5341, -2.4775, -2.4369, -2.3657,
        -2.2989, -2.2310, -2.1474, -2.0555, -1.9715, -1.8902], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.7213, -2.6754, -2.6236, -2.5875, -2.5341, -2.4775, -2.4369, -2.3657,
        -2.2989, -2.2310, -2.1474, -2.0555, -1.9715, -1.8902], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0406, 0.0364, 0.0427, 0.1065, 0.1051, 0.0333, 0.0385, 0.2339, 0.0581,
        0.1452, 0.0387, 0.1210], device='cuda:0')
Count of actions: (array([3, 7, 9]), array([2, 2, 2]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_44.gif
Eval_AverageReturn : -305.29998779296875
Eval_StdReturn : 183.22067260742188
Eval_MaxReturn : 70.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 246.5
Train_AverageReturn : -166.9600067138672
Train_StdReturn : 233.67318725585938
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 163.83999633789062
Train_EnvstepsSoFar : 184320.0
TimeSinceStart : 4854.377732753754
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 30.327516078948975
Loss_Value : 0.4933042600750923
Loss_Entropy : 2.2255327105522156
Loss_Representation : -11.643226146697998
Loss_KL : 1.0791825652122498
Loss_Obs : -1.2663187086582184
Loss_Reward : -0.08772165095433593
Loss_Discount : 0.028499888256192207
Loss_RawKL : 1.0791825652122498
mean_target : -2.2330456376075745
max_target : -1.8067883849143982
min_target : -2.566713869571686
std_target : 0.24425681680440903
Done logging...

Current epsilon: 0.054388338296183426 at iteration 184320


********** Iteration 45 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1277, -0.1234, -0.1231, -0.1294, -0.1418, -0.1346, -0.1230, -0.1343,
        -0.1320, -0.1236, -0.1301, -0.1330, -0.1377, -0.1319, -0.1439],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.0304, -2.9969, -2.9483, -2.9094, -2.8652, -2.8015, -2.7376, -2.6883,
        -2.6114, -2.5409, -2.4595, -2.3774, -2.2791, -2.1824], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.0304, -2.9969, -2.9483, -2.9094, -2.8652, -2.8015, -2.7376, -2.6883,
        -2.6114, -2.5409, -2.4595, -2.3774, -2.2791, -2.1824], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1504, -0.1730, -0.1541, -0.1565, -0.1574, -0.1617, -0.1642, -0.1539,
        -0.1625, -0.1564, -0.1674, -0.1543, -0.1564, -0.1690, -0.1567],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.2495, -3.2112, -3.1278, -3.0665, -2.9936, -2.9118, -2.8199, -2.7269,
        -2.6423, -2.5287, -2.4186, -2.2949, -2.1718, -2.0443], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.2495, -3.2112, -3.1278, -3.0665, -2.9936, -2.9118, -2.8199, -2.7269,
        -2.6423, -2.5287, -2.4186, -2.2949, -2.1718, -2.0443], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1459, -0.1464, -0.1588, -0.1448, -0.1552, -0.1451, -0.1432, -0.1584,
        -0.1524, -0.1501, -0.1655, -0.1551, -0.1710, -0.1502, -0.1638],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1313, -3.0733, -3.0115, -2.9386, -2.8720, -2.7884, -2.7039, -2.6197,
        -2.5128, -2.4145, -2.3011, -2.1736, -2.0463, -1.8951], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1313, -3.0733, -3.0115, -2.9386, -2.8720, -2.7884, -2.7039, -2.6197,
        -2.5128, -2.4145, -2.3011, -2.1736, -2.0463, -1.8951], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0982, -0.1194, -0.1119, -0.1050, -0.1143, -0.0916, -0.0870, -0.0828,
        -0.1163, -0.1151, -0.0991, -0.1055, -0.0953, -0.0974, -0.1015],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.6907, -2.6649, -2.5926, -2.5448, -2.5006, -2.4436, -2.4000, -2.3605,
        -2.3224, -2.2431, -2.1693, -2.1065, -2.0273, -1.9576], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.6907, -2.6649, -2.5926, -2.5448, -2.5006, -2.4436, -2.4000, -2.3605,
        -2.3224, -2.2431, -2.1693, -2.1065, -2.0273, -1.9576], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0420, 0.0371, 0.0439, 0.0981, 0.1108, 0.0371, 0.0411, 0.2539, 0.0570,
        0.1356, 0.0401, 0.1033], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([15, 14, 10, 33, 38, 14, 14, 76, 12, 45,  8, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_45.gif
Eval_AverageReturn : -305.29998779296875
Eval_StdReturn : 185.74339294433594
Eval_MaxReturn : 76.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 245.5
Train_AverageReturn : -157.42308044433594
Train_StdReturn : 229.64756774902344
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 157.53846740722656
Train_EnvstepsSoFar : 188416.0
TimeSinceStart : 4973.3864579200745
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 31.917057037353516
Loss_Value : 0.527228444814682
Loss_Entropy : 2.1994452476501465
Loss_Representation : -12.190639734268188
Loss_KL : 1.0116748064756393
Loss_Obs : -1.3058572709560394
Loss_Reward : -0.1754707396030426
Loss_Discount : 0.03172887349501252
Loss_RawKL : 1.0116747915744781
mean_target : -2.3458017110824585
max_target : -1.880094438791275
min_target : -2.710091471672058
std_target : 0.2663228213787079
Done logging...

Current epsilon: 0.05390368923530522 at iteration 188416


********** Iteration 46 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0857, -0.0863, -0.0832, -0.0997, -0.0887, -0.0906, -0.0984, -0.1152,
        -0.1155, -0.0935, -0.0891, -0.0895, -0.1050, -0.1119, -0.1056],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.6148, -2.5961, -2.5697, -2.5470, -2.5076, -2.4743, -2.4353, -2.3842,
        -2.3074, -2.2245, -2.1679, -2.1091, -2.0501, -1.9658], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.6148, -2.5961, -2.5697, -2.5470, -2.5076, -2.4743, -2.4353, -2.3842,
        -2.3074, -2.2245, -2.1679, -2.1091, -2.0501, -1.9658], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1289, -0.1477, -0.1446, -0.1537, -0.1413, -0.1468, -0.1458, -0.1507,
        -0.1564, -0.1494, -0.1513, -0.1650, -0.1607, -0.1460, -0.1539],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1235, -3.0893, -3.0180, -2.9569, -2.8799, -2.8128, -2.7339, -2.6501,
        -2.5527, -2.4478, -2.3580, -2.2454, -2.1102, -1.9860], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1235, -3.0893, -3.0180, -2.9569, -2.8799, -2.8128, -2.7339, -2.6501,
        -2.5527, -2.4478, -2.3580, -2.2454, -2.1102, -1.9860], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1677, -0.1836, -0.1840, -0.1824, -0.1783, -0.1750, -0.1796, -0.1959,
        -0.1947, -0.1691, -0.1727, -0.1856, -0.1843, -0.1824, -0.1748],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4454, -3.3946, -3.3155, -3.2327, -3.1541, -3.0592, -2.9785, -2.8650,
        -2.7386, -2.6046, -2.5032, -2.3815, -2.2322, -2.0678], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4454, -3.3946, -3.3155, -3.2327, -3.1541, -3.0592, -2.9785, -2.8650,
        -2.7386, -2.6046, -2.5032, -2.3815, -2.2322, -2.0678], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1808, -0.1823, -0.1852, -0.1978, -0.1810, -0.1987, -0.1799, -0.1882,
        -0.1845, -0.1842, -0.1863, -0.1907, -0.1820, -0.1971, -0.1824],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.5506, -3.4834, -3.4081, -3.3302, -3.2275, -3.1441, -3.0316, -2.9298,
        -2.8221, -2.7060, -2.5789, -2.4403, -2.3015, -2.1403], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.5506, -3.4834, -3.4081, -3.3302, -3.2275, -3.1441, -3.0316, -2.9298,
        -2.8221, -2.7060, -2.5789, -2.4403, -2.3015, -2.1403], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0380, 0.0330, 0.0394, 0.0907, 0.1217, 0.0339, 0.0386, 0.2948, 0.0532,
        0.1290, 0.0360, 0.0916], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([12,  7,  9, 31, 33, 10, 11, 91, 18, 39, 17, 22]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_46.gif
Eval_AverageReturn : -247.10000610351562
Eval_StdReturn : 223.74693298339844
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 211.89999389648438
Train_AverageReturn : -220.38095092773438
Train_StdReturn : 217.9006805419922
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 192512.0
TimeSinceStart : 5087.295745134354
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 33.68421173095703
Loss_Value : 0.5921643823385239
Loss_Entropy : 2.141426205635071
Loss_Representation : -12.206368446350098
Loss_KL : 0.9760176688432693
Loss_Obs : -1.31409952044487
Loss_Reward : -0.06967411749064922
Loss_Discount : 0.02828315133228898
Loss_RawKL : 0.9760176539421082
mean_target : -2.4702598452568054
max_target : -1.96998730301857
min_target : -2.851686120033264
std_target : 0.28324298188090324
Done logging...

Current epsilon: 0.053472564924880825 at iteration 192512


********** Iteration 47 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1528, -0.1586, -0.1491, -0.1621, -0.1422, -0.1554, -0.1499, -0.1535,
        -0.1502, -0.1449, -0.1476, -0.1385, -0.1527, -0.1572, -0.1588],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.3313, -3.2758, -3.2144, -3.1585, -3.0792, -3.0219, -2.9442, -2.8696,
        -2.7763, -2.6934, -2.5951, -2.4898, -2.3921, -2.2702], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.3313, -3.2758, -3.2144, -3.1585, -3.0792, -3.0219, -2.9442, -2.8696,
        -2.7763, -2.6934, -2.5951, -2.4898, -2.3921, -2.2702], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0928, -0.1108, -0.0875, -0.0932, -0.0903, -0.0919, -0.0995, -0.0948,
        -0.0957, -0.0934, -0.0869, -0.0952, -0.1080, -0.0999, -0.0990],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.9342, -2.9044, -2.8506, -2.8203, -2.7860, -2.7404, -2.6971, -2.6373,
        -2.5862, -2.5274, -2.4669, -2.4176, -2.3621, -2.2862], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.9342, -2.9044, -2.8506, -2.8203, -2.7860, -2.7404, -2.6971, -2.6373,
        -2.5862, -2.5274, -2.4669, -2.4176, -2.3621, -2.2862], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0595, -0.0692, -0.0728, -0.0825, -0.0696, -0.0683, -0.0741, -0.0531,
        -0.0710, -0.0691, -0.0902, -0.0657, -0.0627, -0.0691, -0.0723],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.7289, -2.7195, -2.6972, -2.6702, -2.6295, -2.6031, -2.5866, -2.5450,
        -2.5354, -2.4927, -2.4693, -2.4191, -2.3893, -2.3606], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.7289, -2.7195, -2.6972, -2.6702, -2.6295, -2.6031, -2.5866, -2.5450,
        -2.5354, -2.4927, -2.4693, -2.4191, -2.3893, -2.3606], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0546, -0.0620, -0.0586, -0.0546, -0.0681, -0.0705, -0.0701, -0.0661,
        -0.0647, -0.0640, -0.0672, -0.0733, -0.0834, -0.0622, -0.0592],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.6159, -2.5990, -2.5716, -2.5507, -2.5279, -2.4950, -2.4547, -2.4244,
        -2.3927, -2.3476, -2.3026, -2.2563, -2.2050, -2.1284], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.6159, -2.5990, -2.5716, -2.5507, -2.5279, -2.4950, -2.4547, -2.4244,
        -2.3927, -2.3476, -2.3026, -2.2563, -2.2050, -2.1284], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0456, 0.0406, 0.0471, 0.0792, 0.1157, 0.0441, 0.0466, 0.2856, 0.0559,
        0.1147, 0.0440, 0.0809], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([13, 15, 10, 13, 30, 16, 10, 93, 22, 31, 15, 32]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_47.gif
Eval_AverageReturn : -170.60000610351562
Eval_StdReturn : 231.10525512695312
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 166.10000610351562
Train_AverageReturn : -224.0
Train_StdReturn : 223.23764038085938
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 196608.0
TimeSinceStart : 5194.231721639633
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 33.01382064819336
Loss_Value : 0.4882073476910591
Loss_Entropy : 2.1600179076194763
Loss_Representation : -12.166114568710327
Loss_KL : 1.2207444608211517
Loss_Obs : -1.3132875859737396
Loss_Reward : -0.2807060293853283
Loss_Discount : 0.026722942013293505
Loss_RawKL : 1.2207444608211517
mean_target : -2.422967493534088
max_target : -2.095756709575653
min_target : -2.6819189190864563
std_target : 0.18816151097416878
Done logging...

Current epsilon: 0.05308905407952376 at iteration 196608


********** Iteration 48 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1031, -0.0886, -0.0955, -0.0965, -0.0987, -0.1148, -0.1120, -0.1143,
        -0.1118, -0.0992, -0.0907, -0.0933, -0.0940, -0.0848, -0.0860],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.0747, -3.0405, -3.0195, -2.9904, -2.9479, -2.8987, -2.8314, -2.7677,
        -2.6986, -2.6308, -2.5779, -2.5224, -2.4600, -2.3933], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.0747, -3.0405, -3.0195, -2.9904, -2.9479, -2.8987, -2.8314, -2.7677,
        -2.6986, -2.6308, -2.5779, -2.5224, -2.4600, -2.3933], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0934, -0.0400, -0.0513, -0.0171, -0.0277, -0.0580, -0.0340, -0.0389,
        -0.0678, -0.0732, -0.1076, -0.0592, -0.0991, -0.0841, -0.0796],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.9020, -1.8414, -1.8378, -1.8281, -1.8512, -1.8657, -1.8529, -1.8606,
        -1.8739, -1.8442, -1.8019, -1.7334, -1.6893, -1.6150], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.9020, -1.8414, -1.8378, -1.8281, -1.8512, -1.8657, -1.8529, -1.8606,
        -1.8739, -1.8442, -1.8019, -1.7334, -1.6893, -1.6150], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1726, -0.1701, -0.1636, -0.1823, -0.1769, -0.1792, -0.1748, -0.1721,
        -0.1716, -0.1789, -0.1762, -0.1778, -0.1741, -0.1731, -0.1729],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.8210, -3.7525, -3.6754, -3.5992, -3.4983, -3.4097, -3.3120, -3.2064,
        -3.0918, -2.9756, -2.8441, -2.7089, -2.5630, -2.4147], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.8210, -3.7525, -3.6754, -3.5992, -3.4983, -3.4097, -3.3120, -3.2064,
        -3.0918, -2.9756, -2.8441, -2.7089, -2.5630, -2.4147], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0093, -0.0461, -0.0312, -0.0308, -0.0472, -0.0348, -0.0678, -0.0811,
        -0.0583, -0.0606, -0.0453, -0.0486, -0.0526, -0.0824, -0.0510],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-1.7529, -1.7814, -1.7843, -1.8066, -1.8184, -1.8080, -1.8123, -1.7844,
        -1.7456, -1.7151, -1.6941, -1.6752, -1.6496, -1.6197], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-1.7529, -1.7814, -1.7843, -1.8066, -1.8184, -1.8080, -1.8123, -1.7844,
        -1.7456, -1.7151, -1.6941, -1.6752, -1.6496, -1.6197], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0379, 0.0343, 0.0387, 0.0670, 0.1154, 0.0359, 0.0389, 0.3674, 0.0474,
        0.1098, 0.0363, 0.0709], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 15,   5,  15,  24,  27,  16,  12, 116,  14,  25,  14,  17]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_48.gif
Eval_AverageReturn : -349.5
Eval_StdReturn : 148.17979431152344
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 270.6000061035156
Train_AverageReturn : -156.65383911132812
Train_StdReturn : 228.479248046875
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 157.53846740722656
Train_EnvstepsSoFar : 200704.0
TimeSinceStart : 5316.574695110321
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 38.1526985168457
Loss_Value : 0.5906617194414139
Loss_Entropy : 1.9967500865459442
Loss_Representation : -12.272284030914307
Loss_KL : 1.3502238392829895
Loss_Obs : -1.3418390154838562
Loss_Reward : -0.2391746067442
Loss_Discount : 0.035056964959949255
Loss_RawKL : 1.3502238392829895
mean_target : -2.785109043121338
max_target : -2.2425215244293213
min_target : -3.2001238465309143
std_target : 0.3079630024731159
Done logging...

Current epsilon: 0.05274789825752498 at iteration 200704


********** Iteration 49 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0345, -0.0679, -0.0482, -0.0621, -0.0607, -0.0520, -0.0441, -0.0492,
        -0.0476, -0.0859, -0.0571, -0.0433, -0.0556, -0.0817, -0.0534],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.5972, -2.6024, -2.5718, -2.5601, -2.5477, -2.5197, -2.4877, -2.4676,
        -2.4406, -2.4187, -2.3603, -2.3354, -2.3134, -2.2776], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.5972, -2.6024, -2.5718, -2.5601, -2.5477, -2.5197, -2.4877, -2.4676,
        -2.4406, -2.4187, -2.3603, -2.3354, -2.3134, -2.2776], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0633, -0.0662, -0.0464, -0.0699, -0.0656, -0.0795, -0.0796, -0.0780,
        -0.0742, -0.0685, -0.0720, -0.0701, -0.0733, -0.0656, -0.0678],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.8398, -2.8276, -2.8030, -2.8035, -2.7712, -2.7604, -2.7235, -2.6791,
        -2.6430, -2.6052, -2.5697, -2.5269, -2.4797, -2.4275], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.8398, -2.8276, -2.8030, -2.8035, -2.7712, -2.7604, -2.7235, -2.6791,
        -2.6430, -2.6052, -2.5697, -2.5269, -2.4797, -2.4275], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1842, -0.1770, -0.1686, -0.1839, -0.1856, -0.1691, -0.1703, -0.1791,
        -0.1789, -0.1778, -0.1809, -0.1783, -0.1804, -0.1856, -0.1742],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.7875, -3.7094, -3.6284, -3.5499, -3.4620, -3.3583, -3.2701, -3.1672,
        -3.0504, -2.9311, -2.7975, -2.6538, -2.5002, -2.3348], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.7875, -3.7094, -3.6284, -3.5499, -3.4620, -3.3583, -3.2701, -3.1672,
        -3.0504, -2.9311, -2.7975, -2.6538, -2.5002, -2.3348], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0113, -0.1018, -0.0713, -0.0859, -0.0953, -0.0980, -0.1058, -0.0582,
        -0.0886, -0.1619, -0.1527, -0.1589, -0.1626, -0.1634, -0.1478],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.6792, -2.7501, -2.7493, -2.7937, -2.7900, -2.7757, -2.7699, -2.7373,
        -2.7660, -2.7430, -2.6374, -2.5298, -2.4040, -2.2746], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.6792, -2.7501, -2.7493, -2.7937, -2.7900, -2.7757, -2.7699, -2.7373,
        -2.7660, -2.7430, -2.6374, -2.5298, -2.4040, -2.2746], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0365, 0.0344, 0.0372, 0.0591, 0.1083, 0.0344, 0.0372, 0.3981, 0.0444,
        0.1093, 0.0350, 0.0660], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([  7,   7,   8,  14,  29,   7,   7, 139,  20,  31,  13,  18]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_49.gif
Eval_AverageReturn : -254.39999389648438
Eval_StdReturn : 224.65138244628906
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 213.1999969482422
Train_AverageReturn : -224.4761962890625
Train_StdReturn : 227.13088989257812
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 204800.0
TimeSinceStart : 5431.014704704285
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 38.56014060974121
Loss_Value : 0.6953782364726067
Loss_Entropy : 1.932558685541153
Loss_Representation : -12.048255443572998
Loss_KL : 1.6899215877056122
Loss_Obs : -1.3306539356708527
Loss_Reward : -0.4623487424105406
Loss_Discount : 0.030710724648088217
Loss_RawKL : 1.6899215579032898
mean_target : -2.812373101711273
max_target : -2.3590577244758606
min_target : -3.147313952445984
std_target : 0.2543742451816797
Done logging...

Current epsilon: 0.05244441976065144 at iteration 204800


********** Iteration 50 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0944, -0.0785, -0.0623, -0.0651, -0.0678, -0.0664, -0.0808, -0.0765,
        -0.0737, -0.0941, -0.0865, -0.0865, -0.1051, -0.0807, -0.0924],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.7754, -2.7252, -2.6902, -2.6668, -2.6416, -2.6087, -2.5879, -2.5384,
        -2.5078, -2.4729, -2.4078, -2.3491, -2.2870, -2.2123], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.7754, -2.7252, -2.6902, -2.6668, -2.6416, -2.6087, -2.5879, -2.5384,
        -2.5078, -2.4729, -2.4078, -2.3491, -2.2870, -2.2123], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0339, -0.0364, -0.0394, -0.0679, -0.0681, -0.0837, -0.0489, -0.0689,
        -0.0642, -0.0678, -0.0570, -0.0499, -0.0426, -0.0472, -0.0341],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.5130, -2.5264, -2.5179, -2.5314, -2.4982, -2.4588, -2.4152, -2.4062,
        -2.3726, -2.3322, -2.3004, -2.2605, -2.2295, -2.2065], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.5130, -2.5264, -2.5179, -2.5314, -2.4982, -2.4588, -2.4152, -2.4062,
        -2.3726, -2.3322, -2.3004, -2.2605, -2.2295, -2.2065], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1080, -0.1153, -0.1166, -0.1241, -0.1227, -0.1108, -0.1137, -0.1180,
        -0.1037, -0.1092, -0.1056, -0.0972, -0.1128, -0.1054, -0.0992],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1645, -3.1484, -3.1110, -3.0839, -3.0397, -2.9922, -2.9587, -2.9072,
        -2.8458, -2.7927, -2.7317, -2.6726, -2.6169, -2.5477], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1645, -3.1484, -3.1110, -3.0839, -3.0397, -2.9922, -2.9587, -2.9072,
        -2.8458, -2.7927, -2.7317, -2.6726, -2.6169, -2.5477], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1552, -0.1373, -0.1389, -0.1604, -0.1512, -0.0803, -0.1526, -0.1532,
        -0.1436, -0.1527, -0.1581, -0.1625, -0.1481, -0.1600, -0.1518],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4642, -3.4112, -3.3725, -3.3300, -3.2609, -3.1989, -3.2006, -3.1253,
        -3.0473, -2.9685, -2.8759, -2.7589, -2.6342, -2.5136], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4642, -3.4112, -3.3725, -3.3300, -3.2609, -3.1989, -3.2006, -3.1253,
        -3.0473, -2.9685, -2.8759, -2.7589, -2.6342, -2.5136], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0264, 0.0267, 0.0268, 0.0461, 0.1026, 0.0237, 0.0270, 0.4954, 0.0338,
        0.1114, 0.0251, 0.0549], device='cuda:0')
Count of actions: (array([4, 7]), array([1, 5]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_50.gif
Eval_AverageReturn : -305.29998779296875
Eval_StdReturn : 190.84500122070312
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 244.0
Train_AverageReturn : -261.1052551269531
Train_StdReturn : 212.62911987304688
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 208896.0
TimeSinceStart : 5543.920754432678
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 37.23685264587402
Loss_Value : 0.45504506677389145
Loss_Entropy : 1.7142511308193207
Loss_Representation : -12.267593383789062
Loss_KL : 1.549507737159729
Loss_Obs : -1.3334650099277496
Loss_Reward : -0.5160970389842987
Loss_Discount : 0.033646127209067345
Loss_RawKL : 1.5495076477527618
mean_target : -2.7111626863479614
max_target : -2.3633797764778137
min_target : -2.9755530953407288
std_target : 0.19625326804816723
Done logging...

Current epsilon: 0.05217445749670698 at iteration 208896


********** Iteration 51 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1573, -0.1577, -0.1570, -0.1572, -0.1565, -0.1556, -0.1604, -0.1570,
        -0.1574, -0.1607, -0.1255, -0.1499, -0.1519, -0.1484, -0.1600],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6552, -3.5865, -3.5139, -3.4442, -3.3647, -3.2795, -3.1967, -3.0935,
        -3.0046, -2.8960, -2.7852, -2.7042, -2.6007, -2.4873], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6552, -3.5865, -3.5139, -3.4442, -3.3647, -3.2795, -3.1967, -3.0935,
        -3.0046, -2.8960, -2.7852, -2.7042, -2.6007, -2.4873], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1032, -0.0897, -0.0928, -0.0957, -0.1007, -0.1030, -0.1048, -0.0990,
        -0.0954, -0.0928, -0.1002, -0.1060, -0.1010, -0.0915, -0.0920],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1451, -3.1309, -3.1130, -3.0865, -3.0500, -2.9996, -2.9452, -2.8919,
        -2.8421, -2.7938, -2.7406, -2.6861, -2.6167, -2.5593], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1451, -3.1309, -3.1130, -3.0865, -3.0500, -2.9996, -2.9452, -2.8919,
        -2.8421, -2.7938, -2.7406, -2.6861, -2.6167, -2.5593], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0532, -0.0510, -0.0582, -0.0329, -0.0369, -0.0512, -0.0517, -0.0422,
        -0.0560, -0.0593, -0.0578, -0.0421, -0.0581, -0.0792, -0.0448],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.7067, -2.6991, -2.6828, -2.6711, -2.6892, -2.6913, -2.6804, -2.6556,
        -2.6459, -2.6169, -2.5892, -2.5745, -2.5572, -2.5209], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.7067, -2.6991, -2.6828, -2.6711, -2.6892, -2.6913, -2.6804, -2.6556,
        -2.6459, -2.6169, -2.5892, -2.5745, -2.5572, -2.5209], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0914, -0.0896, -0.0909, -0.1091, -0.1054, -0.0982, -0.0929, -0.0884,
        -0.0903, -0.0947, -0.0919, -0.0849, -0.0867, -0.0787, -0.0904],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1314, -3.1118, -3.0893, -3.0592, -3.0152, -2.9800, -2.9380, -2.8914,
        -2.8542, -2.8061, -2.7487, -2.6894, -2.6281, -2.5739], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1314, -3.1118, -3.0893, -3.0592, -3.0152, -2.9800, -2.9380, -2.8914,
        -2.8542, -2.8061, -2.7487, -2.6894, -2.6281, -2.5739], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0352, 0.0381, 0.0373, 0.0542, 0.1106, 0.0337, 0.0365, 0.3854, 0.0437,
        0.1264, 0.0352, 0.0638], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([11, 16, 10,  9, 32, 18, 10, 95, 14, 43, 22, 20]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_51.gif
Eval_AverageReturn : -160.5
Eval_StdReturn : 236.8219757080078
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 160.0
Train_AverageReturn : -261.1052551269531
Train_StdReturn : 206.8798065185547
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 212992.0
TimeSinceStart : 5650.396741628647
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 39.79725456237793
Loss_Value : 0.4084826409816742
Loss_Entropy : 1.9705784916877747
Loss_Representation : -12.771366357803345
Loss_KL : 1.3128472566604614
Loss_Obs : -1.3439153134822845
Loss_Reward : -0.6703935489058495
Loss_Discount : 0.02533296961337328
Loss_RawKL : 1.3128472566604614
mean_target : -2.9019525051116943
max_target : -2.560211479663849
min_target : -3.1584527492523193
std_target : 0.1924388874322176
Done logging...

Current epsilon: 0.051934309925446315 at iteration 212992


********** Iteration 52 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1153, -0.1391, -0.1461, -0.1485, -0.1501, -0.1448, -0.1410, -0.1516,
        -0.1523, -0.1492, -0.1395, -0.1458, -0.1461, -0.1447, -0.1494],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6156, -3.6077, -3.5875, -3.5251, -3.4540, -3.3840, -3.3141, -3.2395,
        -3.1512, -3.0541, -2.9659, -2.8732, -2.7716, -2.6629], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6156, -3.6077, -3.5875, -3.5251, -3.4540, -3.3840, -3.3141, -3.2395,
        -3.1512, -3.0541, -2.9659, -2.8732, -2.7716, -2.6629], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1303, -0.1294, -0.1219, -0.1298, -0.1189, -0.1176, -0.1050, -0.1214,
        -0.1125, -0.1243, -0.1282, -0.1257, -0.1266, -0.1300, -0.1243],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4259, -3.3839, -3.3456, -3.2967, -3.2413, -3.1869, -3.1272, -3.0910,
        -3.0248, -2.9615, -2.8781, -2.7810, -2.6822, -2.5826], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4259, -3.3839, -3.3456, -3.2967, -3.2413, -3.1869, -3.1272, -3.0910,
        -3.0248, -2.9615, -2.8781, -2.7810, -2.6822, -2.5826], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0452, -0.0524, -0.0562, -0.0726, -0.0625, -0.0756, -0.0764, -0.0679,
        -0.0688, -0.0464, -0.0442, -0.0467, -0.0462, -0.0612, -0.0598],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.0364, -3.0485, -3.0392, -3.0332, -3.0054, -2.9942, -2.9708, -2.9465,
        -2.9116, -2.8805, -2.8768, -2.8718, -2.8526, -2.8383], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.0364, -3.0485, -3.0392, -3.0332, -3.0054, -2.9942, -2.9708, -2.9465,
        -2.9116, -2.8805, -2.8768, -2.8718, -2.8526, -2.8383], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0604, -0.0704, -0.0748, -0.0788, -0.0835, -0.0765, -0.0868, -0.0717,
        -0.0942, -0.0765, -0.0735, -0.0729, -0.0681, -0.0685, -0.0706],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1343, -3.1308, -3.0957, -3.0703, -3.0450, -3.0151, -2.9901, -2.9434,
        -2.9190, -2.8578, -2.8268, -2.7817, -2.7461, -2.6993], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1343, -3.1308, -3.0957, -3.0703, -3.0450, -3.0151, -2.9901, -2.9434,
        -2.9190, -2.8578, -2.8268, -2.7817, -2.7461, -2.6993], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0415, 0.0464, 0.0460, 0.0576, 0.1073, 0.0422, 0.0436, 0.3295, 0.0499,
        0.1266, 0.0433, 0.0661], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([13, 21, 11, 15, 31, 10, 15, 92, 22, 43,  9, 18]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_52.gif
Eval_AverageReturn : -301.70001220703125
Eval_StdReturn : 184.5508270263672
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 245.39999389648438
Train_AverageReturn : -259.2631530761719
Train_StdReturn : 208.37892150878906
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 217088.0
TimeSinceStart : 5769.110112428665
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 40.77431011199951
Loss_Value : 0.3782276287674904
Loss_Entropy : 2.0485870838165283
Loss_Representation : -12.529127597808838
Loss_KL : 1.744002491235733
Loss_Obs : -1.3442572951316833
Loss_Reward : -0.859684020280838
Loss_Discount : 0.029126975685358047
Loss_RawKL : 1.7440024614334106
mean_target : -2.973983407020569
max_target : -2.598356604576111
min_target : -3.263912796974182
std_target : 0.21402616053819656
Done logging...

Current epsilon: 0.05172068430555501 at iteration 217088


********** Iteration 53 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0934, -0.0871, -0.1175, -0.1298, -0.1137, -0.1041, -0.0515, -0.0493,
        -0.0441, -0.0665, -0.0891, -0.1245, -0.0915, -0.0872, -0.1218],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.2674, -2.2314, -2.2010, -2.1670, -2.0889, -2.0118, -1.9335, -1.9182,
        -1.9099, -1.9242, -1.9086, -1.8458, -1.7383, -1.6605], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.2674, -2.2314, -2.2010, -2.1670, -2.0889, -2.0118, -1.9335, -1.9182,
        -1.9099, -1.9242, -1.9086, -1.8458, -1.7383, -1.6605], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1330, -0.1337, -0.1330, -0.1328, -0.1335, -0.1344, -0.1366, -0.1343,
        -0.1294, -0.1349, -0.1337, -0.1358, -0.1272, -0.1265, -0.1250],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.7218, -3.6716, -3.6084, -3.5393, -3.4723, -3.3974, -3.3198, -3.2297,
        -3.1375, -3.0381, -2.9395, -2.8359, -2.7395, -2.6455], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.7218, -3.6716, -3.6084, -3.5393, -3.4723, -3.3974, -3.3198, -3.2297,
        -3.1375, -3.0381, -2.9395, -2.8359, -2.7395, -2.6455], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0581, -0.0712, -0.0656, -0.0617, -0.0639, -0.0682, -0.0620, -0.0616,
        -0.0667, -0.0587, -0.0636, -0.0735, -0.0812, -0.0895, -0.1082],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.0332, -3.0144, -2.9720, -2.9435, -2.9152, -2.8753, -2.8254, -2.7838,
        -2.7418, -2.6854, -2.6367, -2.5897, -2.5282, -2.4623], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.0332, -3.0144, -2.9720, -2.9435, -2.9152, -2.8753, -2.8254, -2.7838,
        -2.7418, -2.6854, -2.6367, -2.5897, -2.5282, -2.4623], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1032, -0.1090, -0.0953, -0.0911, -0.0934, -0.0930, -0.0859, -0.0957,
        -0.0862, -0.0829, -0.0839, -0.0867, -0.0935, -0.0918, -0.0972],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4488, -3.4254, -3.3845, -3.3556, -3.3146, -3.2669, -3.2218, -3.1835,
        -3.1266, -3.0835, -3.0346, -2.9716, -2.9032, -2.8309], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4488, -3.4254, -3.3845, -3.3556, -3.3146, -3.2669, -3.2218, -3.1835,
        -3.1266, -3.0835, -3.0346, -2.9716, -2.9032, -2.8309], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0517, 0.0570, 0.0595, 0.0634, 0.0974, 0.0553, 0.0542, 0.2602, 0.0587,
        0.1171, 0.0559, 0.0697], device='cuda:0')
Count of actions: (array([3, 4, 7, 9]), array([ 1,  8, 44,  9]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_53.gif
Eval_AverageReturn : -248.39999389648438
Eval_StdReturn : 220.7375030517578
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 213.1999969482422
Train_AverageReturn : -239.5
Train_StdReturn : 213.4182586669922
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 221184.0
TimeSinceStart : 5878.770693778992
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 43.27758312225342
Loss_Value : 0.3656506724655628
Loss_Entropy : 2.1480597853660583
Loss_Representation : -12.748903274536133
Loss_KL : 1.6338114440441132
Loss_Obs : -1.3376606702804565
Loss_Reward : -1.0338551253080368
Loss_Discount : 0.027746780775487423
Loss_RawKL : 1.6338113844394684
mean_target : -3.155750870704651
max_target : -2.727936267852783
min_target : -3.4901586771011353
std_target : 0.24553854763507843
Done logging...

Current epsilon: 0.0515306515468043 at iteration 221184


********** Iteration 54 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1408, -0.1396, -0.1439, -0.1436, -0.1442, -0.1413, -0.1435, -0.1393,
        -0.1425, -0.1409, -0.1377, -0.1417, -0.1392, -0.1434, -0.1437],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.9462, -3.8875, -3.8334, -3.7667, -3.6900, -3.6148, -3.5411, -3.4655,
        -3.3923, -3.3091, -3.2391, -3.1518, -3.0605, -2.9499], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.9462, -3.8875, -3.8334, -3.7667, -3.6900, -3.6148, -3.5411, -3.4655,
        -3.3923, -3.3091, -3.2391, -3.1518, -3.0605, -2.9499], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1176, -0.1161, -0.1176, -0.1142, -0.1195, -0.1195, -0.1206, -0.1247,
        -0.1205, -0.1191, -0.1171, -0.1179, -0.1213, -0.1199, -0.1249],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.7248, -3.6846, -3.6324, -3.5758, -3.5210, -3.4569, -3.3948, -3.3247,
        -3.2523, -3.1851, -3.1231, -3.0515, -2.9641, -2.8708], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.7248, -3.6846, -3.6324, -3.5758, -3.5210, -3.4569, -3.3948, -3.3247,
        -3.2523, -3.1851, -3.1231, -3.0515, -2.9641, -2.8708], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0795, -0.0759, -0.0659, -0.0598, -0.0524, -0.0407, -0.0346, -0.0523,
        -0.0619, -0.0507, -0.0463, -0.0424, -0.0313, -0.0387, -0.0457],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.5650, -2.5777, -2.5537, -2.5357, -2.5192, -2.4994, -2.4916, -2.4920,
        -2.4648, -2.4365, -2.4188, -2.3966, -2.3847, -2.3706], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.5650, -2.5777, -2.5537, -2.5357, -2.5192, -2.4994, -2.4916, -2.4920,
        -2.4648, -2.4365, -2.4188, -2.3966, -2.3847, -2.3706], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0736, -0.0721, -0.0830, -0.0610, -0.0540, -0.0651, -0.0642, -0.0675,
        -0.0727, -0.0621, -0.0557, -0.0534, -0.0637, -0.0630, -0.0622],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.2348, -3.2052, -3.1891, -3.1436, -3.1253, -3.1091, -3.0855, -3.0600,
        -3.0366, -2.9966, -2.9793, -2.9533, -2.9449, -2.9200], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.2348, -3.2052, -3.1891, -3.1436, -3.1253, -3.1091, -3.0855, -3.0600,
        -3.0366, -2.9966, -2.9793, -2.9533, -2.9449, -2.9200], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0511, 0.0575, 0.0592, 0.0618, 0.0930, 0.0548, 0.0540, 0.2683, 0.0581,
        0.1180, 0.0557, 0.0684], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([14, 15, 28, 20, 25, 25, 13, 60, 21, 35, 23, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_54.gif
Eval_AverageReturn : -153.3000030517578
Eval_StdReturn : 240.0146026611328
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 156.3000030517578
Train_AverageReturn : -180.6666717529297
Train_StdReturn : 233.53135681152344
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 170.6666717529297
Train_EnvstepsSoFar : 225280.0
TimeSinceStart : 5984.290721654892
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 42.93617916107178
Loss_Value : 0.3616336025297642
Loss_Entropy : 2.1237855553627014
Loss_Representation : -13.19159984588623
Loss_KL : 1.357968032360077
Loss_Obs : -1.3620270490646362
Loss_Reward : -0.9597516879439354
Loss_Discount : 0.03045447962358594
Loss_RawKL : 1.357968032360077
mean_target : -3.130563795566559
max_target : -2.768750548362732
min_target : -3.413731276988983
std_target : 0.20792443491518497
Done logging...

Current epsilon: 0.05136160604834406 at iteration 225280


********** Iteration 55 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1344, -0.1311, -0.1375, -0.1341, -0.1391, -0.1334, -0.1370, -0.1372,
        -0.1384, -0.1394, -0.1372, -0.1412, -0.1363, -0.1299, -0.1368],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.8893, -3.8447, -3.7867, -3.7300, -3.6799, -3.6152, -3.5532, -3.4780,
        -3.4041, -3.3210, -3.2363, -3.1430, -3.0429, -2.9481], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.8893, -3.8447, -3.7867, -3.7300, -3.6799, -3.6152, -3.5532, -3.4780,
        -3.4041, -3.3210, -3.2363, -3.1430, -3.0429, -2.9481], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1231, -0.1111, -0.1209, -0.1408, -0.1473, -0.1483, -0.1410, -0.1456,
        -0.1422, -0.1520, -0.1480, -0.1501, -0.1436, -0.1415, -0.1290],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6038, -3.5808, -3.5688, -3.5673, -3.5121, -3.4335, -3.3595, -3.2701,
        -3.1910, -3.0901, -2.9813, -2.8520, -2.7351, -2.6125], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6038, -3.5808, -3.5688, -3.5673, -3.5121, -3.4335, -3.3595, -3.2701,
        -3.1910, -3.0901, -2.9813, -2.8520, -2.7351, -2.6125], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1175, -0.1162, -0.1179, -0.1402, -0.1368, -0.1300, -0.1197, -0.1203,
        -0.1187, -0.1208, -0.1149, -0.1163, -0.1216, -0.1125, -0.1243],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.7489, -3.7089, -3.6779, -3.6479, -3.5831, -3.5192, -3.4631, -3.4068,
        -3.3381, -3.2648, -3.1862, -3.1059, -3.0161, -2.9180], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.7489, -3.7089, -3.6779, -3.6479, -3.5831, -3.5192, -3.4631, -3.4068,
        -3.3381, -3.2648, -3.1862, -3.1059, -3.0161, -2.9180], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1077, -0.0975, -0.0762, -0.0694, -0.0703, -0.0612, -0.0616, -0.0630,
        -0.0610, -0.0591, -0.0612, -0.0577, -0.0611, -0.0595, -0.0587],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1981, -3.1942, -3.1617, -3.1273, -3.0962, -3.0545, -3.0180, -2.9743,
        -2.9400, -2.8967, -2.8637, -2.8166, -2.7682, -2.7168], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1981, -3.1942, -3.1617, -3.1273, -3.0962, -3.0545, -3.0180, -2.9743,
        -2.9400, -2.8967, -2.8637, -2.8166, -2.7682, -2.7168], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0534, 0.0613, 0.0615, 0.0637, 0.0884, 0.0572, 0.0568, 0.2537, 0.0608,
        0.1141, 0.0586, 0.0705], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([11, 26, 23,  8, 27, 18, 14, 76, 17, 35, 19, 26]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_55.gif
Eval_AverageReturn : -297.0
Eval_StdReturn : 196.01275634765625
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -208.09091186523438
Train_StdReturn : 226.6240234375
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 186.18182373046875
Train_EnvstepsSoFar : 229376.0
TimeSinceStart : 6102.666618824005
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 45.67951965332031
Loss_Value : 0.44435394182801247
Loss_Entropy : 2.15770423412323
Loss_Representation : -12.97463059425354
Loss_KL : 1.3592370450496674
Loss_Obs : -1.3365286886692047
Loss_Reward : -0.9949633926153183
Loss_Discount : 0.026382753625512123
Loss_RawKL : 1.359237015247345
mean_target : -3.327596664428711
max_target : -2.872076392173767
min_target : -3.6791719794273376
std_target : 0.2617995887994766
Done logging...

Current epsilon: 0.0512112299724636 at iteration 229376


********** Iteration 56 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0590, -0.0570, -0.0527, -0.0555, -0.0698, -0.0724, -0.0725, -0.0728,
        -0.0786, -0.0760, -0.0877, -0.0821, -0.0809, -0.0658, -0.0602],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1446, -3.1327, -3.1148, -3.0926, -3.0799, -3.0582, -3.0317, -3.0124,
        -2.9926, -2.9723, -2.9438, -2.9066, -2.8735, -2.8248], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1446, -3.1327, -3.1148, -3.0926, -3.0799, -3.0582, -3.0317, -3.0124,
        -2.9926, -2.9723, -2.9438, -2.9066, -2.8735, -2.8248], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1105, -0.1137, -0.1133, -0.1216, -0.1114, -0.1093, -0.1163, -0.1109,
        -0.1122, -0.1171, -0.1161, -0.1105, -0.1117, -0.1115, -0.1090],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.7588, -3.7183, -3.6858, -3.6354, -3.5769, -3.5369, -3.4951, -3.4331,
        -3.3764, -3.3105, -3.2412, -3.1671, -3.1011, -3.0099], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.7588, -3.7183, -3.6858, -3.6354, -3.5769, -3.5369, -3.4951, -3.4331,
        -3.3764, -3.3105, -3.2412, -3.1671, -3.1011, -3.0099], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1144, -0.1209, -0.1171, -0.1154, -0.1135, -0.1117, -0.1098, -0.1151,
        -0.1127, -0.1082, -0.1143, -0.1133, -0.1081, -0.1163, -0.1161],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6257, -3.5884, -3.5264, -3.4837, -3.4396, -3.3701, -3.3162, -3.2456,
        -3.1759, -3.1001, -3.0333, -2.9450, -2.8469, -2.7556], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6257, -3.5884, -3.5264, -3.4837, -3.4396, -3.3701, -3.3162, -3.2456,
        -3.1759, -3.1001, -3.0333, -2.9450, -2.8469, -2.7556], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1030, -0.1015, -0.1024, -0.1069, -0.1074, -0.1071, -0.1017, -0.1231,
        -0.1229, -0.1061, -0.1129, -0.1024, -0.1040, -0.1045, -0.1146],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.7332, -3.6946, -3.6451, -3.6077, -3.5568, -3.5164, -3.4631, -3.4283,
        -3.3625, -3.2934, -3.2376, -3.1722, -3.0979, -3.0248], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.7332, -3.6946, -3.6451, -3.6077, -3.5568, -3.5164, -3.4631, -3.4283,
        -3.3625, -3.2934, -3.2376, -3.1722, -3.0979, -3.0248], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0378, 0.0511, 0.0445, 0.0530, 0.0893, 0.0373, 0.0414, 0.3619, 0.0498,
        0.1271, 0.0418, 0.0649], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([  9,  17,  10,  13,  24,   7,   9, 113,  12,  49,  14,  23]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_56.gif
Eval_AverageReturn : -304.29998779296875
Eval_StdReturn : 190.38490295410156
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 244.0
Train_AverageReturn : -257.9473571777344
Train_StdReturn : 211.55177307128906
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 233472.0
TimeSinceStart : 6221.433043003082
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 45.72789287567139
Loss_Value : 0.22549396008253098
Loss_Entropy : 1.9923605620861053
Loss_Representation : -13.12883973121643
Loss_KL : 1.1999624967575073
Loss_Obs : -1.3291528522968292
Loss_Reward : -1.067138597369194
Loss_Discount : 0.029864453244954348
Loss_RawKL : 1.199962466955185
mean_target : -3.326091945171356
max_target : -2.9604819416999817
min_target : -3.603399336338043
std_target : 0.2077525295317173
Done logging...

Current epsilon: 0.0510774614639663 at iteration 233472


********** Iteration 57 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0951, -0.0940, -0.0944, -0.0928, -0.0891, -0.0877, -0.0882, -0.0885,
        -0.0900, -0.0866, -0.0870, -0.0910, -0.0908, -0.0926, -0.0941],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6549, -3.6401, -3.6114, -3.5726, -3.5463, -3.5030, -3.4584, -3.4120,
        -3.3663, -3.3170, -3.2586, -3.2052, -3.1441, -3.0930], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6549, -3.6401, -3.6114, -3.5726, -3.5463, -3.5030, -3.4584, -3.4120,
        -3.3663, -3.3170, -3.2586, -3.2052, -3.1441, -3.0930], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1066, -0.1050, -0.1180, -0.1133, -0.1070, -0.1078, -0.1107, -0.1132,
        -0.1110, -0.1106, -0.1098, -0.1099, -0.1124, -0.1111, -0.1111],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.5445, -3.5208, -3.5049, -3.4839, -3.4568, -3.4429, -3.3992, -3.3664,
        -3.3125, -3.2620, -3.1994, -3.1178, -3.0391, -2.9456], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.5445, -3.5208, -3.5049, -3.4839, -3.4568, -3.4429, -3.3992, -3.3664,
        -3.3125, -3.2620, -3.1994, -3.1178, -3.0391, -2.9456], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0883, -0.0924, -0.0910, -0.0920, -0.0916, -0.0914, -0.0923, -0.0965,
        -0.1036, -0.1029, -0.1017, -0.1018, -0.1029, -0.1072, -0.1022],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6354, -3.6051, -3.5681, -3.5193, -3.4844, -3.4369, -3.3904, -3.3475,
        -3.2892, -3.2276, -3.1559, -3.0949, -3.0233, -2.9450], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6354, -3.6051, -3.5681, -3.5193, -3.4844, -3.4369, -3.3904, -3.3475,
        -3.2892, -3.2276, -3.1559, -3.0949, -3.0233, -2.9450], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1221, -0.1441, -0.1455, -0.1401, -0.1449, -0.1420, -0.1444, -0.1425,
        -0.1443, -0.1466, -0.1431, -0.1417, -0.1409, -0.1453, -0.1450],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.0387, -4.0284, -4.0449, -4.0347, -3.9877, -3.9239, -3.8533, -3.7784,
        -3.6901, -3.6187, -3.5262, -3.4679, -3.3740, -3.2677], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.0387, -4.0284, -4.0449, -4.0347, -3.9877, -3.9239, -3.8533, -3.7784,
        -3.6901, -3.6187, -3.5262, -3.4679, -3.3740, -3.2677], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0311, 0.0467, 0.0376, 0.0469, 0.0838, 0.0292, 0.0349, 0.4242, 0.0449,
        0.1243, 0.0350, 0.0616], device='cuda:0')
Count of actions: (array([7]), array([6]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_57.gif
Eval_AverageReturn : -301.0
Eval_StdReturn : 198.02525329589844
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -356.0
Train_StdReturn : 126.12427520751953
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 273.0666809082031
Train_EnvstepsSoFar : 237568.0
TimeSinceStart : 6333.899104595184
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 48.14881134033203
Loss_Value : 0.25292281806468964
Loss_Entropy : 1.8930431008338928
Loss_Representation : -12.562654256820679
Loss_KL : 2.0417217910289764
Loss_Obs : -1.3117879927158356
Loss_Reward : -1.510539561510086
Loss_Discount : 0.024043331388384104
Loss_RawKL : 2.0417217910289764
mean_target : -3.496205449104309
max_target : -3.070167124271393
min_target : -3.825787663459778
std_target : 0.2437538504600525
Done logging...

Current epsilon: 0.05095846637940368 at iteration 237568


********** Iteration 58 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1270, -0.1256, -0.1327, -0.1259, -0.1231, -0.1137, -0.1153, -0.1147,
        -0.1143, -0.1143, -0.1197, -0.1195, -0.1279, -0.1352, -0.1471],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.8215, -3.7855, -3.7380, -3.6948, -3.6375, -3.5784, -3.5301, -3.4643,
        -3.3923, -3.3189, -3.2391, -3.1457, -3.0643, -2.9564], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.8215, -3.7855, -3.7380, -3.6948, -3.6375, -3.5784, -3.5301, -3.4643,
        -3.3923, -3.3189, -3.2391, -3.1457, -3.0643, -2.9564], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0473, -0.0523, -0.0231, -0.0199, -0.0276, -0.0232, -0.0321, -0.0314,
        -0.0334, -0.0283, -0.0213, -0.0308, -0.0251, -0.0400, -0.0340],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.0816, -3.0747, -3.0434, -3.0430, -3.0380, -3.0303, -3.0390, -3.0281,
        -3.0121, -3.0067, -3.0010, -3.0084, -3.0033, -3.0022], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.0816, -3.0747, -3.0434, -3.0430, -3.0380, -3.0303, -3.0390, -3.0281,
        -3.0121, -3.0067, -3.0010, -3.0084, -3.0033, -3.0022], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1163, -0.1005, -0.0635, -0.0186, -0.0209, -0.0416, -0.0634, -0.0988,
        -0.0875, -0.1208, -0.1088, -0.1123, -0.0599, -0.0396, -0.0452],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4950, -3.4666, -3.4255, -3.4259, -3.4636, -3.4877, -3.4867, -3.4826,
        -3.4375, -3.4271, -3.3526, -3.3006, -3.2479, -3.2185], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4950, -3.4666, -3.4255, -3.4259, -3.4636, -3.4877, -3.4867, -3.4826,
        -3.4375, -3.4271, -3.3526, -3.3006, -3.2479, -3.2185], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0628, -0.0747, -0.0795, -0.0872, -0.1114, -0.1319, -0.1151, -0.0903,
        -0.0922, -0.0773, -0.0827, -0.0806, -0.1024, -0.1141, -0.1425],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6629, -3.6540, -3.6373, -3.6046, -3.5773, -3.5279, -3.4457, -3.3944,
        -3.3473, -3.2813, -3.2359, -3.1796, -3.1289, -3.0584], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6629, -3.6540, -3.6373, -3.6046, -3.5773, -3.5279, -3.4457, -3.3944,
        -3.3473, -3.2813, -3.2359, -3.1796, -3.1289, -3.0584], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0277, 0.0434, 0.0332, 0.0413, 0.0727, 0.0254, 0.0315, 0.4858, 0.0411,
        0.1098, 0.0316, 0.0565], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([ 12,  12,  16,  10,  29,  15,   8, 133,  13,  27,  13,  12]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_58.gif
Eval_AverageReturn : -219.10000610351562
Eval_StdReturn : 223.2516326904297
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 195.0
Train_AverageReturn : -278.72222900390625
Train_StdReturn : 200.36016845703125
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 241664.0
TimeSinceStart : 6445.47372174263
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 45.05339431762695
Loss_Value : 0.3078930452466011
Loss_Entropy : 1.7382997274398804
Loss_Representation : 5.396656394004822
Loss_KL : 2.581987679004669
Loss_Obs : -1.1812850832939148
Loss_Reward : 14.602991469204426
Loss_Discount : 0.024527262896299362
Loss_RawKL : 2.5819876194000244
mean_target : -3.270065426826477
max_target : -3.0179989337921143
min_target : -3.4655174016952515
std_target : 0.14997546188533306
Done logging...

Current epsilon: 0.05085261313853907 at iteration 241664


********** Iteration 59 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1788, -0.1697, -0.1713, -0.1754, -0.1687, -0.1682, -0.1753, -0.1823,
        -0.1850, -0.1767, -0.1695, -0.1741, -0.1762, -0.1840, -0.1912],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.4321, -4.3649, -4.3109, -4.2406, -4.1477, -4.0520, -3.9615, -3.8561,
        -3.7627, -3.6468, -3.5431, -3.4253, -3.3039, -3.1664], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.4321, -4.3649, -4.3109, -4.2406, -4.1477, -4.0520, -3.9615, -3.8561,
        -3.7627, -3.6468, -3.5431, -3.4253, -3.3039, -3.1664], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1370, -0.0919, -0.0684,  0.0088,  0.0296, -0.0128, -0.0893, -0.0726,
        -0.1161, -0.0930, -0.0424, -0.0228,  0.0398, -0.0484, -0.1068],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-2.5636, -2.4462, -2.3724, -2.3236, -2.3759, -2.4418, -2.4742, -2.4262,
        -2.3868, -2.2964, -2.2238, -2.2063, -2.2152, -2.2977], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-2.5636, -2.4462, -2.3724, -2.3236, -2.3759, -2.4418, -2.4742, -2.4262,
        -2.3868, -2.2964, -2.2238, -2.2063, -2.2152, -2.2977], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1367, -0.1449, -0.1395, -0.1581, -0.1458, -0.1346, -0.1553, -0.1458,
        -0.1626, -0.1573, -0.1585, -0.1512, -0.1661, -0.1531, -0.1515],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.2092, -4.1565, -4.0968, -4.0383, -3.9565, -3.9073, -3.8422, -3.7633,
        -3.6792, -3.5786, -3.4862, -3.3804, -3.2920, -3.1665], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.2092, -4.1565, -4.0968, -4.0383, -3.9565, -3.9073, -3.8422, -3.7633,
        -3.6792, -3.5786, -3.4862, -3.3804, -3.2920, -3.1665], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0981, -0.0999, -0.0788, -0.0856, -0.0991, -0.1161, -0.1039, -0.1002,
        -0.0967, -0.1121, -0.0997, -0.1068, -0.1051, -0.1044, -0.0914],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1714, -3.1324, -3.1009, -3.0897, -3.0741, -3.0340, -2.9812, -2.9267,
        -2.8669, -2.8088, -2.7345, -2.6661, -2.5833, -2.5022], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1714, -3.1324, -3.1009, -3.0897, -3.0741, -3.0340, -2.9812, -2.9267,
        -2.8669, -2.8088, -2.7345, -2.6661, -2.5833, -2.5022], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0280, 0.0445, 0.0333, 0.0405, 0.0678, 0.0257, 0.0323, 0.4974, 0.0415,
        0.1006, 0.0322, 0.0561], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([  5,  11,   9,   9,  18,   2,   8, 175,  12,  33,   5,  13]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_59.gif
Eval_AverageReturn : -304.3999938964844
Eval_StdReturn : 195.02113342285156
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 242.60000610351562
Train_AverageReturn : -280.3888854980469
Train_StdReturn : 195.67039489746094
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 245760.0
TimeSinceStart : 6563.530968904495
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 50.121055603027344
Loss_Value : 0.9050060296431184
Loss_Entropy : 1.7212917506694794
Loss_Representation : -9.571645021438599
Loss_KL : 2.77203768491745
Loss_Obs : -1.2119196355342865
Loss_Reward : -0.2516796290874481
Loss_Discount : 0.027193338610231876
Loss_RawKL : 2.772037625312805
mean_target : -3.6317951679229736
max_target : -3.1173848509788513
min_target : -4.037067949771881
std_target : 0.29309144988656044
Done logging...

Current epsilon: 0.050758450353221275 at iteration 245760
Saved model checkpoint to ./checkpoints/dreamerv2_basic_run1_iter_60.pt


********** Iteration 60 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0459, -0.0505, -0.0496, -0.0634, -0.0802, -0.0757, -0.0649, -0.0681,
        -0.0410, -0.0375, -0.0537, -0.0505, -0.0695, -0.0318, -0.0524],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.3622, -3.3449, -3.3207, -3.3021, -3.2758, -3.2361, -3.2041, -3.1955,
        -3.1553, -3.1425, -3.1284, -3.0854, -3.0652, -3.0003], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.3622, -3.3449, -3.3207, -3.3021, -3.2758, -3.2361, -3.2041, -3.1955,
        -3.1553, -3.1425, -3.1284, -3.0854, -3.0652, -3.0003], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0328, -0.0383, -0.0339, -0.0394, -0.0427, -0.0460, -0.0407, -0.0563,
        -0.0347, -0.0374, -0.0430, -0.0286, -0.0532, -0.0640, -0.0491],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.1645, -3.1572, -3.1406, -3.1212, -3.1042, -3.0917, -3.0776, -3.0482,
        -3.0194, -3.0074, -2.9830, -2.9555, -2.9357, -2.9007], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.1645, -3.1572, -3.1406, -3.1212, -3.1042, -3.0917, -3.0776, -3.0482,
        -3.0194, -3.0074, -2.9830, -2.9555, -2.9357, -2.9007], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0566, -0.0468, -0.0485, -0.0474, -0.0550, -0.0402, -0.0393, -0.0287,
        -0.0210, -0.0522, -0.0572, -0.0622, -0.0579, -0.0506, -0.0485],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4225, -3.4023, -3.3903, -3.3677, -3.3373, -3.3103, -3.3012, -3.2840,
        -3.2739, -3.2779, -3.2452, -3.2240, -3.1914, -3.1750], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4225, -3.4023, -3.3903, -3.3677, -3.3373, -3.3103, -3.3012, -3.2840,
        -3.2739, -3.2779, -3.2452, -3.2240, -3.1914, -3.1750], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0840, -0.0801, -0.0913, -0.0693, -0.0828, -0.0793, -0.0888, -0.0896,
        -0.0823, -0.0845, -0.0878, -0.0854, -0.0812, -0.0904, -0.0810],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.8648, -3.8335, -3.7962, -3.7636, -3.7424, -3.7189, -3.6752, -3.6285,
        -3.5851, -3.5463, -3.4961, -3.4375, -3.3803, -3.3219], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.8648, -3.8335, -3.7962, -3.7636, -3.7424, -3.7189, -3.6752, -3.6285,
        -3.5851, -3.5463, -3.4961, -3.4375, -3.3803, -3.3219], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0192, 0.0344, 0.0238, 0.0301, 0.0551, 0.0163, 0.0231, 0.6126, 0.0319,
        0.0846, 0.0224, 0.0464], device='cuda:0')
Count of actions: (array([4, 7]), array([1, 5]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_60.gif
Eval_AverageReturn : -302.0
Eval_StdReturn : 198.52203369140625
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -260.84210205078125
Train_StdReturn : 209.86669921875
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 249856.0
TimeSinceStart : 6676.087430715561
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 47.129637718200684
Loss_Value : 0.17313830740749836
Loss_Entropy : 1.464323490858078
Loss_Representation : -11.736837148666382
Loss_KL : 1.8083901703357697
Loss_Obs : -1.2972618639469147
Loss_Reward : -0.6004250794649124
Loss_Discount : 0.027816586662083864
Loss_RawKL : 1.8083901405334473
mean_target : -3.410477936267853
max_target : -3.209709882736206
min_target : -3.5686755180358887
std_target : 0.11464710533618927
Done logging...

Current epsilon: 0.05067468692693048 at iteration 249856


********** Iteration 61 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1324, -0.1313, -0.1302, -0.1350, -0.1205, -0.1260, -0.1309, -0.1300,
        -0.1269, -0.1311, -0.1297, -0.1318, -0.1262, -0.1425, -0.1343],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3552, -4.2942, -4.2401, -4.1856, -4.1201, -4.0575, -3.9927, -3.9151,
        -3.8349, -3.7475, -3.6539, -3.5716, -3.4809, -3.3745], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3552, -4.2942, -4.2401, -4.1856, -4.1201, -4.0575, -3.9927, -3.9151,
        -3.8349, -3.7475, -3.6539, -3.5716, -3.4809, -3.3745], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1552, -0.1505, -0.1460, -0.1553, -0.1441, -0.1439, -0.1507, -0.1575,
        -0.1503, -0.1501, -0.1582, -0.1556, -0.1537, -0.1590, -0.1548],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5576, -4.5011, -4.4493, -4.3952, -4.3207, -4.2459, -4.1932, -4.1035,
        -4.0094, -3.9464, -3.8392, -3.7191, -3.5991, -3.4750], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5576, -4.5011, -4.4493, -4.3952, -4.3207, -4.2459, -4.1932, -4.1035,
        -4.0094, -3.9464, -3.8392, -3.7191, -3.5991, -3.4750], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1365, -0.1363, -0.1399, -0.1401, -0.1355, -0.1352, -0.1358, -0.1374,
        -0.1360, -0.0934, -0.1136, -0.1291, -0.1300, -0.1315, -0.1237],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.2548, -4.2043, -4.1471, -4.0795, -4.0080, -3.9477, -3.8687, -3.8044,
        -3.7207, -3.6292, -3.5822, -3.5224, -3.4313, -3.3269], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.2548, -4.2043, -4.1471, -4.0795, -4.0080, -3.9477, -3.8687, -3.8044,
        -3.7207, -3.6292, -3.5822, -3.5224, -3.4313, -3.3269], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1081, -0.1082, -0.1137, -0.1066, -0.1076, -0.1059, -0.1044, -0.1066,
        -0.1133, -0.1125, -0.1170, -0.1139, -0.1162, -0.1123, -0.1198],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.0567, -4.0098, -3.9545, -3.8875, -3.8557, -3.7871, -3.7395, -3.6901,
        -3.6464, -3.5815, -3.5154, -3.4509, -3.3856, -3.2989], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.0567, -4.0098, -3.9545, -3.8875, -3.8557, -3.7871, -3.7395, -3.6901,
        -3.6464, -3.5815, -3.5154, -3.4509, -3.3856, -3.2989], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0157, 0.0301, 0.0201, 0.0253, 0.0473, 0.0131, 0.0197, 0.6675, 0.0282,
        0.0732, 0.0188, 0.0409], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([  6,  11,  12,  11,  16,   3,   5, 193,   8,  20,   3,  12]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_61.gif
Eval_AverageReturn : -205.0
Eval_StdReturn : 244.9591827392578
Eval_MaxReturn : 95.0
Eval_MinReturn : -410.0
Eval_AverageEpLen : 182.39999389648438
Train_AverageReturn : -283.1666564941406
Train_StdReturn : 198.3975372314453
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 253952.0
TimeSinceStart : 6785.721229791641
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 54.17013740539551
Loss_Value : 0.9620689898729324
Loss_Entropy : 1.3134581744670868
Loss_Representation : -12.156554937362671
Loss_KL : 1.6368962228298187
Loss_Obs : -1.294095516204834
Loss_Reward : -0.8825345486402512
Loss_Discount : 0.030038791242986917
Loss_RawKL : 1.6368962228298187
mean_target : -3.908811390399933
max_target : -3.37200266122818
min_target : -4.327558279037476
std_target : 0.3087827228009701
Done logging...

Current epsilon: 0.05060017435213467 at iteration 253952


********** Iteration 62 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0631, -0.0686, -0.0698, -0.0648, -0.0634, -0.0590, -0.0666, -0.0660,
        -0.0790, -0.0982, -0.1031, -0.0989, -0.0905, -0.0955, -0.0756],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6265, -3.6145, -3.5769, -3.5521, -3.5189, -3.4917, -3.4849, -3.4553,
        -3.4316, -3.3888, -3.3299, -3.2577, -3.2040, -3.1496], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6265, -3.6145, -3.5769, -3.5521, -3.5189, -3.4917, -3.4849, -3.4553,
        -3.4316, -3.3888, -3.3299, -3.2577, -3.2040, -3.1496], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0820, -0.0909, -0.0948, -0.0927, -0.1086, -0.0903, -0.0941, -0.0912,
        -0.0913, -0.0821, -0.0883, -0.0941, -0.0966, -0.0965, -0.1014],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.4830, -3.4573, -3.4243, -3.3909, -3.3659, -3.3162, -3.2935, -3.2583,
        -3.2094, -3.1683, -3.1237, -3.0603, -2.9928, -2.9261], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.4830, -3.4573, -3.4243, -3.3909, -3.3659, -3.3162, -3.2935, -3.2583,
        -3.2094, -3.1683, -3.1237, -3.0603, -2.9928, -2.9261], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1250, -0.1279, -0.1350, -0.1232, -0.1287, -0.1323, -0.1357, -0.1268,
        -0.1288, -0.1288, -0.1333, -0.1298, -0.1276, -0.1255, -0.1402],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.4437, -4.4101, -4.3660, -4.2827, -4.2428, -4.1987, -4.1360, -4.0625,
        -4.0032, -3.9330, -3.8416, -3.7534, -3.6843, -3.5662], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.4437, -4.4101, -4.3660, -4.2827, -4.2428, -4.1987, -4.1360, -4.0625,
        -4.0032, -3.9330, -3.8416, -3.7534, -3.6843, -3.5662], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1230, -0.1133, -0.1151, -0.1178, -0.1172, -0.1157, -0.1149, -0.1170,
        -0.1154, -0.1159, -0.1201, -0.1173, -0.1171, -0.1271, -0.1200],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3372, -4.2772, -4.2473, -4.1865, -4.1430, -4.0905, -4.0339, -3.9785,
        -3.9186, -3.8670, -3.7933, -3.7140, -3.6454, -3.5772], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3372, -4.2772, -4.2473, -4.1865, -4.1430, -4.0905, -4.0339, -3.9785,
        -3.9186, -3.8670, -3.7933, -3.7140, -3.6454, -3.5772], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0164, 0.0314, 0.0213, 0.0260, 0.0470, 0.0142, 0.0210, 0.6588, 0.0305,
        0.0713, 0.0202, 0.0420], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([  4,  14,  11,  12,  15,   7,   8, 157,  11,  35,   8,  18]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_62.gif
Eval_AverageReturn : -398.0
Eval_StdReturn : 2.4494898319244385
Eval_MaxReturn : -395.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 300.0
Train_AverageReturn : -330.25
Train_StdReturn : 162.5644073486328
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 258048.0
TimeSinceStart : 6911.546053409576
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 52.4268798828125
Loss_Value : 0.25792355462908745
Loss_Entropy : 1.3199053406715393
Loss_Representation : -12.81775188446045
Loss_KL : 1.4387447535991669
Loss_Obs : -1.3150943517684937
Loss_Reward : -1.1297485828399658
Loss_Discount : 0.02419523661956191
Loss_RawKL : 1.4387447535991669
mean_target : -3.784286141395569
max_target : -3.4018932580947876
min_target : -4.078675806522369
std_target : 0.21827396377921104
Done logging...

Current epsilon: 0.050533890962730026 at iteration 258048


********** Iteration 63 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0727, -0.0674, -0.0564, -0.0525, -0.0522, -0.0579, -0.0532, -0.0723,
        -0.0603, -0.0711, -0.0688, -0.0685, -0.0680, -0.0821, -0.0527],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.8835, -3.8651, -3.8542, -3.8438, -3.8324, -3.8188, -3.8044, -3.7893,
        -3.7566, -3.7259, -3.6915, -3.6655, -3.6229, -3.5911], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.8835, -3.8651, -3.8542, -3.8438, -3.8324, -3.8188, -3.8044, -3.7893,
        -3.7566, -3.7259, -3.6915, -3.6655, -3.6229, -3.5911], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1010, -0.0841, -0.0780, -0.0784, -0.0942, -0.0891, -0.0920, -0.0952,
        -0.0868, -0.0899, -0.0830, -0.0856, -0.0863, -0.0871, -0.0815],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.1779, -4.1463, -4.1480, -4.1297, -4.1200, -4.0880, -4.0574, -4.0187,
        -3.9558, -3.9309, -3.8793, -3.8415, -3.7927, -3.7277], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.1779, -4.1463, -4.1480, -4.1297, -4.1200, -4.0880, -4.0574, -4.0187,
        -3.9558, -3.9309, -3.8793, -3.8415, -3.7927, -3.7277], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1713, -0.1616, -0.1627, -0.1612, -0.1626, -0.1644, -0.1619, -0.1699,
        -0.1747, -0.1735, -0.1654, -0.1660, -0.1673, -0.1682, -0.1646],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.9582, -4.8956, -4.8298, -4.7618, -4.6949, -4.6097, -4.5135, -4.4238,
        -4.3220, -4.2102, -4.0957, -3.9960, -3.8781, -3.7544], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.9582, -4.8956, -4.8298, -4.7618, -4.6949, -4.6097, -4.5135, -4.4238,
        -4.3220, -4.2102, -4.0957, -3.9960, -3.8781, -3.7544], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1660, -0.1679, -0.1649, -0.1661, -0.1719, -0.1698, -0.1637, -0.1658,
        -0.1708, -0.1681, -0.1717, -0.1790, -0.1684, -0.1701, -0.1767],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.0175, -4.9743, -4.9191, -4.8629, -4.7922, -4.7133, -4.6259, -4.5465,
        -4.4593, -4.3574, -4.2478, -4.1340, -4.0040, -3.8806], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.0175, -4.9743, -4.9191, -4.8629, -4.7922, -4.7133, -4.6259, -4.5465,
        -4.4593, -4.3574, -4.2478, -4.1340, -4.0040, -3.8806], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0372, 0.0570, 0.0473, 0.0510, 0.0729, 0.0366, 0.0438, 0.3885, 0.0580,
        0.0939, 0.0449, 0.0691], device='cuda:0')
Count of actions: (array([4, 7, 9]), array([1, 4, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_63.gif
Eval_AverageReturn : -298.0
Eval_StdReturn : 196.50953674316406
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -259.52630615234375
Train_StdReturn : 206.3419189453125
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 262144.0
TimeSinceStart : 7023.534825086594
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 57.7255220413208
Loss_Value : 0.7807553159072995
Loss_Entropy : 1.9821800291538239
Loss_Representation : -12.126534461975098
Loss_KL : 1.706612467765808
Loss_Obs : -1.2972956001758575
Loss_Reward : -0.8810214176774025
Loss_Discount : 0.02083061821758747
Loss_RawKL : 1.706612378358841
mean_target : -4.1829692125320435
max_target : -3.7116411328315735
min_target : -4.543904423713684
std_target : 0.2682081237435341
Done logging...

Current epsilon: 0.05047492792564524 at iteration 262144


********** Iteration 64 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1145, -0.1096, -0.1054, -0.0997, -0.1004, -0.1076, -0.1086, -0.1004,
        -0.1067, -0.1123, -0.1294, -0.1445, -0.1230, -0.1292, -0.1269],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3889, -4.3557, -4.3282, -4.3062, -4.2728, -4.2480, -4.1967, -4.1491,
        -4.1090, -4.0566, -4.0010, -3.9171, -3.8227, -3.7444], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3889, -4.3557, -4.3282, -4.3062, -4.2728, -4.2480, -4.1967, -4.1491,
        -4.1090, -4.0566, -4.0010, -3.9171, -3.8227, -3.7444], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0469, -0.0402, -0.0465, -0.0572, -0.0571, -0.0629, -0.0581, -0.0658,
        -0.0596, -0.0821, -0.0686, -0.0811, -0.0656, -0.0656, -0.0594],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.8969, -3.9007, -3.9054, -3.9072, -3.8898, -3.8829, -3.8673, -3.8623,
        -3.8549, -3.8472, -3.8146, -3.7989, -3.7666, -3.7531], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.8969, -3.9007, -3.9054, -3.9072, -3.8898, -3.8829, -3.8673, -3.8623,
        -3.8549, -3.8472, -3.8146, -3.7989, -3.7666, -3.7531], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0575, -0.0532, -0.0514, -0.0529, -0.0745, -0.0809, -0.1196, -0.0920,
        -0.1626, -0.1553, -0.1574, -0.1371, -0.0682, -0.0664, -0.0540],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.1194, -4.1181, -4.1287, -4.1278, -4.1375, -4.1266, -4.0996, -4.0390,
        -4.0072, -3.8932, -3.7882, -3.6777, -3.5924, -3.5634], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.1194, -4.1181, -4.1287, -4.1278, -4.1375, -4.1266, -4.0996, -4.0390,
        -4.0072, -3.8932, -3.7882, -3.6777, -3.5924, -3.5634], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1608, -0.1574, -0.1268, -0.1245, -0.1231, -0.1231, -0.1279, -0.1241,
        -0.1235, -0.1297, -0.1349, -0.1447, -0.1447, -0.1574, -0.1559],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.4215, -4.3571, -4.2817, -4.2403, -4.2021, -4.1345, -4.0779, -4.0032,
        -3.9189, -3.8478, -3.7771, -3.6807, -3.5851, -3.4802], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.4215, -4.3571, -4.2817, -4.2403, -4.2021, -4.1345, -4.0779, -4.0032,
        -3.9189, -3.8478, -3.7771, -3.6807, -3.5851, -3.4802], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0401, 0.0601, 0.0512, 0.0540, 0.0756, 0.0406, 0.0476, 0.3525, 0.0627,
        0.0952, 0.0485, 0.0718], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([11, 23, 17, 15, 18, 10, 16, 98, 30, 29, 14, 19]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_64.gif
Eval_AverageReturn : -258.5
Eval_StdReturn : 211.18771362304688
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 219.3000030517578
Train_AverageReturn : -300.1764831542969
Train_StdReturn : 186.66082763671875
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 266240.0
TimeSinceStart : 7138.881432056427
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 55.769131660461426
Loss_Value : 0.18952215276658535
Loss_Entropy : 2.049338012933731
Loss_Representation : -11.710701704025269
Loss_KL : 1.9077267348766327
Loss_Obs : -1.3219450116157532
Loss_Reward : -0.418569877743721
Loss_Discount : 0.019591516349464655
Loss_RawKL : 1.9077267050743103
mean_target : -4.0449634194374084
max_target : -3.7077744603157043
min_target : -4.297409653663635
std_target : 0.1907611582428217
Done logging...

Current epsilon: 0.05042247677953624 at iteration 266240


********** Iteration 65 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1382, -0.1378, -0.1372, -0.1423, -0.1357, -0.1351, -0.1536, -0.1391,
        -0.1493, -0.1398, -0.1526, -0.1823, -0.1483, -0.1433, -0.1379],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5759, -4.5302, -4.4863, -4.4276, -4.3611, -4.3049, -4.2408, -4.1560,
        -4.0943, -4.0034, -3.9208, -3.8273, -3.6814, -3.5883], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5759, -4.5302, -4.4863, -4.4276, -4.3611, -4.3049, -4.2408, -4.1560,
        -4.0943, -4.0034, -3.9208, -3.8273, -3.6814, -3.5883], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0992, -0.1021, -0.1083, -0.1037, -0.0988, -0.1080, -0.1275, -0.1371,
        -0.1268, -0.1047, -0.0907, -0.0929, -0.0903, -0.0909, -0.0896],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3018, -4.2855, -4.2513, -4.2221, -4.2065, -4.1830, -4.1357, -4.0813,
        -3.9997, -3.9413, -3.8920, -3.8492, -3.8125, -3.7541], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3018, -4.2855, -4.2513, -4.2221, -4.2065, -4.1830, -4.1357, -4.0813,
        -3.9997, -3.9413, -3.8920, -3.8492, -3.8125, -3.7541], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0847, -0.0873, -0.0872, -0.0863, -0.0940, -0.1167, -0.1213, -0.1201,
        -0.1256, -0.0938, -0.0862, -0.0807, -0.0856, -0.0816, -0.0955],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.1051, -4.1042, -4.0772, -4.0674, -4.0471, -4.0234, -3.9675, -3.9012,
        -3.8435, -3.7840, -3.7330, -3.6892, -3.6486, -3.5928], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.1051, -4.1042, -4.0772, -4.0674, -4.0471, -4.0234, -3.9675, -3.9012,
        -3.8435, -3.7840, -3.7330, -3.6892, -3.6486, -3.5928], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1066, -0.1142, -0.1131, -0.1161, -0.1106, -0.1150, -0.1062, -0.1079,
        -0.1087, -0.1263, -0.1273, -0.1486, -0.1590, -0.1154, -0.1091],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3952, -4.3704, -4.3319, -4.3052, -4.2587, -4.2118, -4.1671, -4.1122,
        -4.0581, -4.0088, -3.9234, -3.8309, -3.7176, -3.5956], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3952, -4.3704, -4.3319, -4.3052, -4.2587, -4.2118, -4.1671, -4.1122,
        -4.0581, -4.0088, -3.9234, -3.8309, -3.7176, -3.5956], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0435, 0.0628, 0.0550, 0.0564, 0.0770, 0.0456, 0.0521, 0.3209, 0.0665,
        0.0946, 0.0522, 0.0735], device='cuda:0')
Count of actions: (array([6, 7, 9]), array([ 1, 17,  2]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_65.gif
Eval_AverageReturn : -346.0
Eval_StdReturn : 147.06800842285156
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 270.6000061035156
Train_AverageReturn : -239.0
Train_StdReturn : 216.7138671875
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 270336.0
TimeSinceStart : 7255.489683151245
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 55.961304664611816
Loss_Value : 0.13411170314066112
Loss_Entropy : 2.0930102467536926
Loss_Representation : -12.428043365478516
Loss_KL : 1.648994892835617
Loss_Obs : -1.267716884613037
Loss_Reward : -1.4269908368587494
Loss_Discount : 0.02712149964645505
Loss_RawKL : 1.6489948630332947
mean_target : -4.060015082359314
max_target : -3.6928447484970093
min_target : -4.347081184387207
std_target : 0.2097443751990795
Done logging...

Current epsilon: 0.05037581834971028 at iteration 270336


********** Iteration 66 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1184, -0.1190, -0.1235, -0.1221, -0.1207, -0.1190, -0.1295, -0.1278,
        -0.1204, -0.1238, -0.1209, -0.1220, -0.1204, -0.1195, -0.1157],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5681, -4.5391, -4.5031, -4.4625, -4.4156, -4.3890, -4.3459, -4.2832,
        -4.2195, -4.1660, -4.1038, -4.0347, -3.9637, -3.8822], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5681, -4.5391, -4.5031, -4.4625, -4.4156, -4.3890, -4.3459, -4.2832,
        -4.2195, -4.1660, -4.1038, -4.0347, -3.9637, -3.8822], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0948, -0.0977, -0.0870, -0.0982, -0.0909, -0.0875, -0.0894, -0.0904,
        -0.0881, -0.0959, -0.0951, -0.0999, -0.0912, -0.0877, -0.0957],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.2459, -4.2179, -4.1955, -4.1719, -4.1278, -4.0947, -4.0519, -4.0027,
        -3.9548, -3.9091, -3.8421, -3.7809, -3.7060, -3.6396], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.2459, -4.2179, -4.1955, -4.1719, -4.1278, -4.0947, -4.0519, -4.0027,
        -3.9548, -3.9091, -3.8421, -3.7809, -3.7060, -3.6396], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0970, -0.1000, -0.0963, -0.0933, -0.0941, -0.0941, -0.0951, -0.0930,
        -0.0924, -0.0945, -0.0947, -0.0947, -0.0945, -0.0940, -0.0950],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3724, -4.3458, -4.3047, -4.2717, -4.2503, -4.2218, -4.1766, -4.1344,
        -4.0938, -4.0433, -3.9910, -3.9390, -3.8803, -3.8236], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3724, -4.3458, -4.3047, -4.2717, -4.2503, -4.2218, -4.1766, -4.1344,
        -4.0938, -4.0433, -3.9910, -3.9390, -3.8803, -3.8236], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1601, -0.1415, -0.1359, -0.1372, -0.1347, -0.1367, -0.1401, -0.1370,
        -0.1372, -0.1398, -0.1385, -0.1392, -0.1403, -0.1379, -0.1421],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.7335, -4.6665, -4.6350, -4.5925, -4.5319, -4.4734, -4.4083, -4.3268,
        -4.2459, -4.1608, -4.0739, -3.9838, -3.8910, -3.7993], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.7335, -4.6665, -4.6350, -4.5925, -4.5319, -4.4734, -4.4083, -4.3268,
        -4.2459, -4.1608, -4.0739, -3.9838, -3.8910, -3.7993], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0587, 0.0722, 0.0724, 0.0698, 0.0826, 0.0636, 0.0654, 0.1954, 0.0774,
        0.0928, 0.0678, 0.0819], device='cuda:0')
Count of actions: (array([ 7,  9, 10]), array([4, 1, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_66.gif
Eval_AverageReturn : -162.39999389648438
Eval_StdReturn : 231.01263427734375
Eval_MaxReturn : 94.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 162.89999389648438
Train_AverageReturn : -351.6666564941406
Train_StdReturn : 124.70854949951172
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 273.0666809082031
Train_EnvstepsSoFar : 274432.0
TimeSinceStart : 7356.710348367691
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 57.67142963409424
Loss_Value : 0.20525810273829848
Loss_Entropy : 2.308253765106201
Loss_Representation : -13.0460205078125
Loss_KL : 1.4890816807746887
Loss_Obs : -1.286444753408432
Loss_Reward : -1.6906871497631073
Loss_Discount : 0.02003258466720581
Loss_RawKL : 1.4890816509723663
mean_target : -4.188740253448486
max_target : -3.8133667707443237
min_target : -4.4772772789001465
std_target : 0.21370453387498856
Done logging...

Current epsilon: 0.05033431288728814 at iteration 274432


********** Iteration 67 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1195, -0.1205, -0.1134, -0.1178, -0.1192, -0.1170, -0.1194, -0.1194,
        -0.1173, -0.1195, -0.1214, -0.1322, -0.1534, -0.1525, -0.1415],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5475, -4.5092, -4.4708, -4.4279, -4.3804, -4.3335, -4.2762, -4.2079,
        -4.1337, -4.0556, -3.9779, -3.8914, -3.7938, -3.6732], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5475, -4.5092, -4.4708, -4.4279, -4.3804, -4.3335, -4.2762, -4.2079,
        -4.1337, -4.0556, -3.9779, -3.8914, -3.7938, -3.6732], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1026, -0.1075, -0.0690, -0.0558, -0.0440, -0.0429, -0.0407, -0.0382,
        -0.0392, -0.0407, -0.0381, -0.0418, -0.0453, -0.0472, -0.0609],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.6868, -3.6428, -3.5743, -3.5315, -3.5092, -3.4884, -3.4581, -3.4182,
        -3.3831, -3.3480, -3.3091, -3.2855, -3.2510, -3.2148], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.6868, -3.6428, -3.5743, -3.5315, -3.5092, -3.4884, -3.4581, -3.4182,
        -3.3831, -3.3480, -3.3091, -3.2855, -3.2510, -3.2148], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0436, -0.0413, -0.0428, -0.0411, -0.0412, -0.0428, -0.0552, -0.0647,
        -0.1056, -0.1414, -0.1146, -0.1076, -0.0892, -0.0650, -0.0640],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.9282, -3.9359, -3.9341, -3.9325, -3.9314, -3.9283, -3.9243, -3.9220,
        -3.9079, -3.8649, -3.7778, -3.7158, -3.6398, -3.6025], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.9282, -3.9359, -3.9341, -3.9325, -3.9314, -3.9283, -3.9243, -3.9220,
        -3.9079, -3.8649, -3.7778, -3.7158, -3.6398, -3.6025], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1103, -0.1015, -0.1003, -0.0977, -0.1015, -0.0939, -0.0934, -0.0967,
        -0.0928, -0.0920, -0.0928, -0.0927, -0.0908, -0.0912, -0.0916],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3480, -4.3009, -4.2606, -4.2325, -4.1867, -4.1372, -4.0960, -4.0463,
        -3.9862, -3.9355, -3.8855, -3.8293, -3.7655, -3.7104], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3480, -4.3009, -4.2606, -4.2325, -4.1867, -4.1372, -4.0960, -4.0463,
        -3.9862, -3.9355, -3.8855, -3.8293, -3.7655, -3.7104], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0390, 0.0625, 0.0534, 0.0563, 0.0848, 0.0407, 0.0498, 0.3161, 0.0717,
        0.0991, 0.0491, 0.0775], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([14, 24, 25, 16, 29, 18, 18, 60, 26, 31, 17, 22]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_67.gif
Eval_AverageReturn : -158.60000610351562
Eval_StdReturn : 238.50709533691406
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 159.10000610351562
Train_AverageReturn : -221.61904907226562
Train_StdReturn : 222.86434936523438
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 278528.0
TimeSinceStart : 7463.826558113098
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 53.75299644470215
Loss_Value : 0.24676215020008385
Loss_Entropy : 2.1267359256744385
Loss_Representation : -9.71746289730072
Loss_KL : 1.7456814050674438
Loss_Obs : -1.2363206446170807
Loss_Reward : 0.8702251017093658
Loss_Discount : 0.029837063048034906
Loss_RawKL : 1.7456814050674438
mean_target : -3.9030455350875854
max_target : -3.6248582005500793
min_target : -4.125409305095673
std_target : 0.15980266965925694
Done logging...

Current epsilon: 0.050297391297399636 at iteration 278528


********** Iteration 68 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1661, -0.1688, -0.1420, -0.1464, -0.1551, -0.1479, -0.1622, -0.1668,
        -0.1659, -0.1580, -0.1561, -0.1508, -0.1634, -0.1187, -0.1360],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.4000, -4.3157, -4.2277, -4.1707, -4.1008, -4.0359, -3.9568, -3.8664,
        -3.7580, -3.6379, -3.5240, -3.4047, -3.2740, -3.1372], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.4000, -4.3157, -4.2277, -4.1707, -4.1008, -4.0359, -3.9568, -3.8664,
        -3.7580, -3.6379, -3.5240, -3.4047, -3.2740, -3.1372], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1939, -0.1929, -0.1998, -0.1909, -0.1748, -0.1802, -0.1434, -0.1550,
        -0.1429, -0.1210, -0.0317, -0.0286, -0.0607, -0.1440, -0.1643],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5258, -4.4163, -4.2993, -4.1771, -4.0453, -3.9242, -3.8060, -3.7108,
        -3.6032, -3.5130, -3.4590, -3.4695, -3.4955, -3.5033], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5258, -4.4163, -4.2993, -4.1771, -4.0453, -3.9242, -3.8060, -3.7108,
        -3.6032, -3.5130, -3.4590, -3.4695, -3.4955, -3.5033], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1396, -0.1468, -0.1405, -0.1442, -0.1517, -0.1469, -0.1547, -0.1500,
        -0.1493, -0.1507, -0.1538, -0.1591, -0.1604, -0.1556, -0.1440],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.9009, -4.8650, -4.8148, -4.7662, -4.6990, -4.6365, -4.5676, -4.4782,
        -4.3851, -4.2817, -4.1790, -4.0696, -3.9618, -3.8530], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.9009, -4.8650, -4.8148, -4.7662, -4.6990, -4.6365, -4.5676, -4.4782,
        -4.3851, -4.2817, -4.1790, -4.0696, -3.9618, -3.8530], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1211, -0.1105, -0.1271, -0.0912, -0.1056, -0.0784, -0.0645, -0.0731,
        -0.0665, -0.0685, -0.0685, -0.0617, -0.0617, -0.0692, -0.0696],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3390, -4.2768, -4.2323, -4.1771, -4.1513, -4.0991, -4.0729, -4.0554,
        -4.0338, -4.0067, -3.9758, -3.9384, -3.9077, -3.8737], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3390, -4.2768, -4.2323, -4.1771, -4.1513, -4.0991, -4.0729, -4.0554,
        -4.0338, -4.0067, -3.9758, -3.9384, -3.9077, -3.8737], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0413, 0.0668, 0.0564, 0.0604, 0.0903, 0.0434, 0.0545, 0.2723, 0.0777,
        0.1023, 0.0520, 0.0827], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([10, 22, 18, 17, 32, 18, 13, 83, 22, 27,  9, 29]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_68.gif
Eval_AverageReturn : -352.70001220703125
Eval_StdReturn : 130.24826049804688
Eval_MaxReturn : 38.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 274.79998779296875
Train_AverageReturn : -322.4375
Train_StdReturn : 151.543701171875
Train_MaxReturn : 76.0
Train_MinReturn : -400.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 282624.0
TimeSinceStart : 7586.365793943405
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 61.0372838973999
Loss_Value : 1.2701887302100658
Loss_Entropy : 2.2285397052764893
Loss_Representation : -9.767587542533875
Loss_KL : 2.0006269216537476
Loss_Obs : -1.245782047510147
Loss_Reward : 0.6635740101337433
Loss_Discount : 0.026031671557575464
Loss_RawKL : 2.0006269216537476
mean_target : -4.426614284515381
max_target : -3.8684059977531433
min_target : -4.845185875892639
std_target : 0.31593725271523
Done logging...

Current epsilon: 0.050264547336139075 at iteration 282624


********** Iteration 69 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0302, -0.0570, -0.0415, -0.0511, -0.0294, -0.0583, -0.0384, -0.0862,
        -0.0728, -0.1125, -0.0804, -0.0993, -0.1086, -0.0564, -0.0300],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.0233, -4.0454, -4.0362, -4.0321, -4.0391, -4.0501, -4.0288, -4.0343,
        -3.9858, -3.9641, -3.8865, -3.8453, -3.8406, -3.7842], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.0233, -4.0454, -4.0362, -4.0321, -4.0391, -4.0501, -4.0288, -4.0343,
        -3.9858, -3.9641, -3.8865, -3.8453, -3.8406, -3.7842], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0448, -0.0712, -0.0836, -0.0730, -0.1401, -0.1099, -0.0645, -0.0714,
        -0.0801, -0.0567, -0.0508, -0.0421, -0.0555, -0.0588, -0.0815],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.2164, -4.2320, -4.2066, -4.1735, -4.1700, -4.0786, -4.0189, -4.0276,
        -4.0131, -3.9978, -4.0016, -4.0201, -4.0269, -4.0214], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.2164, -4.2320, -4.2066, -4.1735, -4.1700, -4.0786, -4.0189, -4.0276,
        -4.0131, -3.9978, -4.0016, -4.0201, -4.0269, -4.0214], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0586, -0.0733, -0.0739, -0.0962, -0.1228, -0.1170, -0.1079, -0.1277,
        -0.1029, -0.0997, -0.1106, -0.0958, -0.0819, -0.1146, -0.0940],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5209, -4.5382, -4.5299, -4.5212, -4.4899, -4.4252, -4.3777, -4.3413,
        -4.2727, -4.2405, -4.2094, -4.1563, -4.1115, -4.0860], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5209, -4.5382, -4.5299, -4.5212, -4.4899, -4.4252, -4.3777, -4.3413,
        -4.2727, -4.2405, -4.2094, -4.1563, -4.1115, -4.0860], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1290, -0.1362, -0.1295, -0.1440, -0.1443, -0.1698, -0.1889, -0.1856,
        -0.1891, -0.1550, -0.1436, -0.1391, -0.1358, -0.1426, -0.1425],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.0551, -5.0282, -4.9755, -4.9228, -4.8559, -4.7898, -4.6854, -4.5556,
        -4.4295, -4.2979, -4.1968, -4.0985, -4.0045, -3.9040], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.0551, -5.0282, -4.9755, -4.9228, -4.8559, -4.7898, -4.6854, -4.5556,
        -4.4295, -4.2979, -4.1968, -4.0985, -4.0045, -3.9040], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0443, 0.0711, 0.0583, 0.0631, 0.0916, 0.0475, 0.0603, 0.2390, 0.0817,
        0.1032, 0.0553, 0.0846], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([21, 26, 15, 20, 25, 16, 16, 62, 24, 26, 20, 29]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_69.gif
Eval_AverageReturn : -297.3999938964844
Eval_StdReturn : 191.52243041992188
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 242.60000610351562
Train_AverageReturn : -298.4117736816406
Train_StdReturn : 180.47781372070312
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 286720.0
TimeSinceStart : 7704.763023376465
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 57.97494697570801
Loss_Value : 0.39018610306084156
Loss_Entropy : 2.2905187010765076
Loss_Representation : -12.043923616409302
Loss_KL : 1.5915922820568085
Loss_Obs : -1.3003864288330078
Loss_Reward : -0.6572672575712204
Loss_Discount : 0.025615903548896313
Loss_RawKL : 1.591592252254486
mean_target : -4.209826111793518
max_target : -3.967180907726288
min_target : -4.387139618396759
std_target : 0.1368989022448659
Done logging...

Current epsilon: 0.05023533066929068 at iteration 286720


********** Iteration 70 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1590, -0.1492, -0.1435, -0.1488, -0.1472, -0.1483, -0.1548, -0.1633,
        -0.1697, -0.1701, -0.1769, -0.1721, -0.1655, -0.1602, -0.1633],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.0631, -5.0049, -4.9506, -4.8866, -4.8196, -4.7423, -4.6657, -4.5779,
        -4.4773, -4.3640, -4.2434, -4.1110, -3.9855, -3.8568], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.0631, -5.0049, -4.9506, -4.8866, -4.8196, -4.7423, -4.6657, -4.5779,
        -4.4773, -4.3640, -4.2434, -4.1110, -3.9855, -3.8568], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1012, -0.0620, -0.0363, -0.0300, -0.0200, -0.0232, -0.0110, -0.0737,
        -0.0912, -0.0520, -0.0324, -0.0447, -0.0705, -0.0581, -0.0692],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-3.0397, -2.9657, -2.9402, -2.9447, -2.9651, -3.0030, -3.0569, -3.1289,
        -3.1378, -3.0929, -3.0809, -3.0939, -3.0957, -3.0753], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-3.0397, -2.9657, -2.9402, -2.9447, -2.9651, -3.0030, -3.0569, -3.1289,
        -3.1378, -3.0929, -3.0809, -3.0939, -3.0957, -3.0753], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1427, -0.1125, -0.0970, -0.0972, -0.0791, -0.0794, -0.0810, -0.0858,
        -0.0806, -0.0885, -0.0787, -0.0813, -0.0709, -0.0884, -0.0900],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5995, -4.5345, -4.4931, -4.4552, -4.4221, -4.4050, -4.3733, -4.3389,
        -4.2888, -4.2471, -4.2020, -4.1664, -4.1259, -4.0870], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5995, -4.5345, -4.4931, -4.4552, -4.4221, -4.4050, -4.3733, -4.3389,
        -4.2888, -4.2471, -4.2020, -4.1664, -4.1259, -4.0870], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0821, -0.0846, -0.0857, -0.0822, -0.0821, -0.0698, -0.0731, -0.0826,
        -0.0761, -0.1054, -0.1036, -0.1116, -0.1470, -0.1134, -0.1160],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.5978, -4.5756, -4.5461, -4.5147, -4.4797, -4.4364, -4.4085, -4.3808,
        -4.3397, -4.3012, -4.2323, -4.1620, -4.0784, -3.9577], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.5978, -4.5756, -4.5461, -4.5147, -4.4797, -4.4364, -4.4085, -4.3808,
        -4.3397, -4.3012, -4.2323, -4.1620, -4.0784, -3.9577], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0593, 0.0733, 0.0726, 0.0722, 0.0856, 0.0642, 0.0677, 0.1803, 0.0805,
        0.0911, 0.0684, 0.0848], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([22, 28, 20, 22, 21, 14, 15, 56, 20, 36, 20, 26]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_70.gif
Eval_AverageReturn : -211.39999389648438
Eval_StdReturn : 217.68563842773438
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 194.8000030517578
Train_AverageReturn : -275.6666564941406
Train_StdReturn : 195.68626403808594
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 290816.0
TimeSinceStart : 7816.5463881492615
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 60.38835525512695
Loss_Value : 0.6854058224707842
Loss_Entropy : 2.3304580450057983
Loss_Representation : -12.599845886230469
Loss_KL : 1.8388260304927826
Loss_Obs : -1.353592574596405
Loss_Reward : -0.9309553354978561
Loss_Discount : 0.028208952397108078
Loss_RawKL : 1.8388260006904602
mean_target : -4.383427977561951
max_target : -3.9544360637664795
min_target : -4.720829725265503
std_target : 0.2442317232489586
Done logging...

Current epsilon: 0.05020934069764999 at iteration 290816


********** Iteration 71 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1207, -0.1155, -0.1205, -0.1185, -0.1174, -0.1205, -0.1166, -0.1288,
        -0.1262, -0.1400, -0.1429, -0.1531, -0.1471, -0.1301, -0.1257],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.9436, -4.9093, -4.8674, -4.8198, -4.7676, -4.7222, -4.6755, -4.6217,
        -4.5551, -4.4896, -4.4038, -4.3146, -4.2041, -4.1020], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.9436, -4.9093, -4.8674, -4.8198, -4.7676, -4.7222, -4.6755, -4.6217,
        -4.5551, -4.4896, -4.4038, -4.3146, -4.2041, -4.1020], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1251, -0.1206, -0.1279, -0.1207, -0.1232, -0.1173, -0.1292, -0.1525,
        -0.1669, -0.1663, -0.1420, -0.1255, -0.1215, -0.1193, -0.1220],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.8740, -4.8359, -4.8038, -4.7475, -4.6999, -4.6398, -4.6031, -4.5384,
        -4.4540, -4.3469, -4.2368, -4.1458, -4.0646, -3.9682], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.8740, -4.8359, -4.8038, -4.7475, -4.6999, -4.6398, -4.6031, -4.5384,
        -4.4540, -4.3469, -4.2368, -4.1458, -4.0646, -3.9682], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1022, -0.1245, -0.1418, -0.1597, -0.1474, -0.1744, -0.1581, -0.1573,
        -0.1502, -0.1137, -0.0911, -0.0662, -0.0878, -0.0846, -0.1120],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.3487, -4.3136, -4.2703, -4.2129, -4.1471, -4.0950, -4.0104, -3.9252,
        -3.8197, -3.7237, -3.6428, -3.5708, -3.5316, -3.4588], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.3487, -4.3136, -4.2703, -4.2129, -4.1471, -4.0950, -4.0104, -3.9252,
        -3.8197, -3.7237, -3.6428, -3.5708, -3.5316, -3.4588], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0918, -0.0907, -0.0760, -0.0764, -0.0712, -0.0692, -0.0697, -0.0635,
        -0.0696, -0.0670, -0.0675, -0.0847, -0.0771, -0.0800, -0.0709],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.4933, -4.4653, -4.4313, -4.4080, -4.3780, -4.3569, -4.3347, -4.3089,
        -4.2839, -4.2498, -4.2207, -4.1860, -4.1299, -4.0746], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.4933, -4.4653, -4.4313, -4.4080, -4.3780, -4.3569, -4.3347, -4.3089,
        -4.2839, -4.2498, -4.2207, -4.1860, -4.1299, -4.0746], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0677, 0.0736, 0.0810, 0.0769, 0.0817, 0.0748, 0.0708, 0.1512, 0.0794,
        0.0837, 0.0757, 0.0833], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([29, 21, 26, 16, 27, 17, 25, 40, 28, 20, 19, 32]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_71.gif
Eval_AverageReturn : -249.1999969482422
Eval_StdReturn : 213.0947265625
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 216.0
Train_AverageReturn : -255.3157958984375
Train_StdReturn : 208.0015411376953
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 294912.0
TimeSinceStart : 7930.586456775665
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 63.322622299194336
Loss_Value : 0.45079991966485977
Loss_Entropy : 2.347854197025299
Loss_Representation : -12.967367172241211
Loss_KL : 1.7284215688705444
Loss_Obs : -1.3446286916732788
Loss_Reward : -1.2738586962223053
Loss_Discount : 0.024357153568416834
Loss_RawKL : 1.728421539068222
mean_target : -4.593509078025818
max_target : -4.20036506652832
min_target : -4.886723041534424
std_target : 0.22074905037879944
Done logging...

Current epsilon: 0.050186221064278086 at iteration 294912


********** Iteration 72 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1247, -0.1242, -0.1228, -0.1281, -0.1180, -0.1246, -0.1299, -0.1289,
        -0.1549, -0.1612, -0.1537, -0.1615, -0.1443, -0.1249, -0.1208],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.8051, -4.7611, -4.7308, -4.6859, -4.6286, -4.5777, -4.5132, -4.4481,
        -4.3784, -4.2875, -4.1914, -4.0848, -3.9695, -3.8717], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.8051, -4.7611, -4.7308, -4.6859, -4.6286, -4.5777, -4.5132, -4.4481,
        -4.3784, -4.2875, -4.1914, -4.0848, -3.9695, -3.8717], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1273, -0.1209, -0.1203, -0.1239, -0.1202, -0.1238, -0.1220, -0.1225,
        -0.1208, -0.1259, -0.1252, -0.1300, -0.1319, -0.1353, -0.1493],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.1123, -5.0797, -5.0518, -5.0087, -4.9526, -4.8962, -4.8368, -4.7692,
        -4.7039, -4.6347, -4.5534, -4.4718, -4.3864, -4.3039], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.1123, -5.0797, -5.0518, -5.0087, -4.9526, -4.8962, -4.8368, -4.7692,
        -4.7039, -4.6347, -4.5534, -4.4718, -4.3864, -4.3039], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0672, -0.0650, -0.0637, -0.0628, -0.0616, -0.0648, -0.0635, -0.0671,
        -0.0619, -0.0643, -0.0706, -0.0832, -0.0930, -0.1499, -0.1186],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.6011, -4.5974, -4.5913, -4.5734, -4.5591, -4.5454, -4.5303, -4.5074,
        -4.4833, -4.4639, -4.4410, -4.4069, -4.3552, -4.3012], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.6011, -4.5974, -4.5913, -4.5734, -4.5591, -4.5454, -4.5303, -4.5074,
        -4.4833, -4.4639, -4.4410, -4.4069, -4.3552, -4.3012], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1320, -0.1330, -0.0897, -0.0770, -0.0717, -0.0698, -0.0683, -0.0657,
        -0.0652, -0.0657, -0.0639, -0.0647, -0.0667, -0.0702, -0.0729],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.7380, -4.6996, -4.6453, -4.6526, -4.6389, -4.6246, -4.6126, -4.5981,
        -4.5824, -4.5623, -4.5462, -4.5334, -4.5145, -4.4895], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.7380, -4.6996, -4.6453, -4.6526, -4.6389, -4.6246, -4.6126, -4.5981,
        -4.5824, -4.5623, -4.5462, -4.5334, -4.5145, -4.4895], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0677, 0.0734, 0.0815, 0.0763, 0.0815, 0.0763, 0.0715, 0.1520, 0.0793,
        0.0825, 0.0754, 0.0826], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([18, 17, 31, 30, 20, 25, 17, 39, 32, 21, 20, 30]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_72.gif
Eval_AverageReturn : -294.5
Eval_StdReturn : 189.82162475585938
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 242.6999969482422
Train_AverageReturn : -272.3333435058594
Train_StdReturn : 192.7860107421875
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 299008.0
TimeSinceStart : 8048.746627807617
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 65.3750696182251
Loss_Value : 0.2948933020234108
Loss_Entropy : 2.3473796248435974
Loss_Representation : -12.303600788116455
Loss_KL : 2.2300610542297363
Loss_Obs : -1.3454364836215973
Loss_Reward : -1.109885323792696
Loss_Discount : 0.03058799309656024
Loss_RawKL : 2.2300610542297363
mean_target : -4.740068674087524
max_target : -4.366443753242493
min_target : -5.028267979621887
std_target : 0.21008192375302315
Done logging...

Current epsilon: 0.05016565476837592 at iteration 299008


********** Iteration 73 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1245, -0.1216, -0.1257, -0.1239, -0.1270, -0.1349, -0.1511, -0.1681,
        -0.1797, -0.1377, -0.1276, -0.1262, -0.1204, -0.1235, -0.1219],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.1867, -5.1610, -5.1418, -5.0952, -5.0686, -5.0244, -4.9667, -4.9001,
        -4.8018, -4.6971, -4.6306, -4.5517, -4.4715, -4.3955], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.1867, -5.1610, -5.1418, -5.0952, -5.0686, -5.0244, -4.9667, -4.9001,
        -4.8018, -4.6971, -4.6306, -4.5517, -4.4715, -4.3955], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1733, -0.1640, -0.1292, -0.1379, -0.1614, -0.1487, -0.1677, -0.1265,
        -0.1326, -0.1253, -0.1198, -0.1204, -0.1241, -0.1219, -0.1237],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.9544, -4.9558, -4.9015, -4.8671, -4.8072, -4.7257, -4.6586, -4.5765,
        -4.5142, -4.4268, -4.3684, -4.2956, -4.2183, -4.1398], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.9544, -4.9558, -4.9015, -4.8671, -4.8072, -4.7257, -4.6586, -4.5765,
        -4.5142, -4.4268, -4.3684, -4.2956, -4.2183, -4.1398], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0718, -0.0675, -0.0692, -0.0722, -0.0678, -0.0799, -0.0715, -0.1003,
        -0.1181, -0.1664, -0.1460, -0.1033, -0.0883, -0.0801, -0.0725],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.8876, -4.9057, -4.9126, -4.9193, -4.9188, -4.9310, -4.9208, -4.9239,
        -4.8858, -4.8343, -4.7289, -4.6504, -4.6101, -4.5765], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.8876, -4.9057, -4.9126, -4.9193, -4.9188, -4.9310, -4.9208, -4.9239,
        -4.8858, -4.8343, -4.7289, -4.6504, -4.6101, -4.5765], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0750, -0.0772, -0.0750, -0.0981, -0.0893, -0.1099, -0.1118, -0.1288,
        -0.1485, -0.1914, -0.1780, -0.0939, -0.1064, -0.0867, -0.0798],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.0859, -5.1056, -5.1150, -5.1273, -5.1044, -5.0886, -5.0596, -5.0154,
        -4.9601, -4.8780, -4.7473, -4.6241, -4.5826, -4.5203], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.0859, -5.1056, -5.1150, -5.1273, -5.1044, -5.0886, -5.0596, -5.0154,
        -4.9601, -4.8780, -4.7473, -4.6241, -4.5826, -4.5203], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0649, 0.0750, 0.0798, 0.0760, 0.0841, 0.0733, 0.0732, 0.1484, 0.0817,
        0.0857, 0.0738, 0.0842], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([17, 24, 27, 20, 34, 18, 24, 44, 19, 35, 17, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_73.gif
Eval_AverageReturn : -244.10000610351562
Eval_StdReturn : 221.8023681640625
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 211.89999389648438
Train_AverageReturn : -235.25
Train_StdReturn : 211.98936462402344
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 303104.0
TimeSinceStart : 8162.274075746536
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 68.30696868896484
Loss_Value : 0.217284114100039
Loss_Entropy : 2.374905288219452
Loss_Representation : -12.39134693145752
Loss_KL : 2.339552164077759
Loss_Obs : -1.3566708862781525
Loss_Reward : -1.186423659324646
Loss_Discount : 0.02223374228924513
Loss_RawKL : 2.339552164077759
mean_target : -4.950351119041443
max_target : -4.556563973426819
min_target : -5.23624587059021
std_target : 0.21624545753002167
Done logging...

Current epsilon: 0.050147359818783446 at iteration 303104


********** Iteration 74 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1202, -0.1179, -0.1159, -0.1193, -0.1202, -0.1187, -0.1207, -0.1187,
        -0.1122, -0.1180, -0.1211, -0.1251, -0.1229, -0.1338, -0.1195],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.1343, -5.0995, -5.0674, -5.0390, -4.9976, -4.9414, -4.8933, -4.8292,
        -4.7603, -4.6970, -4.6317, -4.5752, -4.4986, -4.4121], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.1343, -5.0995, -5.0674, -5.0390, -4.9976, -4.9414, -4.8933, -4.8292,
        -4.7603, -4.6970, -4.6317, -4.5752, -4.4986, -4.4121], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1139, -0.1148, -0.1138, -0.1133, -0.1212, -0.1153, -0.1143, -0.1211,
        -0.1152, -0.1173, -0.1200, -0.1170, -0.1263, -0.1142, -0.1195],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2299, -5.2109, -5.1831, -5.1505, -5.1240, -5.0761, -5.0383, -4.9880,
        -4.9411, -4.8894, -4.8329, -4.7633, -4.6974, -4.6094], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2299, -5.2109, -5.1831, -5.1505, -5.1240, -5.0761, -5.0383, -4.9880,
        -4.9411, -4.8894, -4.8329, -4.7633, -4.6974, -4.6094], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0685, -0.0706, -0.0667, -0.0679, -0.0672, -0.0675, -0.0629, -0.0690,
        -0.0871, -0.1166, -0.1341, -0.1072, -0.0720, -0.0679, -0.0676],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.8156, -4.8239, -4.8354, -4.8323, -4.8278, -4.8211, -4.8150, -4.8104,
        -4.7897, -4.7561, -4.7087, -4.6258, -4.5769, -4.5529], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.8156, -4.8239, -4.8354, -4.8323, -4.8278, -4.8211, -4.8150, -4.8104,
        -4.7897, -4.7561, -4.7087, -4.6258, -4.5769, -4.5529], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0709, -0.0686, -0.0719, -0.0738, -0.0691, -0.0708, -0.0718, -0.0699,
        -0.0682, -0.0655, -0.0714, -0.1096, -0.1747, -0.1657, -0.1123],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.7563, -4.7584, -4.7592, -4.7623, -4.7543, -4.7532, -4.7431, -4.7431,
        -4.7440, -4.7495, -4.7390, -4.7234, -4.6748, -4.5641], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.7563, -4.7584, -4.7592, -4.7623, -4.7543, -4.7532, -4.7431, -4.7431,
        -4.7440, -4.7495, -4.7390, -4.7234, -4.6748, -4.5641], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0660, 0.0754, 0.0803, 0.0759, 0.0837, 0.0750, 0.0758, 0.1432, 0.0815,
        0.0853, 0.0743, 0.0837], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([21, 26, 17, 21, 28, 22, 33, 34, 21, 23, 23, 31]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_74.gif
Eval_AverageReturn : -247.6999969482422
Eval_StdReturn : 219.6324462890625
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 213.5
Train_AverageReturn : -235.5
Train_StdReturn : 216.08343505859375
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 307200.0
TimeSinceStart : 8275.734385251999
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 68.05638122558594
Loss_Value : 0.060288500506430864
Loss_Entropy : 2.3851033449172974
Loss_Representation : -12.681036710739136
Loss_KL : 2.2418578267097473
Loss_Obs : -1.3648279309272766
Loss_Reward : -1.3001096844673157
Loss_Discount : 0.02549429191276431
Loss_RawKL : 2.2418578267097473
mean_target : -4.9328166246414185
max_target : -4.650063395500183
min_target : -5.145424962043762
std_target : 0.1592161376029253
Done logging...

Current epsilon: 0.05013108536750728 at iteration 307200


********** Iteration 75 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1199, -0.1193, -0.1165, -0.1154, -0.1133, -0.1154, -0.1153, -0.1191,
        -0.1198, -0.1202, -0.1163, -0.1134, -0.1170, -0.1167, -0.1167],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.1462, -5.1158, -5.0768, -5.0332, -4.9898, -4.9520, -4.9068, -4.8671,
        -4.8135, -4.7571, -4.6929, -4.6265, -4.5651, -4.5007], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.1462, -5.1158, -5.0768, -5.0332, -4.9898, -4.9520, -4.9068, -4.8671,
        -4.8135, -4.7571, -4.6929, -4.6265, -4.5651, -4.5007], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1171, -0.1172, -0.1196, -0.1184, -0.1179, -0.1168, -0.1149, -0.1162,
        -0.1339, -0.1195, -0.1169, -0.1147, -0.1162, -0.1175, -0.1165],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.0562, -5.0293, -4.9937, -4.9584, -4.9097, -4.8594, -4.8126, -4.7584,
        -4.7379, -4.6959, -4.6527, -4.6018, -4.5532, -4.4868], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.0562, -5.0293, -4.9937, -4.9584, -4.9097, -4.8594, -4.8126, -4.7584,
        -4.7379, -4.6959, -4.6527, -4.6018, -4.5532, -4.4868], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0704, -0.0692, -0.0690, -0.0711, -0.0702, -0.0706, -0.0703, -0.0701,
        -0.0689, -0.0771, -0.0907, -0.0935, -0.0923, -0.0804, -0.0722],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.7023, -4.6903, -4.6819, -4.6722, -4.6548, -4.6397, -4.6299, -4.6169,
        -4.5944, -4.5866, -4.5608, -4.5148, -4.4732, -4.4372], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.7023, -4.6903, -4.6819, -4.6722, -4.6548, -4.6397, -4.6299, -4.6169,
        -4.5944, -4.5866, -4.5608, -4.5148, -4.4732, -4.4372], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0814, -0.0765, -0.0754, -0.0722, -0.0712, -0.0843, -0.0829, -0.0883,
        -0.0805, -0.0790, -0.0759, -0.0706, -0.0743, -0.0733, -0.0786],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.8230, -4.8073, -4.7899, -4.7686, -4.7603, -4.7441, -4.7141, -4.6824,
        -4.6598, -4.6398, -4.6136, -4.5824, -4.5598, -4.5333], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.8230, -4.8073, -4.7899, -4.7686, -4.7603, -4.7441, -4.7141, -4.6824,
        -4.6598, -4.6398, -4.6136, -4.5824, -4.5598, -4.5333], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0647, 0.0733, 0.0780, 0.0732, 0.0826, 0.0721, 0.0752, 0.1658, 0.0782,
        0.0840, 0.0709, 0.0821], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([13, 25, 29, 22, 28, 25, 24, 45, 26, 18, 19, 26]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_75.gif
Eval_AverageReturn : -256.3999938964844
Eval_StdReturn : 205.41043090820312
Eval_MaxReturn : 76.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 220.1999969482422
Train_AverageReturn : -273.4444580078125
Train_StdReturn : 196.0026092529297
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 311296.0
TimeSinceStart : 8390.614286661148
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 66.42150497436523
Loss_Value : -0.004719870164990425
Loss_Entropy : 2.3414339423179626
Loss_Representation : -13.466033697128296
Loss_KL : 1.8524508476257324
Loss_Obs : -1.3770874738693237
Loss_Reward : -1.5755954682826996
Loss_Discount : 0.027985617518424988
Loss_RawKL : 1.85245081782341
mean_target : -4.814691781997681
max_target : -4.533734440803528
min_target : -5.039092063903809
std_target : 0.16054102405905724
Done logging...

Current epsilon: 0.0501166082702624 at iteration 311296


********** Iteration 76 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1145, -0.1119, -0.1141, -0.1155, -0.1143, -0.1127, -0.1176, -0.1154,
        -0.1168, -0.1173, -0.1207, -0.1138, -0.1138, -0.1131, -0.1138],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.1663, -5.1564, -5.1228, -5.0927, -5.0479, -4.9986, -4.9530, -4.8936,
        -4.8320, -4.7757, -4.7082, -4.6395, -4.5784, -4.5087], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.1663, -5.1564, -5.1228, -5.0927, -5.0479, -4.9986, -4.9530, -4.8936,
        -4.8320, -4.7757, -4.7082, -4.6395, -4.5784, -4.5087], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1173, -0.1155, -0.1173, -0.1169, -0.1241, -0.1205, -0.1355, -0.1301,
        -0.1370, -0.1126, -0.1125, -0.1138, -0.1157, -0.1148, -0.1128],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.0438, -5.0110, -4.9850, -4.9570, -4.9234, -4.8727, -4.8403, -4.7748,
        -4.7252, -4.6463, -4.6022, -4.5495, -4.4824, -4.4071], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.0438, -5.0110, -4.9850, -4.9570, -4.9234, -4.8727, -4.8403, -4.7748,
        -4.7252, -4.6463, -4.6022, -4.5495, -4.4824, -4.4071], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0769, -0.0802, -0.0751, -0.0777, -0.0724, -0.0704, -0.0756, -0.0740,
        -0.0742, -0.0764, -0.0796, -0.0791, -0.0848, -0.1000, -0.1374],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.8943, -4.8914, -4.8850, -4.8737, -4.8602, -4.8507, -4.8402, -4.8150,
        -4.7929, -4.7742, -4.7551, -4.7263, -4.7018, -4.6640], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.8943, -4.8914, -4.8850, -4.8737, -4.8602, -4.8507, -4.8402, -4.8150,
        -4.7929, -4.7742, -4.7551, -4.7263, -4.7018, -4.6640], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0820, -0.0794, -0.0774, -0.0777, -0.0758, -0.0758, -0.0765, -0.0770,
        -0.0761, -0.0782, -0.0800, -0.0808, -0.0820, -0.0815, -0.0876],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.9156, -4.8973, -4.8799, -4.8685, -4.8465, -4.8235, -4.7962, -4.7687,
        -4.7410, -4.7193, -4.6907, -4.6523, -4.6192, -4.5824], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.9156, -4.8973, -4.8799, -4.8685, -4.8465, -4.8235, -4.7962, -4.7687,
        -4.7410, -4.7193, -4.6907, -4.6523, -4.6192, -4.5824], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0715, 0.0725, 0.0827, 0.0742, 0.0793, 0.0800, 0.0785, 0.1515, 0.0763,
        0.0790, 0.0748, 0.0796], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([21, 18, 23, 25, 26, 31, 26, 45, 20, 21, 20, 24]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_76.gif
Eval_AverageReturn : -245.89999389648438
Eval_StdReturn : 214.910888671875
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 214.6999969482422
Train_AverageReturn : -272.6111145019531
Train_StdReturn : 190.48333740234375
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 315392.0
TimeSinceStart : 8504.391310930252
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 67.93876457214355
Loss_Value : 0.041585393249988556
Loss_Entropy : 2.3338071703910828
Loss_Representation : -13.503692626953125
Loss_KL : 1.7804778814315796
Loss_Obs : -1.3775170147418976
Loss_Reward : -1.538404792547226
Loss_Discount : 0.029404714703559875
Loss_RawKL : 1.7804778218269348
mean_target : -4.922846555709839
max_target : -4.6237040758132935
min_target : -5.161427140235901
std_target : 0.17008887976408005
Done logging...

Current epsilon: 0.05010373002686844 at iteration 315392


********** Iteration 77 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1242, -0.1256, -0.1227, -0.1245, -0.1251, -0.1245, -0.1257, -0.1314,
        -0.1281, -0.1282, -0.1327, -0.1292, -0.1353, -0.1235, -0.1234],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2552, -5.2233, -5.1794, -5.1275, -5.0675, -5.0067, -4.9504, -4.8863,
        -4.8285, -4.7624, -4.6991, -4.6146, -4.5582, -4.4648], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2552, -5.2233, -5.1794, -5.1275, -5.0675, -5.0067, -4.9504, -4.8863,
        -4.8285, -4.7624, -4.6991, -4.6146, -4.5582, -4.4648], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1414, -0.1390, -0.1261, -0.1223, -0.1212, -0.1230, -0.1198, -0.1229,
        -0.1237, -0.1239, -0.1234, -0.1249, -0.1230, -0.1257, -0.1267],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2528, -5.2251, -5.2041, -5.1681, -5.1434, -5.1071, -5.0579, -5.0073,
        -4.9541, -4.8994, -4.8324, -4.7676, -4.6930, -4.6193], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2528, -5.2251, -5.2041, -5.1681, -5.1434, -5.1071, -5.0579, -5.0073,
        -4.9541, -4.8994, -4.8324, -4.7676, -4.6930, -4.6193], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1118, -0.1206, -0.0975, -0.0844, -0.0832, -0.0816, -0.0813, -0.0827,
        -0.0813, -0.0816, -0.0807, -0.0815, -0.0816, -0.0825, -0.0828],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.9660, -4.9466, -4.9108, -4.8957, -4.8921, -4.8767, -4.8495, -4.8202,
        -4.7924, -4.7654, -4.7258, -4.6897, -4.6595, -4.6262], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.9660, -4.9466, -4.9108, -4.8957, -4.8921, -4.8767, -4.8495, -4.8202,
        -4.7924, -4.7654, -4.7258, -4.6897, -4.6595, -4.6262], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0863, -0.0867, -0.0940, -0.0925, -0.1161, -0.1308, -0.1560, -0.1574,
        -0.1453, -0.1116, -0.1007, -0.0908, -0.0863, -0.0944, -0.0863],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2547, -5.2536, -5.2486, -5.2284, -5.2151, -5.1781, -5.1172, -5.0262,
        -4.9415, -4.8558, -4.8116, -4.7703, -4.7397, -4.7010], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2547, -5.2536, -5.2486, -5.2284, -5.2151, -5.1781, -5.1172, -5.0262,
        -4.9415, -4.8558, -4.8116, -4.7703, -4.7397, -4.7010], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0722, 0.0726, 0.0820, 0.0731, 0.0784, 0.0801, 0.0800, 0.1558, 0.0751,
        0.0791, 0.0735, 0.0781], device='cuda:0')
Count of actions: (array([7]), array([6]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_77.gif
Eval_AverageReturn : -153.6999969482422
Eval_StdReturn : 237.41651916503906
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 157.1999969482422
Train_AverageReturn : -191.17391967773438
Train_StdReturn : 228.41734313964844
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 178.0869598388672
Train_EnvstepsSoFar : 319488.0
TimeSinceStart : 8604.763979196548
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 69.23864555358887
Loss_Value : 0.16005896963179111
Loss_Entropy : 2.321423292160034
Loss_Representation : -13.901602745056152
Loss_KL : 1.6899563670158386
Loss_Obs : -1.401208072900772
Loss_Reward : -1.6070619821548462
Loss_Discount : 0.027583476155996323
Loss_RawKL : 1.6899563074111938
mean_target : -5.015337705612183
max_target : -4.680221796035767
min_target : -5.272415399551392
std_target : 0.1897708848118782
Done logging...

Current epsilon: 0.05009227405954924 at iteration 319488


********** Iteration 78 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1275, -0.1302, -0.1280, -0.1306, -0.1294, -0.1282, -0.1288, -0.1316,
        -0.1274, -0.1326, -0.1258, -0.1283, -0.1485, -0.1873, -0.1623],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.4929, -5.4707, -5.4424, -5.4158, -5.3862, -5.3419, -5.3051, -5.2600,
        -5.2012, -5.1378, -5.0671, -4.9920, -4.9094, -4.8100], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.4929, -5.4707, -5.4424, -5.4158, -5.3862, -5.3419, -5.3051, -5.2600,
        -5.2012, -5.1378, -5.0671, -4.9920, -4.9094, -4.8100], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1649, -0.1981, -0.1991, -0.1439, -0.1279, -0.1256, -0.1272, -0.1302,
        -0.1289, -0.1284, -0.1256, -0.1276, -0.1286, -0.1277, -0.1278],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.5277, -5.4832, -5.3957, -5.3331, -5.2942, -5.2485, -5.2142, -5.1713,
        -5.1219, -5.0648, -4.9945, -4.9181, -4.8489, -4.7761], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.5277, -5.4832, -5.3957, -5.3331, -5.2942, -5.2485, -5.2142, -5.1713,
        -5.1219, -5.0648, -4.9945, -4.9181, -4.8489, -4.7761], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0948, -0.0972, -0.0947, -0.1060, -0.1329, -0.1192, -0.1175, -0.1007,
        -0.1173, -0.1269, -0.1124, -0.0984, -0.0957, -0.0920, -0.0882],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.3670, -5.3643, -5.3464, -5.3367, -5.3189, -5.2621, -5.2229, -5.1736,
        -5.1416, -5.0788, -5.0003, -4.9406, -4.8914, -4.8403], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.3670, -5.3643, -5.3464, -5.3367, -5.3189, -5.2621, -5.2229, -5.1736,
        -5.1416, -5.0788, -5.0003, -4.9406, -4.8914, -4.8403], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0891, -0.0898, -0.0896, -0.0884, -0.0917, -0.0896, -0.0964, -0.1119,
        -0.1102, -0.1452, -0.1666, -0.1565, -0.1609, -0.1413, -0.1308],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.3919, -5.3959, -5.3808, -5.3686, -5.3525, -5.3279, -5.3024, -5.2867,
        -5.2409, -5.1894, -5.1029, -4.9920, -4.8850, -4.7688], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.3919, -5.3959, -5.3808, -5.3686, -5.3525, -5.3279, -5.3024, -5.2867,
        -5.2409, -5.1894, -5.1029, -4.9920, -4.8850, -4.7688], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0672, 0.0707, 0.0762, 0.0693, 0.0773, 0.0734, 0.0775, 0.1927, 0.0721,
        0.0802, 0.0676, 0.0758], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([25, 26, 22, 24, 30, 23, 22, 50, 17, 23, 23, 15]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_78.gif
Eval_AverageReturn : -342.0
Eval_StdReturn : 145.742919921875
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 270.6000061035156
Train_AverageReturn : -273.4444580078125
Train_StdReturn : 191.3275909423828
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 323584.0
TimeSinceStart : 8726.892739772797
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 70.34463310241699
Loss_Value : 0.17505430057644844
Loss_Entropy : 2.2576051354408264
Loss_Representation : -14.10040283203125
Loss_KL : 1.5978126227855682
Loss_Obs : -1.414212703704834
Loss_Reward : -1.5815230309963226
Loss_Discount : 0.02543444698676467
Loss_RawKL : 1.5978125929832458
mean_target : -5.092383027076721
max_target : -4.728039979934692
min_target : -5.373735070228577
std_target : 0.20808977261185646
Done logging...

Current epsilon: 0.050082083291817674 at iteration 323584


********** Iteration 79 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1461, -0.1658, -0.1373, -0.1275, -0.1251, -0.1263, -0.1258, -0.1282,
        -0.1271, -0.1280, -0.1275, -0.1251, -0.1297, -0.1417, -0.1783],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.7705, -5.7231, -5.6548, -5.6052, -5.5729, -5.5262, -5.4843, -5.4329,
        -5.3749, -5.3185, -5.2468, -5.1695, -5.0890, -4.9993], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.7705, -5.7231, -5.6548, -5.6052, -5.5729, -5.5262, -5.4843, -5.4329,
        -5.3749, -5.3185, -5.2468, -5.1695, -5.0890, -4.9993], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1185, -0.1223, -0.1138, -0.1196, -0.1225, -0.1238, -0.1233, -0.1241,
        -0.1249, -0.1250, -0.1235, -0.1222, -0.1236, -0.1190, -0.1175],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.9920, -4.9617, -4.9107, -4.8802, -4.8273, -4.7789, -4.7136, -4.6402,
        -4.5678, -4.4979, -4.4146, -4.3329, -4.2462, -4.1583], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.9920, -4.9617, -4.9107, -4.8802, -4.8273, -4.7789, -4.7136, -4.6402,
        -4.5678, -4.4979, -4.4146, -4.3329, -4.2462, -4.1583], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0848, -0.0870, -0.0842, -0.0862, -0.0827, -0.0842, -0.0845, -0.0835,
        -0.0816, -0.0836, -0.0855, -0.0843, -0.0832, -0.0908, -0.1176],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.1501, -5.1463, -5.1296, -5.1148, -5.1058, -5.0926, -5.0832, -5.0667,
        -5.0466, -5.0374, -5.0103, -4.9777, -4.9485, -4.9125], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.1501, -5.1463, -5.1296, -5.1148, -5.1058, -5.0926, -5.0832, -5.0667,
        -5.0466, -5.0374, -5.0103, -4.9777, -4.9485, -4.9125], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0883, -0.0860, -0.0898, -0.0866, -0.0832, -0.0857, -0.0854, -0.0885,
        -0.0996, -0.1420, -0.1469, -0.0927, -0.0897, -0.0844, -0.0853],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-4.8268, -4.7959, -4.7684, -4.7510, -4.7246, -4.7162, -4.6953, -4.6756,
        -4.6476, -4.6156, -4.5166, -4.4423, -4.4050, -4.3631], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-4.8268, -4.7959, -4.7684, -4.7510, -4.7246, -4.7162, -4.6953, -4.6756,
        -4.6476, -4.6156, -4.5166, -4.4423, -4.4050, -4.3631], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0670, 0.0700, 0.0749, 0.0680, 0.0761, 0.0725, 0.0781, 0.2033, 0.0705,
        0.0795, 0.0657, 0.0744], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([19, 24, 21, 22, 25, 25, 23, 52, 22, 21, 25, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_79.gif
Eval_AverageReturn : -154.1999969482422
Eval_StdReturn : 238.04905700683594
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 157.1999969482422
Train_AverageReturn : -204.68182373046875
Train_StdReturn : 223.14739990234375
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 186.18182373046875
Train_EnvstepsSoFar : 327680.0
TimeSinceStart : 8832.845687389374
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 70.68848609924316
Loss_Value : 0.06390730198472738
Loss_Entropy : 2.2225354313850403
Loss_Representation : -14.507137775421143
Loss_KL : 1.3931696116924286
Loss_Obs : -1.423165112733841
Loss_Reward : -1.7019778788089752
Loss_Discount : 0.03332128282636404
Loss_RawKL : 1.3931695520877838
mean_target : -5.115896224975586
max_target : -4.796555876731873
min_target : -5.368710160255432
std_target : 0.1827514711767435
Done logging...

Current epsilon: 0.050073017994748885 at iteration 327680


********** Iteration 80 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1492, -0.1646, -0.1509, -0.1229, -0.1233, -0.1210, -0.1192, -0.1209,
        -0.1202, -0.1213, -0.1214, -0.1221, -0.1209, -0.1203, -0.1203],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.4550, -5.3959, -5.3298, -5.2894, -5.2468, -5.1946, -5.1520, -5.0946,
        -5.0469, -5.0132, -4.9673, -4.9098, -4.8574, -4.7930], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.4550, -5.3959, -5.3298, -5.2894, -5.2468, -5.1946, -5.1520, -5.0946,
        -5.0469, -5.0132, -4.9673, -4.9098, -4.8574, -4.7930], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1171, -0.1170, -0.1168, -0.1179, -0.1160, -0.1161, -0.1174, -0.1165,
        -0.1157, -0.1155, -0.1160, -0.1166, -0.1155, -0.1164, -0.1161],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.5302, -5.5052, -5.4778, -5.4518, -5.4152, -5.3787, -5.3412, -5.3018,
        -5.2669, -5.2192, -5.1619, -5.1120, -5.0494, -4.9801], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.5302, -5.5052, -5.4778, -5.4518, -5.4152, -5.3787, -5.3412, -5.3018,
        -5.2669, -5.2192, -5.1619, -5.1120, -5.0494, -4.9801], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0794, -0.0775, -0.0809, -0.0794, -0.0800, -0.0786, -0.0816, -0.0811,
        -0.0804, -0.0944, -0.1089, -0.1369, -0.1344, -0.1322, -0.1173],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2430, -5.2399, -5.2362, -5.2312, -5.2192, -5.2006, -5.1819, -5.1722,
        -5.1510, -5.1310, -5.0914, -5.0290, -4.9455, -4.8602], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2430, -5.2399, -5.2362, -5.2312, -5.2192, -5.2006, -5.1819, -5.1722,
        -5.1510, -5.1310, -5.0914, -5.0290, -4.9455, -4.8602], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0765, -0.0777, -0.0760, -0.0767, -0.0780, -0.0761, -0.0764, -0.0758,
        -0.0785, -0.0898, -0.1105, -0.1346, -0.1559, -0.1018, -0.0889],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2735, -5.2827, -5.2789, -5.2721, -5.2666, -5.2699, -5.2692, -5.2641,
        -5.2555, -5.2438, -5.2194, -5.1619, -5.0927, -4.9841], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2735, -5.2827, -5.2789, -5.2721, -5.2666, -5.2699, -5.2692, -5.2641,
        -5.2555, -5.2438, -5.2194, -5.1619, -5.0927, -4.9841], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0692, 0.0714, 0.0761, 0.0691, 0.0770, 0.0742, 0.0816, 0.1886, 0.0711,
        0.0803, 0.0662, 0.0751], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([18, 23, 18, 27, 19, 20, 23, 59, 24, 27, 21, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_80.gif
Eval_AverageReturn : -294.0
Eval_StdReturn : 194.53533935546875
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -274.5555419921875
Train_StdReturn : 195.3555908203125
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 331776.0
TimeSinceStart : 8951.029401540756
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 71.98641586303711
Loss_Value : -0.04906516429036856
Loss_Entropy : 2.2434167861938477
Loss_Representation : -14.560757637023926
Loss_KL : 1.2334674298763275
Loss_Obs : -1.3988904058933258
Loss_Reward : -1.8314268589019775
Loss_Discount : 0.026106125209480524
Loss_RawKL : 1.2334674298763275
mean_target : -5.209257364273071
max_target : -4.921204447746277
min_target : -5.437209725379944
std_target : 0.164950555190444
Done logging...

Current epsilon: 0.05006495387111169 at iteration 331776


********** Iteration 81 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1147, -0.1165, -0.1175, -0.1165, -0.1140, -0.1160, -0.1115, -0.1148,
        -0.1250, -0.1368, -0.1308, -0.1152, -0.1147, -0.1136, -0.1145],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.6825, -5.6592, -5.6420, -5.6048, -5.5662, -5.5310, -5.4969, -5.4517,
        -5.4013, -5.3382, -5.2525, -5.1871, -5.1411, -5.0791], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.6825, -5.6592, -5.6420, -5.6048, -5.5662, -5.5310, -5.4969, -5.4517,
        -5.4013, -5.3382, -5.2525, -5.1871, -5.1411, -5.0791], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1105, -0.1110, -0.1117, -0.1110, -0.1118, -0.1112, -0.1104, -0.1106,
        -0.1103, -0.1099, -0.1115, -0.1113, -0.1213, -0.1435, -0.1659],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.5480, -5.5318, -5.5109, -5.4823, -5.4523, -5.4221, -5.3841, -5.3368,
        -5.2902, -5.2474, -5.1875, -5.1255, -5.0551, -4.9736], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.5480, -5.5318, -5.5109, -5.4823, -5.4523, -5.4221, -5.3841, -5.3368,
        -5.2902, -5.2474, -5.1875, -5.1255, -5.0551, -4.9736], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0743, -0.0746, -0.0734, -0.0732, -0.0726, -0.0732, -0.0711, -0.0745,
        -0.0743, -0.1033, -0.1504, -0.2064, -0.1037, -0.0822, -0.0857],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.3032, -5.3071, -5.2988, -5.3020, -5.2973, -5.3037, -5.2987, -5.2939,
        -5.2740, -5.2555, -5.2028, -5.0937, -4.9477, -4.8998], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.3032, -5.3071, -5.2988, -5.3020, -5.2973, -5.3037, -5.2987, -5.2939,
        -5.2740, -5.2555, -5.2028, -5.0937, -4.9477, -4.8998], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0807, -0.0760, -0.0742, -0.0743, -0.0737, -0.0736, -0.0742, -0.0761,
        -0.0769, -0.0930, -0.1357, -0.1708, -0.1508, -0.0922, -0.0784],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2433, -5.2315, -5.2547, -5.2586, -5.2481, -5.2331, -5.2191, -5.2038,
        -5.1704, -5.1402, -5.1007, -5.0166, -4.8875, -4.7829], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2433, -5.2315, -5.2547, -5.2586, -5.2481, -5.2331, -5.2191, -5.2038,
        -5.1704, -5.1402, -5.1007, -5.0166, -4.8875, -4.7829], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0680, 0.0724, 0.0746, 0.0693, 0.0791, 0.0715, 0.0823, 0.1877, 0.0711,
        0.0835, 0.0643, 0.0764], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([24, 22, 28, 11, 24, 21, 22, 54, 19, 28, 20, 27]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_81.gif
Eval_AverageReturn : -200.8000030517578
Eval_StdReturn : 233.05526733398438
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 185.6999969482422
Train_AverageReturn : -274.8333435058594
Train_StdReturn : 196.6997833251953
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 335872.0
TimeSinceStart : 9061.529520750046
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 73.49885559082031
Loss_Value : -0.1099632065743208
Loss_Entropy : 2.257471740245819
Loss_Representation : -14.550233125686646
Loss_KL : 1.2287406921386719
Loss_Obs : -1.39523184299469
Loss_Reward : -1.85032120347023
Loss_Discount : 0.0236658095382154
Loss_RawKL : 1.2287406921386719
mean_target : -5.317722201347351
max_target : -5.0331116914749146
min_target : -5.542634010314941
std_target : 0.16368654556572437
Done logging...

Current epsilon: 0.05005778035108885 at iteration 335872


********** Iteration 82 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1333, -0.1479, -0.1132, -0.1119, -0.1116, -0.1118, -0.1127, -0.1109,
        -0.1110, -0.1109, -0.1111, -0.1111, -0.1143, -0.1229, -0.1726],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.6981, -5.6564, -5.6240, -5.6226, -5.5949, -5.5670, -5.5335, -5.5092,
        -5.4751, -5.4414, -5.4075, -5.3724, -5.3154, -5.2492], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.6981, -5.6564, -5.6240, -5.6226, -5.5949, -5.5670, -5.5335, -5.5092,
        -5.4751, -5.4414, -5.4075, -5.3724, -5.3154, -5.2492], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1249, -0.1162, -0.1129, -0.1098, -0.1076, -0.1098, -0.1106, -0.1091,
        -0.1098, -0.1090, -0.1087, -0.1101, -0.1242, -0.1570, -0.1699],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.6318, -5.6214, -5.6108, -5.5920, -5.5834, -5.5640, -5.5337, -5.4998,
        -5.4723, -5.4269, -5.3695, -5.3208, -5.2634, -5.1903], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.6318, -5.6214, -5.6108, -5.5920, -5.5834, -5.5640, -5.5337, -5.4998,
        -5.4723, -5.4269, -5.3695, -5.3208, -5.2634, -5.1903], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0757, -0.0774, -0.0751, -0.0738, -0.0738, -0.0726, -0.0779, -0.0754,
        -0.1019, -0.0920, -0.0834, -0.0856, -0.0778, -0.0752, -0.0758],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.4361, -5.4371, -5.4272, -5.4166, -5.4080, -5.3876, -5.3766, -5.3496,
        -5.3239, -5.2611, -5.2122, -5.1784, -5.1368, -5.1093], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.4361, -5.4371, -5.4272, -5.4166, -5.4080, -5.3876, -5.3766, -5.3496,
        -5.3239, -5.2611, -5.2122, -5.1784, -5.1368, -5.1093], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0804, -0.0778, -0.0794, -0.0788, -0.0757, -0.0774, -0.0799, -0.0872,
        -0.1414, -0.1776, -0.1140, -0.0799, -0.0767, -0.0781, -0.0782],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.5913, -5.5876, -5.5876, -5.5748, -5.5675, -5.5646, -5.5509, -5.5333,
        -5.4999, -5.4049, -5.2862, -5.2430, -5.2283, -5.1944], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.5913, -5.5876, -5.5876, -5.5748, -5.5675, -5.5646, -5.5509, -5.5333,
        -5.4999, -5.4049, -5.2862, -5.2430, -5.2283, -5.1944], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0604, 0.0691, 0.0671, 0.0648, 0.0782, 0.0615, 0.0754, 0.2385, 0.0672,
        0.0857, 0.0570, 0.0750], device='cuda:0')
Count of actions: (array([1, 4, 6, 7, 8, 9]), array([ 3,  3,  2, 38,  1,  1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_82.gif
Eval_AverageReturn : -206.0
Eval_StdReturn : 226.92686462402344
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 189.39999389648438
Train_AverageReturn : -220.1904754638672
Train_StdReturn : 220.52569580078125
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 339968.0
TimeSinceStart : 9167.262414932251
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 74.31964302062988
Loss_Value : -0.13412050274200737
Loss_Entropy : 2.172592282295227
Loss_Representation : -14.598778486251831
Loss_KL : 1.2578341364860535
Loss_Obs : -1.402355134487152
Loss_Reward : -1.8592486679553986
Loss_Discount : 0.0261873216368258
Loss_RawKL : 1.257834106683731
mean_target : -5.373794317245483
max_target : -5.101486563682556
min_target : -5.5855056047439575
std_target : 0.1551636178046465
Done logging...

Current epsilon: 0.05005139907621841 at iteration 339968


********** Iteration 83 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1155, -0.1146, -0.1162, -0.1152, -0.1187, -0.1143, -0.1144, -0.1138,
        -0.1136, -0.1134, -0.1155, -0.1129, -0.1137, -0.1147, -0.1175],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.7773, -5.7537, -5.7289, -5.6913, -5.6575, -5.6302, -5.5962, -5.5674,
        -5.5376, -5.5062, -5.4544, -5.4013, -5.3429, -5.2854], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.7773, -5.7537, -5.7289, -5.6913, -5.6575, -5.6302, -5.5962, -5.5674,
        -5.5376, -5.5062, -5.4544, -5.4013, -5.3429, -5.2854], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1261, -0.1343, -0.1133, -0.1160, -0.1113, -0.1120, -0.1129, -0.1120,
        -0.1162, -0.1207, -0.1445, -0.1935, -0.1459, -0.1102, -0.1133],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.9657, -5.9443, -5.9001, -5.8927, -5.8700, -5.8362, -5.7965, -5.7513,
        -5.7049, -5.6535, -5.5900, -5.5029, -5.3592, -5.2886], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.9657, -5.9443, -5.9001, -5.8927, -5.8700, -5.8362, -5.7965, -5.7513,
        -5.7049, -5.6535, -5.5900, -5.5029, -5.3592, -5.2886], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0832, -0.0787, -0.0782, -0.0844, -0.0875, -0.1084, -0.1484, -0.1481,
        -0.0789, -0.0778, -0.0773, -0.0775, -0.0771, -0.0786, -0.0768],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.2010, -5.1949, -5.1890, -5.1907, -5.1815, -5.1698, -5.1372, -5.0750,
        -5.0124, -4.9868, -4.9582, -4.9259, -4.8909, -4.8430], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.2010, -5.1949, -5.1890, -5.1907, -5.1815, -5.1698, -5.1372, -5.0750,
        -5.0124, -4.9868, -4.9582, -4.9259, -4.8909, -4.8430], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1610, -0.1580, -0.1276, -0.0934, -0.0812, -0.0807, -0.0832, -0.0809,
        -0.0813, -0.0805, -0.0807, -0.0806, -0.0816, -0.0810, -0.0810],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.5235, -5.4358, -5.3650, -5.3057, -5.2845, -5.2668, -5.2474, -5.2252,
        -5.1968, -5.1676, -5.1495, -5.1164, -5.0866, -5.0407], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.5235, -5.4358, -5.3650, -5.3057, -5.2845, -5.2668, -5.2474, -5.2252,
        -5.1968, -5.1676, -5.1495, -5.1164, -5.0866, -5.0407], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0594, 0.0697, 0.0657, 0.0648, 0.0801, 0.0594, 0.0756, 0.2376, 0.0671,
        0.0889, 0.0553, 0.0764], device='cuda:0')
Count of actions: (array([7]), array([6]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_83.gif
Eval_AverageReturn : -248.89999389648438
Eval_StdReturn : 221.0743865966797
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 213.1999969482422
Train_AverageReturn : -277.3333435058594
Train_StdReturn : 202.69107055664062
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 344064.0
TimeSinceStart : 9275.338544607162
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 75.84127998352051
Loss_Value : -0.03496676683425903
Loss_Entropy : 2.191859483718872
Loss_Representation : -14.751875638961792
Loss_KL : 1.273018717765808
Loss_Obs : -1.407790184020996
Loss_Reward : -1.9706840217113495
Loss_Discount : 0.02369146840646863
Loss_RawKL : 1.273018717765808
mean_target : -5.483099937438965
max_target : -5.194314360618591
min_target : -5.707063674926758
std_target : 0.16419238410890102
Done logging...

Current epsilon: 0.05004572255076892 at iteration 344064


********** Iteration 84 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1537, -0.1674, -0.1235, -0.1175, -0.1167, -0.1169, -0.1169, -0.1164,
        -0.1176, -0.1169, -0.1160, -0.1176, -0.1171, -0.1192, -0.1210],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.8200, -5.7568, -5.7098, -5.6915, -5.6651, -5.6210, -5.5819, -5.5433,
        -5.5009, -5.4566, -5.4024, -5.3410, -5.2892, -5.2154], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.8200, -5.7568, -5.7098, -5.6915, -5.6651, -5.6210, -5.5819, -5.5433,
        -5.5009, -5.4566, -5.4024, -5.3410, -5.2892, -5.2154], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1161, -0.1158, -0.1177, -0.1153, -0.1156, -0.1197, -0.1355, -0.1355,
        -0.1166, -0.1143, -0.1160, -0.1160, -0.1155, -0.1154, -0.1166],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.8596, -5.8281, -5.7887, -5.7418, -5.6849, -5.6353, -5.5812, -5.5283,
        -5.4601, -5.4093, -5.3602, -5.3105, -5.2509, -5.1889], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.8596, -5.8281, -5.7887, -5.7418, -5.6849, -5.6353, -5.5812, -5.5283,
        -5.4601, -5.4093, -5.3602, -5.3105, -5.2509, -5.1889], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1126, -0.1466, -0.1262, -0.0875, -0.0829, -0.0818, -0.0823, -0.0822,
        -0.0813, -0.0815, -0.0816, -0.0813, -0.0821, -0.0818, -0.0828],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.5466, -5.5066, -5.4296, -5.3703, -5.3512, -5.3355, -5.3245, -5.3192,
        -5.3013, -5.2812, -5.2553, -5.2255, -5.1872, -5.1477], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.5466, -5.5066, -5.4296, -5.3703, -5.3512, -5.3355, -5.3245, -5.3192,
        -5.3013, -5.2812, -5.2553, -5.2255, -5.1872, -5.1477], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0857, -0.0846, -0.0842, -0.0837, -0.0834, -0.0831, -0.0835, -0.0839,
        -0.0844, -0.0836, -0.0861, -0.1019, -0.1130, -0.1754, -0.1015],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.5753, -5.6128, -5.6106, -5.5914, -5.5729, -5.5535, -5.5329, -5.5195,
        -5.5002, -5.4725, -5.4553, -5.4309, -5.3819, -5.3159], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.5753, -5.6128, -5.6106, -5.5914, -5.5729, -5.5535, -5.5329, -5.5195,
        -5.5002, -5.4725, -5.4553, -5.4309, -5.3819, -5.3159], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0591, 0.0692, 0.0641, 0.0639, 0.0798, 0.0586, 0.0763, 0.2444, 0.0660,
        0.0891, 0.0537, 0.0756], device='cuda:0')
Count of actions: (array([ 7, 11]), array([5, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_84.gif
Eval_AverageReturn : -157.10000610351562
Eval_StdReturn : 236.53982543945312
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 159.10000610351562
Train_AverageReturn : -275.9444580078125
Train_StdReturn : 194.45350646972656
Train_MaxReturn : 95.0
Train_MinReturn : -405.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 348160.0
TimeSinceStart : 9375.678930521011
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 77.34766387939453
Loss_Value : 0.0707184374332428
Loss_Entropy : 2.172097384929657
Loss_Representation : -14.75002670288086
Loss_KL : 1.359047383069992
Loss_Obs : -1.4182256162166595
Loss_Reward : -1.9500759840011597
Loss_Discount : 0.02325832098722458
Loss_RawKL : 1.3590473532676697
mean_target : -5.590115427970886
max_target : -5.282110333442688
min_target : -5.829638361930847
std_target : 0.17545601353049278
Done logging...

Current epsilon: 0.05004067294205703 at iteration 348160


********** Iteration 85 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1232, -0.1207, -0.1192, -0.1164, -0.1190, -0.1160, -0.1185, -0.1190,
        -0.1194, -0.1183, -0.1176, -0.1175, -0.1180, -0.1166, -0.1184],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.9355, -5.9016, -5.8566, -5.8317, -5.7977, -5.7636, -5.7353, -5.6969,
        -5.6572, -5.6081, -5.5449, -5.4751, -5.3949, -5.3243], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.9355, -5.9016, -5.8566, -5.8317, -5.7977, -5.7636, -5.7353, -5.6969,
        -5.6572, -5.6081, -5.5449, -5.4751, -5.3949, -5.3243], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1109, -0.1161, -0.1158, -0.1166, -0.1134, -0.1130, -0.1134, -0.1239,
        -0.1518, -0.1312, -0.1149, -0.1161, -0.1156, -0.1175, -0.1173],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.6619, -5.6344, -5.5939, -5.5601, -5.5245, -5.4843, -5.4453, -5.4031,
        -5.3558, -5.2776, -5.2277, -5.1841, -5.1227, -5.0492], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.6619, -5.6344, -5.5939, -5.5601, -5.5245, -5.4843, -5.4453, -5.4031,
        -5.3558, -5.2776, -5.2277, -5.1841, -5.1227, -5.0492], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0836, -0.0838, -0.0826, -0.0850, -0.0827, -0.0830, -0.0856, -0.0913,
        -0.1300, -0.1746, -0.1182, -0.0875, -0.0874, -0.0838, -0.0839],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.6359, -5.6323, -5.6277, -5.6304, -5.6085, -5.6001, -5.5886, -5.5619,
        -5.5237, -5.4460, -5.3438, -5.2826, -5.2543, -5.2044], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.6359, -5.6323, -5.6277, -5.6304, -5.6085, -5.6001, -5.5886, -5.5619,
        -5.5237, -5.4460, -5.3438, -5.2826, -5.2543, -5.2044], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1235, -0.1488, -0.1070, -0.0864, -0.0870, -0.0852, -0.0858, -0.0858,
        -0.0853, -0.0853, -0.0844, -0.0853, -0.0844, -0.0854, -0.0855],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.7760, -5.7197, -5.6490, -5.6300, -5.6187, -5.5985, -5.5746, -5.5489,
        -5.5232, -5.4975, -5.4708, -5.4395, -5.4025, -5.3574], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.7760, -5.7197, -5.6490, -5.6300, -5.6187, -5.5985, -5.5746, -5.5489,
        -5.5232, -5.4975, -5.4708, -5.4395, -5.4025, -5.3574], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0644, 0.0721, 0.0673, 0.0666, 0.0820, 0.0639, 0.0838, 0.2094, 0.0681,
        0.0893, 0.0562, 0.0771], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([14, 18, 24, 23, 21, 26, 18, 66, 23, 28, 20, 19]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_85.gif
Eval_AverageReturn : -207.0
Eval_StdReturn : 228.74876403808594
Eval_MaxReturn : 95.0
Eval_MinReturn : -405.0
Eval_AverageEpLen : 189.39999389648438
Train_AverageReturn : -275.1111145019531
Train_StdReturn : 198.11048889160156
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 352256.0
TimeSinceStart : 9485.958031892776
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 78.81968688964844
Loss_Value : 0.022111039608716965
Loss_Entropy : 2.2250189185142517
Loss_Representation : -14.879575490951538
Loss_KL : 1.2245103418827057
Loss_Obs : -1.4137687981128693
Loss_Reward : -1.992781013250351
Loss_Discount : 0.026383076794445515
Loss_RawKL : 1.2245103418827057
mean_target : -5.6968196630477905
max_target : -5.376814961433411
min_target : -5.946986198425293
std_target : 0.1835341416299343
Done logging...

Current epsilon: 0.050036181013258324 at iteration 352256


********** Iteration 86 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1178, -0.1179, -0.1157, -0.1176, -0.1180, -0.1183, -0.1180, -0.1180,
        -0.1186, -0.1179, -0.1175, -0.1185, -0.1179, -0.1182, -0.1178],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.9917, -5.9659, -5.9238, -5.8863, -5.8501, -5.8091, -5.7625, -5.7232,
        -5.6737, -5.6138, -5.5499, -5.4838, -5.4037, -5.3213], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.9917, -5.9659, -5.9238, -5.8863, -5.8501, -5.8091, -5.7625, -5.7232,
        -5.6737, -5.6138, -5.5499, -5.4838, -5.4037, -5.3213], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1224, -0.1320, -0.0925, -0.1087, -0.1051, -0.1135, -0.1173, -0.1167,
        -0.1163, -0.1177, -0.1184, -0.1178, -0.1165, -0.1145, -0.1123],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.3221, -5.2931, -5.2486, -5.2443, -5.2347, -5.2642, -5.2237, -5.1806,
        -5.1228, -5.0505, -4.9906, -4.9141, -4.8344, -4.7454], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.3221, -5.2931, -5.2486, -5.2443, -5.2347, -5.2642, -5.2237, -5.1806,
        -5.1228, -5.0505, -4.9906, -4.9141, -4.8344, -4.7454], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0838, -0.0840, -0.0834, -0.0836, -0.0837, -0.0839, -0.0829, -0.0838,
        -0.0907, -0.0909, -0.1178, -0.1055, -0.1358, -0.1067, -0.0937],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.7754, -5.7683, -5.7529, -5.7461, -5.7354, -5.7254, -5.7141, -5.6924,
        -5.6651, -5.6341, -5.5946, -5.5267, -5.4843, -5.3988], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.7754, -5.7683, -5.7529, -5.7461, -5.7354, -5.7254, -5.7141, -5.6924,
        -5.6651, -5.6341, -5.5946, -5.5267, -5.4843, -5.3988], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1004, -0.1212, -0.0877, -0.0891, -0.0864, -0.0846, -0.0856, -0.0846,
        -0.0838, -0.0843, -0.0845, -0.0842, -0.0846, -0.0840, -0.0843],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.7071, -5.6848, -5.6636, -5.6664, -5.6429, -5.6407, -5.6214, -5.6052,
        -5.5865, -5.5573, -5.5348, -5.5100, -5.4791, -5.4481], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.7071, -5.6848, -5.6636, -5.6664, -5.6429, -5.6407, -5.6214, -5.6052,
        -5.5865, -5.5573, -5.5348, -5.5100, -5.4791, -5.4481], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0649, 0.0702, 0.0656, 0.0650, 0.0799, 0.0640, 0.0851, 0.2244, 0.0659,
        0.0861, 0.0546, 0.0745], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([26, 22, 23, 23, 30, 19, 27, 56, 20, 15, 18, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_86.gif
Eval_AverageReturn : -299.29998779296875
Eval_StdReturn : 187.88137817382812
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 244.0
Train_AverageReturn : -236.5
Train_StdReturn : 220.4020233154297
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 356352.0
TimeSinceStart : 9604.873663187027
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 79.85634994506836
Loss_Value : -0.01018458977341652
Loss_Entropy : 2.167226493358612
Loss_Representation : -14.836319923400879
Loss_KL : 1.342953622341156
Loss_Obs : -1.4198794960975647
Loss_Reward : -2.0068052113056183
Loss_Discount : 0.02632660511881113
Loss_RawKL : 1.342953622341156
mean_target : -5.769143223762512
max_target : -5.4521037340164185
min_target : -6.015276074409485
std_target : 0.18098167330026627
Done logging...

Current epsilon: 0.05003218517407872 at iteration 356352


********** Iteration 87 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1147, -0.1137, -0.1159, -0.1111, -0.1105, -0.1123, -0.1267, -0.1211,
        -0.1383, -0.1138, -0.1168, -0.1160, -0.1153, -0.1188, -0.1175],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.6263, -5.5859, -5.5470, -5.5106, -5.4735, -5.4400, -5.4031, -5.3627,
        -5.3303, -5.3600, -5.3293, -5.3025, -5.2428, -5.1722], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.6263, -5.5859, -5.5470, -5.5106, -5.4735, -5.4400, -5.4031, -5.3627,
        -5.3303, -5.3600, -5.3293, -5.3025, -5.2428, -5.1722], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1145, -0.1141, -0.1185, -0.1160, -0.1174, -0.1147, -0.1154, -0.1142,
        -0.1147, -0.1140, -0.1158, -0.1149, -0.1142, -0.1157, -0.1169],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.9455, -5.9224, -5.8921, -5.8532, -5.8173, -5.7723, -5.7418, -5.7046,
        -5.6658, -5.6175, -5.5656, -5.5068, -5.4537, -5.3802], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.9455, -5.9224, -5.8921, -5.8532, -5.8173, -5.7723, -5.7418, -5.7046,
        -5.6658, -5.6175, -5.5656, -5.5068, -5.4537, -5.3802], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0833, -0.0825, -0.0821, -0.0835, -0.0824, -0.0831, -0.0834, -0.0862,
        -0.1127, -0.1913, -0.1255, -0.0884, -0.0853, -0.0823, -0.0825],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.9116, -5.9000, -5.8867, -5.8805, -5.8657, -5.8491, -5.8316, -5.8027,
        -5.7702, -5.7056, -5.5614, -5.5123, -5.4807, -5.4443], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.9116, -5.9000, -5.8867, -5.8805, -5.8657, -5.8491, -5.8316, -5.8027,
        -5.7702, -5.7056, -5.5614, -5.5123, -5.4807, -5.4443], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0870, -0.0857, -0.0853, -0.0876, -0.0934, -0.1470, -0.1688, -0.1068,
        -0.0868, -0.0859, -0.0849, -0.0848, -0.0838, -0.0837, -0.0834],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.1290, -6.1332, -6.1183, -6.1173, -6.1024, -6.0783, -5.9968, -5.8861,
        -5.8547, -5.8271, -5.7922, -5.7564, -5.7206, -5.6777], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.1290, -6.1332, -6.1183, -6.1173, -6.1024, -6.0783, -5.9968, -5.8861,
        -5.8547, -5.8271, -5.7922, -5.7564, -5.7206, -5.6777], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0660, 0.0712, 0.0651, 0.0655, 0.0818, 0.0640, 0.0882, 0.2138, 0.0663,
        0.0881, 0.0541, 0.0760], device='cuda:0')
Count of actions: (array([7, 9]), array([5, 1]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_87.gif
Eval_AverageReturn : -345.3999938964844
Eval_StdReturn : 140.5042266845703
Eval_MaxReturn : 76.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 272.0
Train_AverageReturn : -236.25
Train_StdReturn : 214.81011962890625
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 360448.0
TimeSinceStart : 9721.415227890015
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 80.45500946044922
Loss_Value : -0.05576124042272568
Loss_Entropy : 2.1927032470703125
Loss_Representation : -15.259939670562744
Loss_KL : 1.2691685259342194
Loss_Obs : -1.445669949054718
Loss_Reward : -2.0974559485912323
Loss_Discount : 0.02504702750593424
Loss_RawKL : 1.269168496131897
mean_target : -5.812638163566589
max_target : -5.512453198432922
min_target : -6.04496443271637
std_target : 0.1712658889591694
Done logging...

Current epsilon: 0.05002863063626995 at iteration 360448


********** Iteration 88 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1376, -0.1466, -0.1412, -0.1177, -0.1166, -0.1166, -0.1162, -0.1161,
        -0.1158, -0.1176, -0.1169, -0.1161, -0.1168, -0.1165, -0.1157],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3220, -6.2714, -6.2140, -6.1592, -6.1169, -6.0788, -6.0427, -5.9947,
        -5.9379, -5.8820, -5.8177, -5.7487, -5.6795, -5.6090], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3220, -6.2714, -6.2140, -6.1592, -6.1169, -6.0788, -6.0427, -5.9947,
        -5.9379, -5.8820, -5.8177, -5.7487, -5.6795, -5.6090], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1152, -0.1144, -0.1155, -0.1146, -0.1150, -0.1157, -0.1153, -0.1149,
        -0.1159, -0.1150, -0.1149, -0.1140, -0.1158, -0.1328, -0.1914],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.1351, -6.1141, -6.1047, -6.0985, -6.0710, -6.0422, -6.0084, -5.9750,
        -5.9325, -5.8909, -5.8433, -5.7934, -5.7268, -5.6577], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.1351, -6.1141, -6.1047, -6.0985, -6.0710, -6.0422, -6.0084, -5.9750,
        -5.9325, -5.8909, -5.8433, -5.7934, -5.7268, -5.6577], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0825, -0.0823, -0.0840, -0.0829, -0.0834, -0.0826, -0.0845, -0.0872,
        -0.0960, -0.0943, -0.0891, -0.0847, -0.0858, -0.0826, -0.0828],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.6984, -5.6834, -5.6887, -5.6599, -5.6362, -5.6126, -5.5867, -5.5664,
        -5.5358, -5.4877, -5.4594, -5.4479, -5.4428, -5.4264], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.6984, -5.6834, -5.6887, -5.6599, -5.6362, -5.6126, -5.5867, -5.5664,
        -5.5358, -5.4877, -5.4594, -5.4479, -5.4428, -5.4264], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0845, -0.0853, -0.0846, -0.0843, -0.0838, -0.0836, -0.0840, -0.0842,
        -0.0836, -0.0840, -0.0839, -0.0848, -0.0848, -0.0918, -0.1192],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.8754, -5.8627, -5.8547, -5.8390, -5.8224, -5.8121, -5.8003, -5.7796,
        -5.7584, -5.7329, -5.6989, -5.6640, -5.6158, -5.5711], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.8754, -5.8627, -5.8547, -5.8390, -5.8224, -5.8121, -5.8003, -5.7796,
        -5.7584, -5.7329, -5.6989, -5.6640, -5.6158, -5.5711], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0690, 0.0721, 0.0655, 0.0662, 0.0828, 0.0662, 0.0935, 0.1999, 0.0666,
        0.0873, 0.0544, 0.0765], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([20, 22, 35, 18, 28, 14, 30, 54, 20, 21, 17, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_88.gif
Eval_AverageReturn : -245.89999389648438
Eval_StdReturn : 219.09652709960938
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 213.1999969482422
Train_AverageReturn : -254.7894744873047
Train_StdReturn : 208.15951538085938
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 364544.0
TimeSinceStart : 9835.344974517822
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 81.06986236572266
Loss_Value : -0.09081521816551685
Loss_Entropy : 2.206009268760681
Loss_Representation : -15.183449983596802
Loss_KL : 1.2922276258468628
Loss_Obs : -1.4409185647964478
Loss_Reward : -2.0888750553131104
Loss_Discount : 0.02238324098289013
Loss_RawKL : 1.2922275960445404
mean_target : -5.856958508491516
max_target : -5.569076061248779
min_target : -6.078312516212463
std_target : 0.16370528005063534
Done logging...

Current epsilon: 0.05002546866241013 at iteration 364544


********** Iteration 89 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1162, -0.1162, -0.1154, -0.1160, -0.1167, -0.1159, -0.1154, -0.1149,
        -0.1217, -0.1771, -0.1344, -0.1159, -0.1167, -0.1157, -0.1158],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.1176, -6.1315, -6.1137, -6.0900, -6.0580, -6.0331, -6.0018, -5.9476,
        -5.8892, -5.8291, -5.7273, -5.6759, -5.6584, -5.5941], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.1176, -6.1315, -6.1137, -6.0900, -6.0580, -6.0331, -6.0018, -5.9476,
        -5.8892, -5.8291, -5.7273, -5.6759, -5.6584, -5.5941], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1133, -0.1142, -0.1144, -0.1147, -0.1135, -0.1135, -0.1167, -0.1144,
        -0.1267, -0.1176, -0.1149, -0.1135, -0.1145, -0.1139, -0.1131],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.1664, -6.1387, -6.1042, -6.0657, -6.0349, -5.9933, -5.9571, -5.9177,
        -5.8683, -5.7997, -5.7947, -5.7332, -5.6867, -5.6279], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.1664, -6.1387, -6.1042, -6.0657, -6.0349, -5.9933, -5.9571, -5.9177,
        -5.8683, -5.7997, -5.7947, -5.7332, -5.6867, -5.6279], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0821, -0.0821, -0.0836, -0.0815, -0.0845, -0.0854, -0.0889, -0.1600,
        -0.1602, -0.1146, -0.0887, -0.0847, -0.0841, -0.0830, -0.0827],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.8645, -5.8505, -5.8307, -5.8121, -5.7941, -5.7885, -5.7586, -5.7275,
        -5.6308, -5.5249, -5.4718, -5.4479, -5.4150, -5.3799], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.8645, -5.8505, -5.8307, -5.8121, -5.7941, -5.7885, -5.7586, -5.7275,
        -5.6308, -5.5249, -5.4718, -5.4479, -5.4150, -5.3799], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0839, -0.0839, -0.0841, -0.0850, -0.0840, -0.0861, -0.0936, -0.1260,
        -0.1763, -0.1539, -0.1367, -0.0930, -0.0863, -0.0849, -0.0837],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.9740, -5.9557, -5.9453, -5.9264, -5.9071, -5.8911, -5.8745, -5.8521,
        -5.7824, -5.6650, -5.5923, -5.5290, -5.5006, -5.4847], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.9740, -5.9557, -5.9453, -5.9264, -5.9071, -5.8911, -5.8745, -5.8521,
        -5.7824, -5.6650, -5.5923, -5.5290, -5.5006, -5.4847], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0727, 0.0716, 0.0653, 0.0661, 0.0821, 0.0688, 0.0996, 0.1950, 0.0657,
        0.0838, 0.0542, 0.0751], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([24, 18, 18, 18, 26, 16, 26, 63, 21, 24, 16, 30]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_89.gif
Eval_AverageReturn : -244.5
Eval_StdReturn : 222.29991149902344
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 211.8000030517578
Train_AverageReturn : -297.23529052734375
Train_StdReturn : 181.57302856445312
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 368640.0
TimeSinceStart : 9949.15405201912
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 82.43361282348633
Loss_Value : -0.051001131534576416
Loss_Entropy : 2.1847176551818848
Loss_Representation : -15.366242170333862
Loss_KL : 1.2987681031227112
Loss_Obs : -1.45693239569664
Loss_Reward : -2.120799422264099
Loss_Discount : 0.025113295298069715
Loss_RawKL : 1.2987680435180664
mean_target : -5.953725218772888
max_target : -5.664262056350708
min_target : -6.17838978767395
std_target : 0.16498753055930138
Done logging...

Current epsilon: 0.05002265589764912 at iteration 368640
Saved model checkpoint to ./checkpoints/dreamerv2_basic_run1_iter_90.pt


********** Iteration 90 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1150, -0.1145, -0.1144, -0.1155, -0.1160, -0.1152, -0.1134, -0.1137,
        -0.1152, -0.1152, -0.1759, -0.1336, -0.1211, -0.1163, -0.1165],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.2050, -6.1820, -6.1619, -6.1290, -6.0873, -6.0512, -6.0089, -5.9542,
        -5.9023, -5.8377, -5.7782, -5.6548, -5.5849, -5.5436], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.2050, -6.1820, -6.1619, -6.1290, -6.0873, -6.0512, -6.0089, -5.9542,
        -5.9023, -5.8377, -5.7782, -5.6548, -5.5849, -5.5436], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1124, -0.1129, -0.1114, -0.1135, -0.1134, -0.1179, -0.1280, -0.1267,
        -0.1139, -0.1128, -0.1133, -0.1134, -0.1130, -0.1125, -0.1123],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3370, -6.3071, -6.2795, -6.2380, -6.1948, -6.1444, -6.1002, -6.0533,
        -6.0045, -5.9752, -5.9447, -5.9018, -5.8719, -5.8149], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3370, -6.3071, -6.2795, -6.2380, -6.1948, -6.1444, -6.1002, -6.0533,
        -6.0045, -5.9752, -5.9447, -5.9018, -5.8719, -5.8149], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1054, -0.1473, -0.1013, -0.0825, -0.0824, -0.0821, -0.0820, -0.0814,
        -0.0810, -0.0806, -0.0818, -0.0815, -0.0825, -0.0812, -0.0815],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.9702, -5.9497, -5.8881, -5.8585, -5.8581, -5.8427, -5.8336, -5.8227,
        -5.8025, -5.7813, -5.7636, -5.7420, -5.7155, -5.6943], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.9702, -5.9497, -5.8881, -5.8585, -5.8581, -5.8427, -5.8336, -5.8227,
        -5.8025, -5.7813, -5.7636, -5.7420, -5.7155, -5.6943], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0840, -0.0848, -0.0830, -0.0863, -0.0889, -0.1414, -0.1548, -0.1148,
        -0.0879, -0.0856, -0.0832, -0.0824, -0.0836, -0.0839, -0.0844],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.8664, -5.8430, -5.8249, -5.8107, -5.7898, -5.7740, -5.7129, -5.6255,
        -5.5811, -5.5712, -5.5423, -5.5267, -5.5060, -5.4658], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.8664, -5.8430, -5.8249, -5.8107, -5.7898, -5.7740, -5.7129, -5.6255,
        -5.5811, -5.5712, -5.5423, -5.5267, -5.5060, -5.4658], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0759, 0.0724, 0.0649, 0.0668, 0.0838, 0.0698, 0.1056, 0.1808, 0.0661,
        0.0837, 0.0539, 0.0763], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([21, 22, 21, 17, 24, 19, 37, 59, 13, 26, 17, 24]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_90.gif
Eval_AverageReturn : -294.5
Eval_StdReturn : 194.7620391845703
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -347.6666564941406
Train_StdReturn : 116.34183502197266
Train_MaxReturn : 57.0
Train_MinReturn : -395.0
Train_AverageEpLen : 273.0666809082031
Train_EnvstepsSoFar : 372736.0
TimeSinceStart : 10067.464303731918
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 83.78357887268066
Loss_Value : -0.020854465663433075
Loss_Entropy : 2.201499879360199
Loss_Representation : -15.335743427276611
Loss_KL : 1.3327550292015076
Loss_Obs : -1.460872232913971
Loss_Reward : -2.086814224720001
Loss_Discount : 0.02703772159293294
Loss_RawKL : 1.3327549993991852
mean_target : -6.050665497779846
max_target : -5.761256694793701
min_target : -6.273711562156677
std_target : 0.16459866613149643
Done logging...

Current epsilon: 0.05002015377525611 at iteration 372736


********** Iteration 91 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1150, -0.1135, -0.1138, -0.1132, -0.1141, -0.1131, -0.1129, -0.1136,
        -0.1145, -0.1132, -0.1148, -0.1136, -0.1177, -0.1150, -0.1133],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.1664, -6.1525, -6.1459, -6.1265, -6.0969, -6.0651, -6.0280, -5.9816,
        -5.9384, -5.8843, -5.8228, -5.7667, -5.7099, -5.6480], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.1664, -6.1525, -6.1459, -6.1265, -6.0969, -6.0651, -6.0280, -5.9816,
        -5.9384, -5.8843, -5.8228, -5.7667, -5.7099, -5.6480], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1431, -0.1662, -0.1410, -0.1166, -0.1129, -0.1121, -0.1114, -0.1097,
        -0.1120, -0.1119, -0.1115, -0.1116, -0.1119, -0.1107, -0.1659],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4565, -6.4083, -6.3419, -6.3031, -6.2847, -6.2666, -6.2342, -6.2008,
        -6.1623, -6.1263, -6.0824, -6.0271, -5.9677, -5.8977], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4565, -6.4083, -6.3419, -6.3031, -6.2847, -6.2666, -6.2342, -6.2008,
        -6.1623, -6.1263, -6.0824, -6.0271, -5.9677, -5.8977], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1251, -0.1419, -0.0897, -0.0831, -0.0832, -0.0817, -0.0827, -0.0819,
        -0.0819, -0.0816, -0.0824, -0.0826, -0.0818, -0.0816, -0.0811],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.0180, -5.9746, -5.9143, -5.9353, -5.9342, -5.9334, -5.9372, -5.9158,
        -5.8948, -5.8864, -5.8683, -5.8461, -5.8330, -5.8036], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.0180, -5.9746, -5.9143, -5.9353, -5.9342, -5.9334, -5.9372, -5.9158,
        -5.8948, -5.8864, -5.8683, -5.8461, -5.8330, -5.8036], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0842, -0.0843, -0.0851, -0.0848, -0.0859, -0.0863, -0.0839, -0.0840,
        -0.0838, -0.0832, -0.0835, -0.0840, -0.0830, -0.0826, -0.0829],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3853, -6.3801, -6.3719, -6.3546, -6.3311, -6.3066, -6.2854, -6.2637,
        -6.2378, -6.2108, -6.1879, -6.1536, -6.1206, -6.0862], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3853, -6.3801, -6.3719, -6.3546, -6.3311, -6.3066, -6.2854, -6.2637,
        -6.2378, -6.2108, -6.1879, -6.1536, -6.1206, -6.0862], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0781, 0.0720, 0.0633, 0.0665, 0.0847, 0.0692, 0.1100, 0.1783, 0.0656,
        0.0827, 0.0527, 0.0769], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([25, 22, 10, 22, 21, 22, 41, 47, 22, 25, 18, 25]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_91.gif
Eval_AverageReturn : -252.10000610351562
Eval_StdReturn : 211.0689239501953
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 217.39999389648438
Train_AverageReturn : -218.76190185546875
Train_StdReturn : 219.9097137451172
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 376832.0
TimeSinceStart : 10182.16618514061
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 85.00109672546387
Loss_Value : -0.04620793089270592
Loss_Entropy : 2.1918214559555054
Loss_Representation : -15.338025331497192
Loss_KL : 1.3689805269241333
Loss_Obs : -1.4633272886276245
Loss_Reward : -2.10425865650177
Loss_Discount : 0.030526168644428253
Loss_RawKL : 1.3689804673194885
mean_target : -6.137355446815491
max_target : -5.859544396400452
min_target : -6.345615744590759
std_target : 0.15619438514113426
Done logging...

Current epsilon: 0.050017927987818636 at iteration 376832


********** Iteration 92 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1429, -0.1926, -0.1627, -0.1113, -0.1138, -0.1141, -0.1146, -0.1134,
        -0.1135, -0.1141, -0.1140, -0.1150, -0.1138, -0.1139, -0.1151],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4927, -6.4396, -6.3494, -6.2929, -6.2953, -6.2577, -6.2380, -6.2057,
        -6.1614, -6.1202, -6.0793, -6.0257, -5.9865, -5.9407], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4927, -6.4396, -6.3494, -6.2929, -6.2953, -6.2577, -6.2380, -6.2057,
        -6.1614, -6.1202, -6.0793, -6.0257, -5.9865, -5.9407], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1109, -0.1122, -0.1108, -0.1120, -0.1106, -0.1117, -0.1113, -0.1107,
        -0.1115, -0.1107, -0.1137, -0.1099, -0.1119, -0.1104, -0.1115],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4373, -6.4256, -6.3987, -6.3798, -6.3731, -6.3396, -6.3107, -6.2832,
        -6.2422, -6.2043, -6.1462, -6.0965, -6.0331, -5.9633], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4373, -6.4256, -6.3987, -6.3798, -6.3731, -6.3396, -6.3107, -6.2832,
        -6.2422, -6.2043, -6.1462, -6.0965, -6.0331, -5.9633], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0831, -0.0822, -0.0817, -0.0811, -0.0813, -0.0814, -0.0821, -0.0823,
        -0.0838, -0.0852, -0.1274, -0.2112, -0.1948, -0.1447, -0.0849],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.2761, -6.2800, -6.2876, -6.2791, -6.2796, -6.2777, -6.2747, -6.2706,
        -6.2528, -6.2292, -6.1998, -6.1243, -5.9666, -5.8183], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.2761, -6.2800, -6.2876, -6.2791, -6.2796, -6.2777, -6.2747, -6.2706,
        -6.2528, -6.2292, -6.1998, -6.1243, -5.9666, -5.8183], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0836, -0.0834, -0.0837, -0.0838, -0.0839, -0.0837, -0.0835, -0.0829,
        -0.0842, -0.0962, -0.1646, -0.1508, -0.0934, -0.0844, -0.0823],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.0760, -6.0862, -6.0644, -6.0521, -6.0327, -6.0150, -6.0054, -5.9804,
        -5.9566, -5.9266, -5.8892, -5.7782, -5.7084, -5.6919], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.0760, -6.0862, -6.0644, -6.0521, -6.0327, -6.0150, -6.0054, -5.9804,
        -5.9566, -5.9266, -5.8892, -5.7782, -5.7084, -5.6919], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0812, 0.0736, 0.0623, 0.0673, 0.0883, 0.0685, 0.1157, 0.1592, 0.0669,
        0.0846, 0.0523, 0.0801], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([25, 22, 16, 22, 22, 21, 32, 46, 20, 27, 17, 30]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_92.gif
Eval_AverageReturn : -155.10000610351562
Eval_StdReturn : 234.99337768554688
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 158.60000610351562
Train_AverageReturn : -234.5
Train_StdReturn : 212.03643798828125
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 380928.0
TimeSinceStart : 10288.152041435242
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 85.74614524841309
Loss_Value : -0.13096000091172755
Loss_Entropy : 2.225456655025482
Loss_Representation : -15.362897634506226
Loss_KL : 1.3542892634868622
Loss_Obs : -1.4662086963653564
Loss_Reward : -2.0803252458572388
Loss_Discount : 0.025225022807717323
Loss_RawKL : 1.3542892634868622
mean_target : -6.1915669441223145
max_target : -5.925856113433838
min_target : -6.395242214202881
std_target : 0.15054477751255035
Done logging...

Current epsilon: 0.05001594801684254 at iteration 380928


********** Iteration 93 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1135, -0.1133, -0.1128, -0.1125, -0.1137, -0.1130, -0.1130, -0.1124,
        -0.1122, -0.1125, -0.1141, -0.1132, -0.1393, -0.1747, -0.1770],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3853, -6.3659, -6.3469, -6.3232, -6.2925, -6.2751, -6.2457, -6.2221,
        -6.1852, -6.1504, -6.1003, -6.0526, -6.0039, -5.9306], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3853, -6.3659, -6.3469, -6.3232, -6.2925, -6.2751, -6.2457, -6.2221,
        -6.1852, -6.1504, -6.1003, -6.0526, -6.0039, -5.9306], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1110, -0.1120, -0.1115, -0.1110, -0.1114, -0.1120, -0.1114, -0.1123,
        -0.1114, -0.1110, -0.1119, -0.1141, -0.1945, -0.1459, -0.1189],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4416, -6.4168, -6.3910, -6.3733, -6.3516, -6.3237, -6.2907, -6.2483,
        -6.2186, -6.1775, -6.1334, -6.0690, -6.0097, -5.8644], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4416, -6.4168, -6.3910, -6.3733, -6.3516, -6.3237, -6.2907, -6.2483,
        -6.2186, -6.1775, -6.1334, -6.0690, -6.0097, -5.8644], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0833, -0.0819, -0.0823, -0.0816, -0.0820, -0.0816, -0.0826, -0.0825,
        -0.0819, -0.0817, -0.0836, -0.0933, -0.0951, -0.1202, -0.1338],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4412, -6.4310, -6.4222, -6.4105, -6.3939, -6.3728, -6.3450, -6.3186,
        -6.2876, -6.2588, -6.2241, -6.1872, -6.1304, -6.0705], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4412, -6.4310, -6.4222, -6.4105, -6.3939, -6.3728, -6.3450, -6.3186,
        -6.2876, -6.2588, -6.2241, -6.1872, -6.1304, -6.0705], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0831, -0.0832, -0.0838, -0.0828, -0.0870, -0.0836, -0.0908, -0.1880,
        -0.1743, -0.0951, -0.0837, -0.0888, -0.0827, -0.0828, -0.0829],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3232, -6.3383, -6.3394, -6.3379, -6.3334, -6.3131, -6.2948, -6.2579,
        -6.1308, -6.0128, -5.9734, -5.9399, -5.8958, -5.8505], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3232, -6.3383, -6.3394, -6.3379, -6.3334, -6.3131, -6.2948, -6.2579,
        -6.1308, -6.0128, -5.9734, -5.9399, -5.8958, -5.8505], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0831, 0.0736, 0.0602, 0.0669, 0.0907, 0.0662, 0.1184, 0.1547, 0.0671,
        0.0856, 0.0511, 0.0825], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([22, 20, 15, 19, 28, 19, 38, 48, 19, 34,  8, 30]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_93.gif
Eval_AverageReturn : -301.6000061035156
Eval_StdReturn : 180.1278533935547
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 246.8000030517578
Train_AverageReturn : -319.3125
Train_StdReturn : 155.9582061767578
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 385024.0
TimeSinceStart : 10407.173394918442
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 86.71217346191406
Loss_Value : -0.12484750524163246
Loss_Entropy : 2.22500479221344
Loss_Representation : -15.374824523925781
Loss_KL : 1.34352445602417
Loss_Obs : -1.4591939151287079
Loss_Reward : -2.1481988430023193
Loss_Discount : 0.021789225284010172
Loss_RawKL : 1.34352445602417
mean_target : -6.260544776916504
max_target : -5.991794466972351
min_target : -6.47195839881897
std_target : 0.15421350114047527
Done logging...

Current epsilon: 0.05001418671430296 at iteration 385024


********** Iteration 94 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1129, -0.1121, -0.1127, -0.1128, -0.1125, -0.1121, -0.1124, -0.1114,
        -0.1125, -0.1122, -0.1286, -0.1579, -0.1786, -0.1715, -0.1114],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4506, -6.4419, -6.4274, -6.4169, -6.3929, -6.3614, -6.3255, -6.2920,
        -6.2430, -6.1897, -6.1290, -6.0455, -5.9526, -5.8185], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4506, -6.4419, -6.4274, -6.4169, -6.3929, -6.3614, -6.3255, -6.2920,
        -6.2430, -6.1897, -6.1290, -6.0455, -5.9526, -5.8185], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1120, -0.1109, -0.1114, -0.1104, -0.1107, -0.1116, -0.1107, -0.1102,
        -0.1102, -0.1106, -0.1102, -0.1106, -0.1125, -0.1100, -0.1105],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.2902, -6.2692, -6.2455, -6.2224, -6.2082, -6.1703, -6.1521, -6.1120,
        -6.0750, -6.0353, -5.9938, -5.9433, -5.8895, -5.8231], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.2902, -6.2692, -6.2455, -6.2224, -6.2082, -6.1703, -6.1521, -6.1120,
        -6.0750, -6.0353, -5.9938, -5.9433, -5.8895, -5.8231], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0826, -0.0822, -0.0818, -0.0822, -0.0816, -0.0833, -0.0837, -0.1460,
        -0.1808, -0.0991, -0.0824, -0.0816, -0.0824, -0.0822, -0.0824],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.1682, -6.1584, -6.1363, -6.1322, -6.1156, -6.1088, -6.0845, -6.0672,
        -5.9886, -5.8793, -5.8984, -5.8645, -5.8525, -5.8141], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.1682, -6.1584, -6.1363, -6.1322, -6.1156, -6.1088, -6.0845, -6.0672,
        -5.9886, -5.8793, -5.8984, -5.8645, -5.8525, -5.8141], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0834, -0.0836, -0.0836, -0.0827, -0.0829, -0.0832, -0.0825, -0.0828,
        -0.0831, -0.0823, -0.0840, -0.0862, -0.0908, -0.1347, -0.1745],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4961, -6.5013, -6.4915, -6.4801, -6.4701, -6.4521, -6.4318, -6.4075,
        -6.3852, -6.3659, -6.3418, -6.3081, -6.2614, -6.2133], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4961, -6.5013, -6.4915, -6.4801, -6.4701, -6.4521, -6.4318, -6.4075,
        -6.3852, -6.3659, -6.3418, -6.3081, -6.2614, -6.2133], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0821, 0.0711, 0.0559, 0.0639, 0.0912, 0.0610, 0.1164, 0.1760, 0.0655,
        0.0857, 0.0478, 0.0834], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([33, 14, 21, 16, 23, 17, 36, 42, 22, 28, 17, 31]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_94.gif
Eval_AverageReturn : -148.89999389648438
Eval_StdReturn : 240.164306640625
Eval_MaxReturn : 95.0
Eval_MinReturn : -390.0
Eval_AverageEpLen : 154.39999389648438
Train_AverageReturn : -320.25
Train_StdReturn : 161.4572296142578
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 389120.0
TimeSinceStart : 10512.658470630646
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 87.72234725952148
Loss_Value : -0.1327920611947775
Loss_Entropy : 2.1700921654701233
Loss_Representation : -15.717474222183228
Loss_KL : 1.2964639961719513
Loss_Obs : -1.488330990076065
Loss_Reward : -2.159255713224411
Loss_Discount : 0.028627431020140648
Loss_RawKL : 1.2964639961719513
mean_target : -6.331059217453003
max_target : -6.0864362716674805
min_target : -6.51665198802948
std_target : 0.13818220794200897
Done logging...

Current epsilon: 0.05001261993040896 at iteration 389120


********** Iteration 95 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1125, -0.1089, -0.1079, -0.1083, -0.1103, -0.1103, -0.1116, -0.1128,
        -0.1119, -0.1128, -0.1116, -0.1117, -0.1092, -0.1085, -0.1262],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.1125, -6.0904, -6.1140, -6.0945, -6.1426, -6.1278, -6.0871, -6.0637,
        -6.0568, -6.0064, -5.9777, -5.9256, -5.8654, -5.8046], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.1125, -6.0904, -6.1140, -6.0945, -6.1426, -6.1278, -6.0871, -6.0637,
        -6.0568, -6.0064, -5.9777, -5.9256, -5.8654, -5.8046], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1093, -0.1119, -0.1105, -0.1103, -0.1104, -0.1099, -0.1100, -0.1108,
        -0.1106, -0.1091, -0.1090, -0.1093, -0.1086, -0.1564, -0.1373],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.6481, -6.6476, -6.6297, -6.6139, -6.5864, -6.5548, -6.5304, -6.5002,
        -6.4621, -6.4241, -6.3747, -6.3181, -6.2591, -6.1932], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.6481, -6.6476, -6.6297, -6.6139, -6.5864, -6.5548, -6.5304, -6.5002,
        -6.4621, -6.4241, -6.3747, -6.3181, -6.2591, -6.1932], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0817, -0.0810, -0.0806, -0.0815, -0.0813, -0.0816, -0.0814, -0.0858,
        -0.0935, -0.1558, -0.1494, -0.1085, -0.0847, -0.0816, -0.0827],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.5943, -6.5974, -6.5994, -6.5843, -6.5789, -6.5700, -6.5534, -6.5383,
        -6.5099, -6.4718, -6.3694, -6.2705, -6.2147, -6.1869], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.5943, -6.5974, -6.5994, -6.5843, -6.5789, -6.5700, -6.5534, -6.5383,
        -6.5099, -6.4718, -6.3694, -6.2705, -6.2147, -6.1869], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1207, -0.1121, -0.0888, -0.0835, -0.0836, -0.0826, -0.0829, -0.0824,
        -0.0828, -0.0849, -0.0870, -0.1767, -0.2062, -0.0879, -0.0830],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.5779, -6.5500, -6.5242, -6.5198, -6.5238, -6.5186, -6.4991, -6.4808,
        -6.4712, -6.4550, -6.4258, -6.3866, -6.2562, -6.0990], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.5779, -6.5500, -6.5242, -6.5198, -6.5238, -6.5186, -6.4991, -6.4808,
        -6.4712, -6.4550, -6.4258, -6.3866, -6.2562, -6.0990], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0869, 0.0729, 0.0560, 0.0651, 0.0976, 0.0606, 0.1201, 0.1444, 0.0686,
        0.0892, 0.0485, 0.0901], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([31, 19, 13, 14, 38, 13, 25, 60, 21, 22, 15, 29]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_95.gif
Eval_AverageReturn : -198.89999389648438
Eval_StdReturn : 236.1543731689453
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 183.8000030517578
Train_AverageReturn : -320.875
Train_StdReturn : 161.4337615966797
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 393216.0
TimeSinceStart : 10622.455506324768
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 88.62004661560059
Loss_Value : -0.1572462059557438
Loss_Entropy : 2.231459677219391
Loss_Representation : -15.510394096374512
Loss_KL : 1.3109218180179596
Loss_Obs : -1.4627269506454468
Loss_Reward : -2.2143154740333557
Loss_Discount : 0.02026947634294629
Loss_RawKL : 1.3109217286109924
mean_target : -6.397022366523743
max_target : -6.1561830043792725
min_target : -6.585016846656799
std_target : 0.13697975873947144
Done logging...

Current epsilon: 0.050011226182477914 at iteration 393216


********** Iteration 96 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1097, -0.1102, -0.1104, -0.1107, -0.1101, -0.1099, -0.1088, -0.1102,
        -0.1113, -0.1118, -0.1725, -0.1485, -0.1150, -0.1075, -0.1091],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.9959, -6.9661, -6.9373, -6.9081, -6.8706, -6.8266, -6.7759, -6.7222,
        -6.6655, -6.6014, -6.5418, -6.4217, -6.3276, -6.2562], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.9959, -6.9661, -6.9373, -6.9081, -6.8706, -6.8266, -6.7759, -6.7222,
        -6.6655, -6.6014, -6.5418, -6.4217, -6.3276, -6.2562], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1086, -0.1092, -0.1097, -0.1091, -0.1094, -0.1088, -0.1304, -0.1910,
        -0.1133, -0.1095, -0.1107, -0.1096, -0.1104, -0.1095, -0.1093],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.6586, -6.6414, -6.6133, -6.5846, -6.5517, -6.5077, -6.4627, -6.4029,
        -6.2777, -6.2515, -6.2070, -6.1565, -6.1001, -6.0465], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.6586, -6.6414, -6.6133, -6.5846, -6.5517, -6.5077, -6.4627, -6.4029,
        -6.2777, -6.2515, -6.2070, -6.1565, -6.1001, -6.0465], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0814, -0.0817, -0.0817, -0.0817, -0.0811, -0.1176, -0.1555, -0.1480,
        -0.0902, -0.0816, -0.0823, -0.0820, -0.0818, -0.0817, -0.0820],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.6298, -6.6341, -6.6309, -6.6226, -6.6094, -6.5946, -6.5364, -6.4397,
        -6.3422, -6.3094, -6.2730, -6.2366, -6.2078, -6.1758], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.6298, -6.6341, -6.6309, -6.6226, -6.6094, -6.5946, -6.5364, -6.4397,
        -6.3422, -6.3094, -6.2730, -6.2366, -6.2078, -6.1758], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0834, -0.0841, -0.0835, -0.0831, -0.0832, -0.0824, -0.0838, -0.1176,
        -0.1078, -0.0880, -0.0862, -0.0844, -0.0829, -0.0832, -0.0848],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.7043, -6.6911, -6.6789, -6.6661, -6.6464, -6.6288, -6.6019, -6.5864,
        -6.5310, -6.4827, -6.4615, -6.4362, -6.4077, -6.3729], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.7043, -6.6911, -6.6789, -6.6661, -6.6464, -6.6288, -6.6019, -6.5864,
        -6.5310, -6.4827, -6.4615, -6.4362, -6.4077, -6.3729], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0868, 0.0704, 0.0531, 0.0622, 0.0987, 0.0569, 0.1155, 0.1599, 0.0680,
        0.0901, 0.0465, 0.0919], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([20, 21, 17, 16, 25, 16, 39, 54, 25, 16, 15, 36]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_96.gif
Eval_AverageReturn : -105.69999694824219
Eval_StdReturn : 234.4461669921875
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 127.80000305175781
Train_AverageReturn : -323.0625
Train_StdReturn : 157.50057983398438
Train_MaxReturn : 76.0
Train_MinReturn : -400.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 397312.0
TimeSinceStart : 10724.303734779358
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 90.08845520019531
Loss_Value : -0.08501878008246422
Loss_Entropy : 2.188901364803314
Loss_Representation : -15.467747449874878
Loss_KL : 1.32883021235466
Loss_Obs : -1.4615598320960999
Loss_Reward : -2.203944206237793
Loss_Discount : 0.022964564617723227
Loss_RawKL : 1.3288301825523376
mean_target : -6.500617027282715
max_target : -6.241925001144409
min_target : -6.7031004428863525
std_target : 0.14777535200119019
Done logging...

Current epsilon: 0.05000998636037944 at iteration 397312


********** Iteration 97 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1099, -0.1131, -0.1098, -0.1095, -0.1098, -0.1102, -0.1094, -0.1107,
        -0.1292, -0.1344, -0.1115, -0.1106, -0.1107, -0.1117, -0.1115],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.5177, -6.5342, -6.5188, -6.4968, -6.4866, -6.4770, -6.4570, -6.4222,
        -6.3859, -6.3180, -6.2599, -6.2175, -6.1644, -6.1087], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.5177, -6.5342, -6.5188, -6.4968, -6.4866, -6.4770, -6.4570, -6.4222,
        -6.3859, -6.3180, -6.2599, -6.2175, -6.1644, -6.1087], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1296, -0.1429, -0.1207, -0.1087, -0.1094, -0.1084, -0.1087, -0.1083,
        -0.1094, -0.1095, -0.1088, -0.1096, -0.1094, -0.1083, -0.1672],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.5040, -6.4575, -6.4020, -6.3765, -6.3539, -6.3383, -6.3136, -6.2884,
        -6.2707, -6.2368, -6.2117, -6.1653, -6.1169, -6.0615], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.5040, -6.4575, -6.4020, -6.3765, -6.3539, -6.3383, -6.3136, -6.2884,
        -6.2707, -6.2368, -6.2117, -6.1653, -6.1169, -6.0615], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1068, -0.1438, -0.0873, -0.0843, -0.0814, -0.0805, -0.0812, -0.0818,
        -0.0806, -0.0807, -0.0818, -0.0808, -0.0819, -0.0818, -0.0817],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3952, -6.3755, -6.3179, -6.3142, -6.3054, -6.2978, -6.2897, -6.2777,
        -6.2734, -6.2650, -6.2448, -6.2247, -6.1988, -6.1856], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3952, -6.3755, -6.3179, -6.3142, -6.3054, -6.2978, -6.2897, -6.2777,
        -6.2734, -6.2650, -6.2448, -6.2247, -6.1988, -6.1856], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0838, -0.0827, -0.0819, -0.0834, -0.0825, -0.0824, -0.0838, -0.0832,
        -0.0832, -0.0823, -0.0828, -0.0871, -0.0826, -0.0922, -0.1237],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3318, -6.3277, -6.3108, -6.2998, -6.2953, -6.2796, -6.2671, -6.2443,
        -6.2209, -6.1914, -6.1823, -6.1651, -6.1253, -6.0932], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3318, -6.3277, -6.3108, -6.2998, -6.2953, -6.2796, -6.2671, -6.2443,
        -6.2209, -6.1914, -6.1823, -6.1651, -6.1253, -6.0932], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0902, 0.0706, 0.0525, 0.0613, 0.1037, 0.0561, 0.1135, 0.1433, 0.0705,
        0.0940, 0.0467, 0.0977], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([24, 22, 13, 17, 29, 18, 35, 38, 28, 32, 14, 30]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_97.gif
Eval_AverageReturn : -247.39999389648438
Eval_StdReturn : 220.08143615722656
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 213.1999969482422
Train_AverageReturn : -275.1111145019531
Train_StdReturn : 201.1803741455078
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 227.55555725097656
Train_EnvstepsSoFar : 401408.0
TimeSinceStart : 10838.15199136734
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 90.22893524169922
Loss_Value : -0.1562962317839265
Loss_Entropy : 2.222365140914917
Loss_Representation : -15.586199045181274
Loss_KL : 1.3243254125118256
Loss_Obs : -1.4669365584850311
Loss_Reward : -2.26755091547966
Loss_Discount : 0.026392437517642975
Loss_RawKL : 1.3243253529071808
mean_target : -6.511675953865051
max_target : -6.266592979431152
min_target : -6.705048084259033
std_target : 0.14065994136035442
Done logging...

Current epsilon: 0.05000888346451025 at iteration 401408


********** Iteration 98 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1105, -0.1104, -0.1103, -0.1107, -0.1106, -0.1155, -0.2024, -0.1855,
        -0.1133, -0.1115, -0.1111, -0.1096, -0.1103, -0.1106, -0.1081],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.9138, -6.8966, -6.8746, -6.8480, -6.8187, -6.7751, -6.7266, -6.5914,
        -6.4770, -6.4453, -6.3971, -6.3424, -6.2988, -6.2546], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.9138, -6.8966, -6.8746, -6.8480, -6.8187, -6.7751, -6.7266, -6.5914,
        -6.4770, -6.4453, -6.3971, -6.3424, -6.2988, -6.2546], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1280, -0.1251, -0.1125, -0.1100, -0.1084, -0.1089, -0.1083, -0.1075,
        -0.1085, -0.1086, -0.1088, -0.1093, -0.1096, -0.1088, -0.1177],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.8594, -6.8233, -6.7775, -6.7585, -6.7246, -6.6949, -6.6654, -6.6460,
        -6.6083, -6.5698, -6.5303, -6.4820, -6.4341, -6.3777], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.8594, -6.8233, -6.7775, -6.7585, -6.7246, -6.6949, -6.6654, -6.6460,
        -6.6083, -6.5698, -6.5303, -6.4820, -6.4341, -6.3777], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0824, -0.0824, -0.0819, -0.0811, -0.0820, -0.0821, -0.0826, -0.0826,
        -0.0822, -0.0828, -0.0833, -0.0863, -0.0830, -0.0827, -0.0824],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3933, -6.3854, -6.3798, -6.3803, -6.3766, -6.3658, -6.3655, -6.3446,
        -6.3176, -6.2969, -6.2773, -6.3025, -6.3023, -6.3128], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3933, -6.3854, -6.3798, -6.3803, -6.3766, -6.3658, -6.3655, -6.3446,
        -6.3176, -6.2969, -6.2773, -6.3025, -6.3023, -6.3128], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0842, -0.0840, -0.0830, -0.0827, -0.0839, -0.0841, -0.0838, -0.0831,
        -0.0857, -0.0837, -0.1146, -0.2370, -0.0904, -0.0846, -0.0845],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.3343, -6.3504, -6.3464, -6.3410, -6.3556, -6.3583, -6.3515, -6.3590,
        -6.3493, -6.3346, -6.3126, -6.2732, -6.0928, -6.0785], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.3343, -6.3504, -6.3464, -6.3410, -6.3556, -6.3583, -6.3515, -6.3590,
        -6.3493, -6.3346, -6.3126, -6.2732, -6.0928, -6.0785], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0875, 0.0673, 0.0498, 0.0577, 0.1042, 0.0520, 0.1020, 0.1693, 0.0700,
        0.0961, 0.0446, 0.0994], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([25, 19, 11, 21, 41, 14, 20, 57, 17, 34, 12, 29]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_98.gif
Eval_AverageReturn : -295.0
Eval_StdReturn : 195.0128173828125
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -349.3333435058594
Train_StdReturn : 123.5025863647461
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 273.0666809082031
Train_EnvstepsSoFar : 405504.0
TimeSinceStart : 10955.725180149078
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 91.2723617553711
Loss_Value : -0.13822002708911896
Loss_Entropy : 2.1738705039024353
Loss_Representation : -15.62234377861023
Loss_KL : 1.3147469460964203
Loss_Obs : -1.4673281013965607
Loss_Reward : -2.2844706177711487
Loss_Discount : 0.020660818088799715
Loss_RawKL : 1.314746916294098
mean_target : -6.584745407104492
max_target : -6.330025553703308
min_target : -6.785747766494751
std_target : 0.14579729363322258
Done logging...

Current epsilon: 0.050007902372707004 at iteration 405504


********** Iteration 99 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1114, -0.1114, -0.1125, -0.1109, -0.1111, -0.1109, -0.1106, -0.1096,
        -0.1196, -0.1841, -0.1409, -0.1146, -0.1119, -0.1117, -0.1120],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.8517, -6.8317, -6.8165, -6.7894, -6.7538, -6.7185, -6.6821, -6.6402,
        -6.5929, -6.5314, -6.4195, -6.3388, -6.2785, -6.2231], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.8517, -6.8317, -6.8165, -6.7894, -6.7538, -6.7185, -6.6821, -6.6402,
        -6.5929, -6.5314, -6.4195, -6.3388, -6.2785, -6.2231], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1112, -0.1108, -0.1110, -0.1111, -0.1097, -0.1097, -0.1085, -0.1093,
        -0.1105, -0.1082, -0.1136, -0.1400, -0.1351, -0.1148, -0.1114],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.9665, -6.9671, -6.9427, -6.9116, -6.8836, -6.8435, -6.8037, -6.7652,
        -6.7236, -6.6708, -6.6212, -6.5556, -6.4636, -6.3744], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.9665, -6.9671, -6.9427, -6.9116, -6.8836, -6.8435, -6.8037, -6.7652,
        -6.7236, -6.6708, -6.6212, -6.5556, -6.4636, -6.3744], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0847, -0.0840, -0.0845, -0.0837, -0.0839, -0.0838, -0.0834, -0.0838,
        -0.0838, -0.0826, -0.1206, -0.1627, -0.1508, -0.0972, -0.0846],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.6539, -6.6558, -6.6500, -6.6425, -6.6376, -6.6338, -6.6246, -6.6030,
        -6.5844, -6.5649, -6.5401, -6.4680, -6.3473, -6.2582], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.6539, -6.6558, -6.6500, -6.6425, -6.6376, -6.6338, -6.6246, -6.6030,
        -6.5844, -6.5649, -6.5401, -6.4680, -6.3473, -6.2582], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0874, -0.0859, -0.0858, -0.0837, -0.0851, -0.0842, -0.0849, -0.0887,
        -0.0846, -0.0953, -0.1453, -0.2269, -0.1897, -0.1363, -0.0889],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0356, -7.0475, -7.0382, -7.0316, -7.0219, -7.0095, -6.9942, -6.9797,
        -6.9558, -6.9276, -6.8897, -6.7962, -6.6201, -6.4708], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0356, -7.0475, -7.0382, -7.0316, -7.0219, -7.0095, -6.9942, -6.9797,
        -6.9558, -6.9276, -6.8897, -6.7962, -6.6201, -6.4708], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0901, 0.0656, 0.0491, 0.0557, 0.1062, 0.0515, 0.0946, 0.1735, 0.0715,
        0.0969, 0.0444, 0.1008], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([25, 19, 13, 18, 32, 15, 20, 60, 25, 37, 12, 24]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_99.gif
Eval_AverageReturn : -249.39999389648438
Eval_StdReturn : 221.40966796875
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 213.1999969482422
Train_AverageReturn : -222.8095245361328
Train_StdReturn : 220.61053466796875
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 409600.0
TimeSinceStart : 11069.624816894531
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 92.39133834838867
Loss_Value : -0.05014057457447052
Loss_Entropy : 2.158461570739746
Loss_Representation : -15.677479028701782
Loss_KL : 1.3193103969097137
Loss_Obs : -1.4735075533390045
Loss_Reward : -2.287002682685852
Loss_Discount : 0.025288627482950687
Loss_RawKL : 1.319310337305069
mean_target : -6.66421103477478
max_target : -6.401067495346069
min_target : -6.866913080215454
std_target : 0.14978172443807125
Done logging...

Current epsilon: 0.05000702963290149 at iteration 409600


********** Iteration 100 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1111, -0.1130, -0.1119, -0.1132, -0.1124, -0.1130, -0.1124, -0.1117,
        -0.1110, -0.1774, -0.2349, -0.1137, -0.1139, -0.1137, -0.1136],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0405, -7.0353, -7.0220, -7.0069, -6.9903, -6.9636, -6.9415, -6.9202,
        -6.8793, -6.8375, -6.7343, -6.5574, -6.5204, -6.4490], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0405, -7.0353, -7.0220, -7.0069, -6.9903, -6.9636, -6.9415, -6.9202,
        -6.8793, -6.8375, -6.7343, -6.5574, -6.5204, -6.4490], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1118, -0.1120, -0.1115, -0.1111, -0.1115, -0.1121, -0.1118, -0.1112,
        -0.1110, -0.1193, -0.2022, -0.1963, -0.1166, -0.1116, -0.1127],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0886, -7.0649, -7.0391, -7.0182, -7.0112, -6.9844, -6.9645, -6.9327,
        -6.8941, -6.8486, -6.8017, -6.6659, -6.5313, -6.4837], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0886, -7.0649, -7.0391, -7.0182, -7.0112, -6.9844, -6.9645, -6.9327,
        -6.8941, -6.8486, -6.8017, -6.6659, -6.5313, -6.4837], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0857, -0.0855, -0.0855, -0.0854, -0.0850, -0.0852, -0.0856, -0.0854,
        -0.0899, -0.0883, -0.1105, -0.1867, -0.0902, -0.0858, -0.0861],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.6771, -6.6688, -6.6593, -6.6514, -6.6388, -6.6232, -6.6080, -6.5884,
        -6.5661, -6.5315, -6.4962, -6.4431, -6.3139, -6.2936], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.6771, -6.6688, -6.6593, -6.6514, -6.6388, -6.6232, -6.6080, -6.5884,
        -6.5661, -6.5315, -6.4962, -6.4431, -6.3139, -6.2936], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1475, -0.1668, -0.1110, -0.0869, -0.0870, -0.0855, -0.0853, -0.0857,
        -0.0867, -0.0853, -0.0877, -0.0866, -0.0863, -0.0869, -0.0880],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.7882, -6.7447, -6.6645, -6.6536, -6.6445, -6.6473, -6.6366, -6.6267,
        -6.6345, -6.6109, -6.5919, -6.5708, -6.5423, -6.5123], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.7882, -6.7447, -6.6645, -6.6536, -6.6445, -6.6473, -6.6366, -6.6267,
        -6.6345, -6.6109, -6.5919, -6.5708, -6.5423, -6.5123], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0955, 0.0656, 0.0493, 0.0546, 0.1096, 0.0529, 0.0892, 0.1624, 0.0745,
        0.0988, 0.0451, 0.1024], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([28, 20, 14, 12, 30, 15, 30, 46, 30, 34, 11, 30]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_100.gif
Eval_AverageReturn : -301.70001220703125
Eval_StdReturn : 184.5643768310547
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 245.39999389648438
Train_AverageReturn : -221.38095092773438
Train_StdReturn : 227.25494384765625
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 195.04762268066406
Train_EnvstepsSoFar : 413696.0
TimeSinceStart : 11187.832125902176
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 93.04611015319824
Loss_Value : -0.11051387712359428
Loss_Entropy : 2.1797807216644287
Loss_Representation : -15.673802137374878
Loss_KL : 1.3361722230911255
Loss_Obs : -1.4689770340919495
Loss_Reward : -2.3456298112869263
Loss_Discount : 0.025425824336707592
Loss_RawKL : 1.3361721336841583
mean_target : -6.7116276025772095
max_target : -6.457098960876465
min_target : -6.90745210647583
std_target : 0.14447707682847977
Done logging...

Current epsilon: 0.05000625327867488 at iteration 413696


********** Iteration 101 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1382, -0.1839, -0.1633, -0.1154, -0.1127, -0.1120, -0.1123, -0.1124,
        -0.1130, -0.1112, -0.1118, -0.1125, -0.1137, -0.1144, -0.1137],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0512, -7.0343, -6.9601, -6.9114, -6.8936, -6.8697, -6.8637, -6.8483,
        -6.8201, -6.8034, -6.7678, -6.7263, -6.6773, -6.6269], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0512, -7.0343, -6.9601, -6.9114, -6.8936, -6.8697, -6.8637, -6.8483,
        -6.8201, -6.8034, -6.7678, -6.7263, -6.6773, -6.6269], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1124, -0.1125, -0.1108, -0.1124, -0.1116, -0.1125, -0.1117, -0.1586,
        -0.1843, -0.1144, -0.1127, -0.1121, -0.1119, -0.1116, -0.1106],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.9511, -6.9377, -6.9143, -6.8871, -6.8574, -6.8173, -6.7648, -6.7190,
        -6.6394, -6.5276, -6.4842, -6.4498, -6.4043, -6.3518], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.9511, -6.9377, -6.9143, -6.8871, -6.8574, -6.8173, -6.7648, -6.7190,
        -6.6394, -6.5276, -6.4842, -6.4498, -6.4043, -6.3518], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0858, -0.0862, -0.0859, -0.0850, -0.0860, -0.0860, -0.0863, -0.0862,
        -0.0854, -0.0861, -0.0878, -0.2156, -0.1238, -0.0863, -0.0869],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.8946, -6.9032, -6.8962, -6.8839, -6.8694, -6.8558, -6.8416, -6.8219,
        -6.7972, -6.7676, -6.7330, -6.7029, -6.5359, -6.4649], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.8946, -6.9032, -6.8962, -6.8839, -6.8694, -6.8558, -6.8416, -6.8219,
        -6.7972, -6.7676, -6.7330, -6.7029, -6.5359, -6.4649], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0889, -0.0871, -0.0870, -0.0867, -0.0868, -0.0873, -0.0872, -0.0873,
        -0.0877, -0.0872, -0.0875, -0.0872, -0.0889, -0.2290, -0.1061],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.5885, -6.5771, -6.5819, -6.5900, -6.6051, -6.6214, -6.6355, -6.6267,
        -6.6163, -6.6071, -6.5890, -6.5600, -6.5136, -6.4789], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.5885, -6.5771, -6.5819, -6.5900, -6.6051, -6.6214, -6.6355, -6.6267,
        -6.6163, -6.6071, -6.5890, -6.5600, -6.5136, -6.4789], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.0973, 0.0627, 0.0482, 0.0518, 0.1077, 0.0530, 0.0795, 0.1874, 0.0746,
        0.0953, 0.0445, 0.0980], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([30, 18,  8, 15, 32, 11, 28, 45, 19, 29, 16, 49]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_101.gif
Eval_AverageReturn : -298.79998779296875
Eval_StdReturn : 187.413330078125
Eval_MaxReturn : 76.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 244.0
Train_AverageReturn : -238.5
Train_StdReturn : 214.18788146972656
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 417792.0
TimeSinceStart : 11306.16759109497
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 93.60600852966309
Loss_Value : -0.07465958595275879
Loss_Entropy : 2.12011182308197
Loss_Representation : -15.83817195892334
Loss_KL : 1.3306087255477905
Loss_Obs : -1.48607137799263
Loss_Reward : -2.329785645008087
Loss_Discount : 0.02171824825927615
Loss_RawKL : 1.3306087255477905
mean_target : -6.7498496770858765
max_target : -6.492975473403931
min_target : -6.94809091091156
std_target : 0.14641545340418816
Done logging...

Current epsilon: 0.050005562665182336 at iteration 417792


********** Iteration 102 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1141, -0.1139, -0.1144, -0.1144, -0.1143, -0.1134, -0.1145, -0.1139,
        -0.1126, -0.1126, -0.1114, -0.1098, -0.1326, -0.1697, -0.1469],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.9612, -6.9329, -6.9033, -6.8742, -6.8495, -6.8190, -6.7805, -6.7484,
        -6.7094, -6.6724, -6.6118, -6.5508, -6.4860, -6.4046], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.9612, -6.9329, -6.9033, -6.8742, -6.8495, -6.8190, -6.7805, -6.7484,
        -6.7094, -6.6724, -6.6118, -6.5508, -6.4860, -6.4046], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1120, -0.1122, -0.1130, -0.1135, -0.1078, -0.1054, -0.1249, -0.1916,
        -0.1190, -0.1093, -0.1115, -0.1132, -0.1114, -0.1143, -0.1134],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2880, -7.2804, -7.2536, -7.2406, -7.2279, -7.2019, -7.1638, -7.1204,
        -6.9864, -6.9487, -6.8946, -6.8424, -6.7837, -6.7256], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2880, -7.2804, -7.2536, -7.2406, -7.2279, -7.2019, -7.1638, -7.1204,
        -6.9864, -6.9487, -6.8946, -6.8424, -6.7837, -6.7256], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0878, -0.0873, -0.0875, -0.0889, -0.0874, -0.0862, -0.0832, -0.0836,
        -0.1573, -0.1758, -0.0966, -0.0833, -0.0840, -0.0868, -0.0868],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2415, -7.2582, -7.2719, -7.2401, -7.2757, -7.2637, -7.2583, -7.2524,
        -7.2316, -7.1227, -6.9712, -6.9468, -6.9141, -6.8897], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2415, -7.2582, -7.2719, -7.2401, -7.2757, -7.2637, -7.2583, -7.2524,
        -7.2316, -7.1227, -6.9712, -6.9468, -6.9141, -6.8897], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0870, -0.0878, -0.0870, -0.0875, -0.0874, -0.0885, -0.0883, -0.0865,
        -0.0865, -0.0865, -0.1103, -0.0967, -0.1420, -0.0963, -0.0865],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.8754, -6.8797, -6.8693, -6.8529, -6.8476, -6.8425, -6.8261, -6.8057,
        -6.7854, -6.7456, -6.7111, -6.6494, -6.6005, -6.5197], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.8754, -6.8797, -6.8693, -6.8529, -6.8476, -6.8425, -6.8261, -6.8057,
        -6.7854, -6.7456, -6.7111, -6.6494, -6.6005, -6.5197], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1218, 0.0638, 0.0514, 0.0525, 0.1163, 0.0629, 0.0783, 0.1305, 0.0832,
        0.0907, 0.0493, 0.0994], device='cuda:0')
Count of actions: (array([7]), array([20]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_102.gif
Eval_AverageReturn : -353.0
Eval_StdReturn : 116.04309844970703
Eval_MaxReturn : -5.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 278.1000061035156
Train_AverageReturn : -299.0
Train_StdReturn : 186.3065185546875
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 421888.0
TimeSinceStart : 11423.716202020645
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 95.25863265991211
Loss_Value : -0.0471506267786026
Loss_Entropy : 2.1925140023231506
Loss_Representation : -15.617820262908936
Loss_KL : 1.2652659118175507
Loss_Obs : -1.44967383146286
Loss_Reward : -2.40480774641037
Loss_Discount : 0.018460272811353207
Loss_RawKL : 1.2652658820152283
mean_target : -6.870032429695129
max_target : -6.607213735580444
min_target : -7.072881579399109
std_target : 0.1490762196481228
Done logging...

Current epsilon: 0.05000494832319805 at iteration 421888


********** Iteration 103 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1134, -0.1137, -0.1142, -0.1130, -0.1129, -0.1132, -0.1121, -0.1135,
        -0.1124, -0.1422, -0.2335, -0.1140, -0.1158, -0.1139, -0.1118],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.6059, -7.5994, -7.5887, -7.5728, -7.5457, -7.5154, -7.4809, -7.4394,
        -7.3908, -7.3362, -7.2525, -7.0719, -7.0161, -6.9407], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.6059, -7.5994, -7.5887, -7.5728, -7.5457, -7.5154, -7.4809, -7.4394,
        -7.3908, -7.3362, -7.2525, -7.0719, -7.0161, -6.9407], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1126, -0.1125, -0.1132, -0.1133, -0.1137, -0.1127, -0.1124, -0.1127,
        -0.1116, -0.1119, -0.1124, -0.1124, -0.1120, -0.1120, -0.1153],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0171, -6.9900, -6.9525, -6.9083, -6.8741, -6.8441, -6.7969, -6.7611,
        -6.7153, -6.6619, -6.6031, -6.5572, -6.4924, -6.4304], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0171, -6.9900, -6.9525, -6.9083, -6.8741, -6.8441, -6.7969, -6.7611,
        -6.7153, -6.6619, -6.6031, -6.5572, -6.4924, -6.4304], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0865, -0.0866, -0.0864, -0.0903, -0.0898, -0.0942, -0.0873, -0.0887,
        -0.0951, -0.0891, -0.0979, -0.0914, -0.0999, -0.1142, -0.1246],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.9682, -6.9509, -6.9259, -6.8975, -6.8688, -6.8367, -6.7987, -6.7706,
        -6.7371, -6.6935, -6.6537, -6.5956, -6.5370, -6.4739], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.9682, -6.9509, -6.9259, -6.8975, -6.8688, -6.8367, -6.7987, -6.7706,
        -6.7371, -6.6935, -6.6537, -6.5956, -6.5370, -6.4739], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1308, -0.1151, -0.0932, -0.0895, -0.0889, -0.0872, -0.0861, -0.0854,
        -0.0854, -0.0870, -0.0873, -0.0872, -0.0879, -0.0878, -0.0935],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0490, -7.0127, -7.0027, -7.0318, -7.0371, -7.0474, -7.0474, -7.0403,
        -7.0284, -7.0202, -7.0051, -6.9924, -6.9689, -6.9461], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0490, -7.0127, -7.0027, -7.0318, -7.0371, -7.0474, -7.0474, -7.0403,
        -7.0284, -7.0202, -7.0051, -6.9924, -6.9689, -6.9461], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1307, 0.0618, 0.0508, 0.0498, 0.1162, 0.0674, 0.0702, 0.1395, 0.0849,
        0.0851, 0.0503, 0.0934], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([39, 25, 11, 14, 28, 20, 12, 43, 30, 29, 18, 31]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_103.gif
Eval_AverageReturn : -253.10000610351562
Eval_StdReturn : 211.5364990234375
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 217.39999389648438
Train_AverageReturn : -255.84210205078125
Train_StdReturn : 211.65570068359375
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 425984.0
TimeSinceStart : 11538.368191242218
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 96.57891464233398
Loss_Value : -0.0589936301112175
Loss_Entropy : 2.148567795753479
Loss_Representation : -15.775889158248901
Loss_KL : 1.2490196526050568
Loss_Obs : -1.4655034840106964
Loss_Reward : -2.3959367871284485
Loss_Discount : 0.026063154451549053
Loss_RawKL : 1.2490196228027344
mean_target : -6.963018536567688
max_target : -6.683328628540039
min_target : -7.180494904518127
std_target : 0.1597505621612072
Done logging...

Current epsilon: 0.05000440182927963 at iteration 425984


********** Iteration 104 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1128, -0.1131, -0.1112, -0.1120, -0.1129, -0.1134, -0.1128, -0.1117,
        -0.1473, -0.2002, -0.1318, -0.1147, -0.1128, -0.1134, -0.1128],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.1302, -7.1253, -7.1255, -7.1035, -7.0876, -7.0641, -7.0298, -6.9867,
        -6.9493, -6.8770, -6.7320, -6.6584, -6.6055, -6.5452], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.1302, -7.1253, -7.1255, -7.1035, -7.0876, -7.0641, -7.0298, -6.9867,
        -6.9493, -6.8770, -6.7320, -6.6584, -6.6055, -6.5452], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1088, -0.1109, -0.1089, -0.1114, -0.1111, -0.1110, -0.1117, -0.1105,
        -0.1085, -0.1101, -0.1244, -0.1904, -0.1360, -0.1123, -0.1116],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.5569, -7.5617, -7.5620, -7.5366, -7.5334, -7.4776, -7.4610, -7.4385,
        -7.3876, -7.3379, -7.2786, -7.2038, -7.0616, -6.9771], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.5569, -7.5617, -7.5620, -7.5366, -7.5334, -7.4776, -7.4610, -7.4385,
        -7.3876, -7.3379, -7.2786, -7.2038, -7.0616, -6.9771], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0867, -0.0872, -0.0870, -0.0864, -0.0867, -0.0869, -0.0869, -0.0853,
        -0.0853, -0.0829, -0.1180, -0.1979, -0.0905, -0.0979, -0.0900],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.6191, -6.6344, -6.6448, -6.6333, -6.6282, -6.6313, -6.6183, -6.6013,
        -6.5682, -6.5437, -6.5315, -6.4758, -6.3622, -6.3418], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.6191, -6.6344, -6.6448, -6.6333, -6.6282, -6.6313, -6.6183, -6.6013,
        -6.5682, -6.5437, -6.5315, -6.4758, -6.3622, -6.3418], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1615, -0.1937, -0.1132, -0.0900, -0.0900, -0.0886, -0.0891, -0.0892,
        -0.0895, -0.0888, -0.0893, -0.0883, -0.0882, -0.0883, -0.0889],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.9638, -6.8923, -6.7737, -6.7532, -6.7370, -6.7267, -6.7203, -6.7091,
        -6.7032, -6.6982, -6.6827, -6.6643, -6.6388, -6.6084], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.9638, -6.8923, -6.7737, -6.7532, -6.7370, -6.7267, -6.7203, -6.7091,
        -6.7032, -6.6982, -6.6827, -6.6643, -6.6388, -6.6084], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1341, 0.0605, 0.0500, 0.0474, 0.1162, 0.0711, 0.0627, 0.1489, 0.0860,
        0.0824, 0.0511, 0.0895], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([49, 14, 10, 12, 40, 25, 17, 38, 36, 22, 17, 20]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_104.gif
Eval_AverageReturn : -259.3999938964844
Eval_StdReturn : 204.02020263671875
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 222.6999969482422
Train_AverageReturn : -322.4375
Train_StdReturn : 159.45138549804688
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 430080.0
TimeSinceStart : 11653.6036298275
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 97.49137878417969
Loss_Value : -0.09376240335404873
Loss_Entropy : 2.1230189204216003
Loss_Representation : -15.786871910095215
Loss_KL : 1.202461153268814
Loss_Obs : -1.4598043262958527
Loss_Reward : -2.4152923226356506
Loss_Discount : 0.024002264253795147
Loss_RawKL : 1.202461153268814
mean_target : -7.027428984642029
max_target : -6.765584707260132
min_target : -7.226937413215637
std_target : 0.14842666313052177
Done logging...

Current epsilon: 0.05000391569027152 at iteration 430080


********** Iteration 105 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1135, -0.1127, -0.1111, -0.1136, -0.1129, -0.1132, -0.1141, -0.1131,
        -0.1142, -0.1122, -0.1120, -0.1112, -0.1109, -0.1206, -0.1914],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.6703, -7.6892, -7.7113, -7.6857, -7.6594, -7.6343, -7.6069, -7.5684,
        -7.5283, -7.4863, -7.4372, -7.3781, -7.3228, -7.2657], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.6703, -7.6892, -7.7113, -7.6857, -7.6594, -7.6343, -7.6069, -7.5684,
        -7.5283, -7.4863, -7.4372, -7.3781, -7.3228, -7.2657], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1566, -0.1713, -0.1387, -0.1097, -0.1111, -0.1109, -0.1108, -0.1107,
        -0.1115, -0.1116, -0.1114, -0.1115, -0.1110, -0.1109, -0.1110],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.5579, -7.4875, -7.4124, -7.3590, -7.3263, -7.2846, -7.2441, -7.2068,
        -7.1684, -7.1305, -7.0862, -7.0369, -6.9870, -6.9311], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.5579, -7.4875, -7.4124, -7.3590, -7.3263, -7.2846, -7.2441, -7.2068,
        -7.1684, -7.1305, -7.0862, -7.0369, -6.9870, -6.9311], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1587, -0.2031, -0.1490, -0.0870, -0.0875, -0.0864, -0.0859, -0.0862,
        -0.0858, -0.0852, -0.0860, -0.0865, -0.0860, -0.0865, -0.0862],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.1678, -7.0939, -6.9686, -6.9069, -6.9042, -6.8826, -6.8599, -6.8494,
        -6.8480, -6.8452, -6.8265, -6.8105, -6.8036, -6.7839], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.1678, -7.0939, -6.9686, -6.9069, -6.9042, -6.8826, -6.8599, -6.8494,
        -6.8480, -6.8452, -6.8265, -6.8105, -6.8036, -6.7839], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0876, -0.0870, -0.0875, -0.0865, -0.0856, -0.0881, -0.0875, -0.1081,
        -0.1917, -0.1125, -0.0857, -0.0878, -0.0873, -0.0872, -0.0869],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.1843, -7.1955, -7.1913, -7.1985, -7.1970, -7.1857, -7.1771, -7.1630,
        -7.1181, -6.9885, -6.9608, -6.9456, -6.9519, -6.9186], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.1843, -7.1955, -7.1913, -7.1985, -7.1970, -7.1857, -7.1771, -7.1630,
        -7.1181, -6.9885, -6.9608, -6.9456, -6.9519, -6.9186], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1494, 0.0622, 0.0522, 0.0470, 0.1222, 0.0830, 0.0591, 0.1062, 0.0921,
        0.0805, 0.0566, 0.0895], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([41, 12, 25, 12, 30, 18, 15, 41, 28, 22, 17, 39]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_105.gif
Eval_AverageReturn : -343.3999938964844
Eval_StdReturn : 139.81788635253906
Eval_MaxReturn : 76.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 272.0
Train_AverageReturn : -321.5
Train_StdReturn : 159.36985778808594
Train_MaxReturn : 95.0
Train_MinReturn : -395.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 434176.0
TimeSinceStart : 11775.584932565689
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 98.11734199523926
Loss_Value : -0.1832682453095913
Loss_Entropy : 2.196876049041748
Loss_Representation : -15.621028900146484
Loss_KL : 1.2262715697288513
Loss_Obs : -1.4433892369270325
Loss_Reward : -2.436686933040619
Loss_Discount : 0.023278321139514446
Loss_RawKL : 1.2262715697288513
mean_target : -7.074374556541443
max_target : -6.823094725608826
min_target : -7.263594031333923
std_target : 0.14097172021865845
Done logging...

Current epsilon: 0.05000348324056398 at iteration 434176


********** Iteration 106 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1209, -0.1173, -0.1116, -0.1112, -0.1119, -0.1103, -0.1123, -0.1130,
        -0.1133, -0.1265, -0.2272, -0.1172, -0.1123, -0.1120, -0.1114],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-6.4922, -6.4733, -6.5401, -6.5927, -6.5974, -6.6204, -6.6127, -6.5883,
        -6.5920, -6.5228, -6.5477, -6.4337, -6.4404, -6.4112], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-6.4922, -6.4733, -6.5401, -6.5927, -6.5974, -6.6204, -6.6127, -6.5883,
        -6.5920, -6.5228, -6.5477, -6.4337, -6.4404, -6.4112], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1110, -0.1108, -0.1103, -0.1109, -0.1102, -0.1110, -0.1118, -0.1373,
        -0.2138, -0.1200, -0.1105, -0.1111, -0.1103, -0.1104, -0.1101],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.3245, -7.3023, -7.2830, -7.2550, -7.2167, -7.1771, -7.1243, -7.0757,
        -7.0001, -6.8422, -6.8039, -6.7485, -6.6839, -6.6323], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.3245, -7.3023, -7.2830, -7.2550, -7.2167, -7.1771, -7.1243, -7.0757,
        -7.0001, -6.8422, -6.8039, -6.7485, -6.6839, -6.6323], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0869, -0.0857, -0.0865, -0.0864, -0.0859, -0.0861, -0.0868, -0.0857,
        -0.0872, -0.1841, -0.2179, -0.0933, -0.0861, -0.0863, -0.0862],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0329, -7.0329, -7.0215, -7.0271, -7.0216, -7.0089, -6.9889, -6.9648,
        -6.9349, -6.8936, -6.7668, -6.6184, -6.6014, -6.5704], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0329, -7.0329, -7.0215, -7.0271, -7.0216, -7.0089, -6.9889, -6.9648,
        -6.9349, -6.8936, -6.7668, -6.6184, -6.6014, -6.5704], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0899, -0.0873, -0.0883, -0.0863, -0.0875, -0.0879, -0.1106, -0.1439,
        -0.1238, -0.1183, -0.0882, -0.0882, -0.0926, -0.0869, -0.0872],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.1746, -7.1666, -7.1537, -7.1413, -7.1286, -7.1121, -7.0796, -7.0391,
        -6.9520, -6.8806, -6.8168, -6.7866, -6.7348, -6.6779], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.1746, -7.1666, -7.1537, -7.1413, -7.1286, -7.1121, -7.0796, -7.0391,
        -6.9520, -6.8806, -6.8168, -6.7866, -6.7348, -6.6779], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1398, 0.0586, 0.0491, 0.0424, 0.1131, 0.0844, 0.0521, 0.1641, 0.0857,
        0.0744, 0.0556, 0.0806], device='cuda:0')
Count of actions: (array([7]), array([6]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_106.gif
Eval_AverageReturn : -342.5
Eval_StdReturn : 145.88095092773438
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 270.6000061035156
Train_AverageReturn : -236.0
Train_StdReturn : 212.63113403320312
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 204.8000030517578
Train_EnvstepsSoFar : 438272.0
TimeSinceStart : 11891.765048742294
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 98.88166999816895
Loss_Value : -0.13023898005485535
Loss_Entropy : 2.0683541893959045
Loss_Representation : -15.807886600494385
Loss_KL : 1.2355467975139618
Loss_Obs : -1.474656492471695
Loss_Reward : -2.3279589414596558
Loss_Discount : 0.03109024278819561
Loss_RawKL : 1.2355467975139618
mean_target : -7.125108122825623
max_target : -6.8751442432403564
min_target : -7.314823865890503
std_target : 0.14159866981208324
Done logging...

Current epsilon: 0.05000309855069866 at iteration 438272


********** Iteration 107 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1115, -0.1124, -0.1131, -0.1117, -0.1120, -0.1124, -0.1127, -0.1120,
        -0.1115, -0.1107, -0.1107, -0.1426, -0.1815, -0.1321, -0.1193],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2584, -7.2448, -7.2176, -7.1922, -7.1574, -7.1249, -7.0850, -7.0687,
        -7.0362, -6.9834, -6.9288, -6.8599, -6.7731, -6.6458], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2584, -7.2448, -7.2176, -7.1922, -7.1574, -7.1249, -7.0850, -7.0687,
        -7.0362, -6.9834, -6.9288, -6.8599, -6.7731, -6.6458], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1107, -0.1117, -0.1115, -0.1112, -0.1100, -0.1111, -0.1102, -0.1124,
        -0.1119, -0.1121, -0.1098, -0.1094, -0.1100, -0.1078, -0.1192],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2529, -7.2487, -7.2630, -7.2573, -7.2533, -7.2371, -7.2148, -7.1922,
        -7.1784, -7.1564, -7.1205, -7.0786, -7.0407, -6.9802], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2529, -7.2487, -7.2630, -7.2573, -7.2533, -7.2371, -7.2148, -7.1922,
        -7.1784, -7.1564, -7.1205, -7.0786, -7.0407, -6.9802], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0860, -0.0867, -0.0866, -0.0863, -0.0868, -0.0872, -0.0868, -0.0867,
        -0.0860, -0.0866, -0.0849, -0.0841, -0.1139, -0.1005, -0.1311],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0127, -6.9957, -6.9719, -6.9693, -6.9821, -6.9800, -6.9908, -6.9773,
        -6.9768, -6.9489, -6.9241, -6.9008, -6.8789, -6.8207], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0127, -6.9957, -6.9719, -6.9693, -6.9821, -6.9800, -6.9908, -6.9773,
        -6.9768, -6.9489, -6.9241, -6.9008, -6.8789, -6.8207], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0879, -0.0880, -0.0876, -0.0875, -0.0875, -0.0869, -0.0858, -0.0931,
        -0.1711, -0.1330, -0.0996, -0.0911, -0.0878, -0.0873, -0.0874],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.5669, -7.5661, -7.5496, -7.5277, -7.5045, -7.4794, -7.4491, -7.4164,
        -7.3831, -7.2701, -7.1894, -7.1444, -7.1036, -7.0688], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.5669, -7.5661, -7.5496, -7.5277, -7.5045, -7.4794, -7.4491, -7.4164,
        -7.3831, -7.2701, -7.1894, -7.1444, -7.1036, -7.0688], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1452, 0.0591, 0.0500, 0.0406, 0.1134, 0.0954, 0.0489, 0.1510, 0.0864,
        0.0717, 0.0599, 0.0784], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([50, 15, 16, 15, 35, 35, 12, 44, 24, 22, 16, 16]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_107.gif
Eval_AverageReturn : -296.8999938964844
Eval_StdReturn : 191.27334594726562
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 242.60000610351562
Train_AverageReturn : -254.7894744873047
Train_StdReturn : 212.45497131347656
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 442368.0
TimeSinceStart : 12009.674050569534
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 99.19341850280762
Loss_Value : -0.17552524560596794
Loss_Entropy : 2.0791890025138855
Loss_Representation : -15.987182140350342
Loss_KL : 1.2219525277614594
Loss_Obs : -1.4809992611408234
Loss_Reward : -2.42524653673172
Loss_Discount : 0.026104455813765526
Loss_RawKL : 1.2219525277614594
mean_target : -7.147680640220642
max_target : -6.921463966369629
min_target : -7.319127678871155
std_target : 0.12760681100189686
Done logging...

Current epsilon: 0.05000275634606793 at iteration 442368


********** Iteration 108 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1121, -0.1112, -0.1122, -0.1094, -0.1104, -0.1105, -0.1252, -0.1433,
        -0.1118, -0.1127, -0.1133, -0.1122, -0.1120, -0.1116, -0.1119],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2241, -7.2059, -7.1724, -7.1566, -7.1245, -7.0961, -7.0690, -7.0545,
        -6.9938, -6.9669, -6.9461, -6.9075, -6.8830, -6.8158], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2241, -7.2059, -7.1724, -7.1566, -7.1245, -7.0961, -7.0690, -7.0545,
        -6.9938, -6.9669, -6.9461, -6.9075, -6.8830, -6.8158], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1100, -0.1099, -0.1100, -0.1100, -0.1097, -0.1098, -0.1097, -0.1099,
        -0.1100, -0.1109, -0.1103, -0.1117, -0.1171, -0.2310, -0.1576],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2338, -7.2513, -7.2731, -7.3073, -7.2867, -7.2854, -7.2786, -7.2743,
        -7.2597, -7.2309, -7.1990, -7.1566, -7.1000, -7.0362], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2338, -7.2513, -7.2731, -7.3073, -7.2867, -7.2854, -7.2786, -7.2743,
        -7.2597, -7.2309, -7.1990, -7.1566, -7.1000, -7.0362], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0856, -0.0858, -0.0858, -0.0891, -0.0868, -0.0916, -0.1888, -0.0969,
        -0.0918, -0.0907, -0.0865, -0.0859, -0.0852, -0.0853, -0.0849],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.6787, -7.6865, -7.6789, -7.6639, -7.6405, -7.6088, -7.5760, -7.4488,
        -7.4177, -7.3761, -7.3313, -7.3019, -7.2678, -7.2301], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.6787, -7.6865, -7.6789, -7.6639, -7.6405, -7.6088, -7.5760, -7.4488,
        -7.4177, -7.3761, -7.3313, -7.3019, -7.2678, -7.2301], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0879, -0.0874, -0.0875, -0.0878, -0.0865, -0.1002, -0.1939, -0.0927,
        -0.0875, -0.0884, -0.0873, -0.0870, -0.0865, -0.0872, -0.0870],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.1978, -7.2046, -7.1826, -7.1746, -7.1568, -7.1379, -7.1073, -6.9842,
        -7.0156, -7.0049, -7.0036, -6.9572, -6.9466, -6.9096], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.1978, -7.2046, -7.1826, -7.1746, -7.1568, -7.1379, -7.1073, -6.9842,
        -7.0156, -7.0049, -7.0036, -6.9572, -6.9466, -6.9096], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1487, 0.0616, 0.0517, 0.0401, 0.1157, 0.1044, 0.0480, 0.1253, 0.0875,
        0.0741, 0.0638, 0.0793], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([42, 18, 18, 13, 42, 30, 14, 40, 32, 16, 22, 13]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_108.gif
Eval_AverageReturn : -343.0
Eval_StdReturn : 146.01712036132812
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 270.6000061035156
Train_AverageReturn : -254.0
Train_StdReturn : 209.56773376464844
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 446464.0
TimeSinceStart : 12131.78209900856
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 100.49534797668457
Loss_Value : -0.13114092126488686
Loss_Entropy : 2.1422561407089233
Loss_Representation : -15.93107008934021
Loss_KL : 1.211237907409668
Loss_Obs : -1.4666977226734161
Loss_Reward : -2.4952697157859802
Loss_Discount : 0.01993958093225956
Loss_RawKL : 1.211237907409668
mean_target : -7.242589592933655
max_target : -7.024475336074829
min_target : -7.409014582633972
std_target : 0.123306380584836
Done logging...

Current epsilon: 0.05000245193459299 at iteration 446464


********** Iteration 109 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1115, -0.1118, -0.1120, -0.1122, -0.1113, -0.1112, -0.1121, -0.1125,
        -0.1115, -0.1105, -0.1103, -0.1084, -0.2206, -0.1784, -0.1150],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2936, -7.2849, -7.2697, -7.2533, -7.2279, -7.2126, -7.1770, -7.1425,
        -7.1064, -7.0593, -6.9976, -6.9326, -6.8711, -6.7035], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2936, -7.2849, -7.2697, -7.2533, -7.2279, -7.2126, -7.1770, -7.1425,
        -7.1064, -7.0593, -6.9976, -6.9326, -6.8711, -6.7035], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1756, -0.1324, -0.1190, -0.1094, -0.1072, -0.1096, -0.1092, -0.1099,
        -0.1094, -0.1094, -0.1093, -0.1102, -0.1087, -0.1087, -0.1160],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.6806, -7.6124, -7.5895, -7.5569, -7.5310, -7.5117, -7.4906, -7.4552,
        -7.4171, -7.3688, -7.3155, -7.2588, -7.2024, -7.1375], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.6806, -7.6124, -7.5895, -7.5569, -7.5310, -7.5117, -7.4906, -7.4552,
        -7.4171, -7.3688, -7.3155, -7.2588, -7.2024, -7.1375], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1623, -0.2085, -0.1105, -0.0859, -0.0849, -0.0860, -0.0880, -0.0857,
        -0.0869, -0.0859, -0.0862, -0.0856, -0.0866, -0.0865, -0.0856],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.6036, -7.5413, -7.4235, -7.4187, -7.4094, -7.3957, -7.3741, -7.3521,
        -7.3353, -7.3212, -7.3003, -7.2785, -7.2409, -7.2054], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.6036, -7.5413, -7.4235, -7.4187, -7.4094, -7.3957, -7.3741, -7.3521,
        -7.3353, -7.3212, -7.3003, -7.2785, -7.2409, -7.2054], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0871, -0.0871, -0.0875, -0.0871, -0.0869, -0.0866, -0.0875, -0.0865,
        -0.0876, -0.0872, -0.0867, -0.0874, -0.0873, -0.0874, -0.0880],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2950, -7.3090, -7.3386, -7.3450, -7.3410, -7.3350, -7.3203, -7.3041,
        -7.2753, -7.2524, -7.2269, -7.1992, -7.1662, -7.1453], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2950, -7.3090, -7.3386, -7.3450, -7.3410, -7.3350, -7.3203, -7.3041,
        -7.2753, -7.2524, -7.2269, -7.1992, -7.1662, -7.1453], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1461, 0.0593, 0.0507, 0.0365, 0.1090, 0.1087, 0.0438, 0.1568, 0.0820,
        0.0682, 0.0650, 0.0738], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([46, 11, 17,  8, 24, 18, 15, 57, 31, 27, 23, 23]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_109.gif
Eval_AverageReturn : -243.5
Eval_StdReturn : 221.61961364746094
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 211.8000030517578
Train_AverageReturn : -296.6470642089844
Train_StdReturn : 178.5526580810547
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 450560.0
TimeSinceStart : 12245.026314735413
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 100.98513793945312
Loss_Value : -0.147390010766685
Loss_Entropy : 2.0546284914016724
Loss_Representation : -16.010560989379883
Loss_KL : 1.2205406427383423
Loss_Obs : -1.4814702570438385
Loss_Reward : -2.445877194404602
Loss_Discount : 0.02947862958535552
Loss_RawKL : 1.2205406427383423
mean_target : -7.274962782859802
max_target : -7.04994797706604
min_target : -7.447804570198059
std_target : 0.12750483117997646
Done logging...

Current epsilon: 0.05000218114238928 at iteration 450560


********** Iteration 110 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1311, -0.1434, -0.1347, -0.1716, -0.1246, -0.1109, -0.1110, -0.1109,
        -0.1110, -0.1104, -0.1092, -0.1107, -0.1101, -0.1104, -0.1113],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.2508, -7.2405, -7.1941, -7.1562, -7.0853, -7.0897, -7.0804, -7.0596,
        -7.0507, -7.0399, -7.0275, -7.0267, -7.0256, -6.9930], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.2508, -7.2405, -7.1941, -7.1562, -7.0853, -7.0897, -7.0804, -7.0596,
        -7.0507, -7.0399, -7.0275, -7.0267, -7.0256, -6.9930], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1667, -0.1780, -0.1871, -0.1140, -0.1114, -0.1098, -0.1096, -0.1090,
        -0.1101, -0.1094, -0.1086, -0.1082, -0.1094, -0.1091, -0.1092],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.3316, -7.3024, -7.2384, -7.2187, -7.3230, -7.3083, -7.3114, -7.2953,
        -7.2864, -7.2585, -7.2429, -7.2187, -7.1827, -7.1425], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.3316, -7.3024, -7.2384, -7.2187, -7.3230, -7.3083, -7.3114, -7.2953,
        -7.2864, -7.2585, -7.2429, -7.2187, -7.1827, -7.1425], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0867, -0.0858, -0.0860, -0.0856, -0.0856, -0.0847, -0.0871, -0.0868,
        -0.0856, -0.0920, -0.2203, -0.1349, -0.0874, -0.0854, -0.0861],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.9847, -7.9921, -7.9838, -7.9773, -7.9668, -7.9548, -7.9366, -7.9379,
        -7.9116, -7.9147, -7.8730, -7.7033, -7.6369, -7.6124], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.9847, -7.9921, -7.9838, -7.9773, -7.9668, -7.9548, -7.9366, -7.9379,
        -7.9116, -7.9147, -7.8730, -7.7033, -7.6369, -7.6124], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0872, -0.0873, -0.0881, -0.0881, -0.0874, -0.0869, -0.0870, -0.0878,
        -0.1864, -0.1282, -0.0897, -0.0875, -0.0889, -0.0874, -0.0867],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.6684, -7.6834, -7.6860, -7.6790, -7.7209, -7.7214, -7.7116, -7.7044,
        -7.6833, -7.5479, -7.4482, -7.4284, -7.4142, -7.3781], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.6684, -7.6834, -7.6860, -7.6790, -7.7209, -7.7214, -7.7116, -7.7044,
        -7.6833, -7.5479, -7.4482, -7.4284, -7.4142, -7.3781], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1504, 0.0591, 0.0517, 0.0339, 0.1081, 0.1174, 0.0411, 0.1527, 0.0803,
        0.0646, 0.0690, 0.0717], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([47, 21, 15, 14, 39, 41, 11, 39, 20, 17, 23, 13]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_110.gif
Eval_AverageReturn : -299.70001220703125
Eval_StdReturn : 183.56690979003906
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 245.39999389648438
Train_AverageReturn : -295.4705810546875
Train_StdReturn : 177.3242645263672
Train_MaxReturn : 76.0
Train_MinReturn : -400.0
Train_AverageEpLen : 240.94117736816406
Train_EnvstepsSoFar : 454656.0
TimeSinceStart : 12363.781598806381
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 102.24099922180176
Loss_Value : -0.14934335416182876
Loss_Entropy : 2.0440143644809723
Loss_Representation : -15.974730491638184
Loss_KL : 1.231643259525299
Loss_Obs : -1.4806338846683502
Loss_Reward : -2.4254942536354065
Loss_Discount : 0.02545957639813423
Loss_RawKL : 1.231643259525299
mean_target : -7.364348649978638
max_target : -7.141828894615173
min_target : -7.53271746635437
std_target : 0.12449953332543373
Done logging...

Current epsilon: 0.050001940256536995 at iteration 454656


********** Iteration 111 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1107, -0.1098, -0.1107, -0.1105, -0.1115, -0.1099, -0.1109, -0.1102,
        -0.1321, -0.1911, -0.1662, -0.1197, -0.1106, -0.1097, -0.1104],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.0844, -7.0547, -7.0473, -7.0281, -7.0016, -6.9754, -6.9424, -6.8973,
        -6.8351, -6.7560, -6.6454, -6.5374, -6.5006, -6.4868], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.0844, -7.0547, -7.0473, -7.0281, -7.0016, -6.9754, -6.9424, -6.8973,
        -6.8351, -6.7560, -6.6454, -6.5374, -6.5006, -6.4868], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1103, -0.1083, -0.1083, -0.1084, -0.1097, -0.1103, -0.1081, -0.1094,
        -0.1090, -0.1085, -0.1423, -0.1781, -0.1130, -0.1086, -0.1083],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-8.2509, -8.2612, -8.2556, -8.2453, -8.2213, -8.1925, -8.1639, -8.1308,
        -8.0941, -8.0530, -8.0075, -7.9235, -7.8090, -7.7554], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-8.2509, -8.2612, -8.2556, -8.2453, -8.2213, -8.1925, -8.1639, -8.1308,
        -8.0941, -8.0530, -8.0075, -7.9235, -7.8090, -7.7554], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0857, -0.0854, -0.0853, -0.0858, -0.0852, -0.0858, -0.0859, -0.1617,
        -0.1875, -0.1283, -0.0904, -0.0849, -0.0853, -0.0856, -0.0863],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.3633, -7.3645, -7.3503, -7.3411, -7.3289, -7.3070, -7.2784, -7.2680,
        -7.1671, -7.0374, -6.9769, -6.9648, -6.9654, -6.9599], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.3633, -7.3645, -7.3503, -7.3411, -7.3289, -7.3070, -7.2784, -7.2680,
        -7.1671, -7.0374, -6.9769, -6.9648, -6.9654, -6.9599], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0867, -0.0865, -0.0871, -0.0874, -0.0865, -0.0865, -0.0868, -0.0853,
        -0.1093, -0.2396, -0.0929, -0.0872, -0.0872, -0.0867, -0.0867],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-5.7545, -5.7465, -5.7821, -5.8138, -5.8867, -5.8980, -5.9044, -5.9189,
        -5.8848, -5.8921, -5.8035, -5.9748, -6.0123, -6.1252], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-5.7545, -5.7465, -5.7821, -5.8138, -5.8867, -5.8980, -5.9044, -5.9189,
        -5.8848, -5.8921, -5.8035, -5.9748, -6.0123, -6.1252], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1383, 0.0617, 0.0530, 0.0336, 0.1100, 0.1106, 0.0414, 0.1593, 0.0800,
        0.0681, 0.0695, 0.0746], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([38, 18, 15,  9, 40, 27, 12, 36, 25, 31, 23, 26]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_111.gif
Eval_AverageReturn : -294.5
Eval_StdReturn : 194.800537109375
Eval_MaxReturn : 95.0
Eval_MinReturn : -400.0
Eval_AverageEpLen : 241.1999969482422
Train_AverageReturn : -255.3157958984375
Train_StdReturn : 206.56056213378906
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 215.57894897460938
Train_EnvstepsSoFar : 458752.0
TimeSinceStart : 12482.282133579254
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 101.86833953857422
Loss_Value : -0.1064858864992857
Loss_Entropy : 2.047840565443039
Loss_Representation : -15.847606420516968
Loss_KL : 1.264081597328186
Loss_Obs : -1.4691071510314941
Loss_Reward : -2.446210563182831
Loss_Discount : 0.025593900587409735
Loss_RawKL : 1.264081597328186
mean_target : -7.337811589241028
max_target : -7.1291749477386475
min_target : -7.506226897239685
std_target : 0.12038016319274902
Done logging...

Current epsilon: 0.050001725974172 at iteration 458752


********** Iteration 112 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1124, -0.1095, -0.1105, -0.1097, -0.1092, -0.1102, -0.1091, -0.1103,
        -0.1114, -0.1110, -0.1105, -0.1105, -0.1103, -0.1089, -0.1752],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.7477, -7.7514, -7.7347, -7.7059, -7.6699, -7.6430, -7.6229, -7.5891,
        -7.5533, -7.5216, -7.4764, -7.4265, -7.3795, -7.3170], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.7477, -7.7514, -7.7347, -7.7059, -7.6699, -7.6430, -7.6229, -7.5891,
        -7.5533, -7.5216, -7.4764, -7.4265, -7.3795, -7.3170], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1121, -0.1094, -0.1091, -0.1095, -0.1095, -0.1099, -0.1102, -0.1089,
        -0.1090, -0.1096, -0.1096, -0.1096, -0.1102, -0.1641, -0.1779],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.5699, -7.5529, -7.5225, -7.5030, -7.4855, -7.4667, -7.4560, -7.4379,
        -7.4025, -7.3647, -7.3376, -7.3169, -7.2715, -7.2282], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.5699, -7.5529, -7.5225, -7.5030, -7.4855, -7.4667, -7.4560, -7.4379,
        -7.4025, -7.3647, -7.3376, -7.3169, -7.2715, -7.2282], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0862, -0.0854, -0.0860, -0.0863, -0.0864, -0.0876, -0.0870, -0.0900,
        -0.1055, -0.1041, -0.1097, -0.0873, -0.0855, -0.0858, -0.0860],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.5511, -7.5313, -7.5137, -7.4947, -7.4733, -7.4462, -7.4281, -7.4087,
        -7.3921, -7.3664, -7.3456, -7.3086, -7.2954, -7.2672], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.5511, -7.5313, -7.5137, -7.4947, -7.4733, -7.4462, -7.4281, -7.4087,
        -7.3921, -7.3664, -7.3456, -7.3086, -7.2954, -7.2672], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0873, -0.0866, -0.0858, -0.0869, -0.0872, -0.0873, -0.0861, -0.0872,
        -0.0874, -0.0879, -0.0976, -0.1200, -0.1812, -0.1013, -0.0898],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.5994, -7.6012, -7.5775, -7.5617, -7.5414, -7.5172, -7.4899, -7.4569,
        -7.4283, -7.3979, -7.3693, -7.3164, -7.2384, -7.1009], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.5994, -7.6012, -7.5775, -7.5617, -7.5414, -7.5172, -7.4899, -7.4569,
        -7.4283, -7.3979, -7.3693, -7.3164, -7.2384, -7.1009], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1260, 0.0655, 0.0557, 0.0344, 0.1204, 0.1007, 0.0412, 0.1418, 0.0854,
        0.0756, 0.0698, 0.0835], device='cuda:0')
Count of actions: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), array([32, 19, 16,  7, 33, 21, 18, 62, 24, 23, 24, 21]))
Saved episode GIF to ./logs/tb_dreamerv2_basic/episode_112.gif
Eval_AverageReturn : -217.0
Eval_StdReturn : 215.52076721191406
Eval_MaxReturn : 95.0
Eval_MinReturn : -395.0
Eval_AverageEpLen : 197.39999389648438
Train_AverageReturn : -320.5625
Train_StdReturn : 161.01162719726562
Train_MaxReturn : 95.0
Train_MinReturn : -400.0
Train_AverageEpLen : 256.0
Train_EnvstepsSoFar : 462848.0
TimeSinceStart : 12593.897976636887
Initial_DataCollection_AverageReturn : -235.5
Loss_Policy : 102.26880073547363
Loss_Value : -0.07030102331191301
Loss_Entropy : 2.105984628200531
Loss_Representation : -16.01774001121521
Loss_KL : 1.2134290933609009
Loss_Obs : -1.4743033051490784
Loss_Reward : -2.51223623752594
Loss_Discount : 0.024100259877741337
Loss_RawKL : 1.2134290933609009
mean_target : -7.368166208267212
max_target : -7.173910617828369
min_target : -7.527454137802124
std_target : 0.11305425874888897
Done logging...

Current epsilon: 0.050001535357199214 at iteration 462848


********** Iteration 113 ************
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1102, -0.1339, -0.1152, -0.1084, -0.1085, -0.1089, -0.1090, -0.1092,
        -0.1074, -0.1088, -0.1083, -0.1087, -0.1082, -0.1089, -0.1066],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.3455, -7.3359, -7.3021, -7.3792, -7.3593, -7.3470, -7.3294, -7.3327,
        -7.3155, -7.2838, -7.2549, -7.2231, -7.1752, -7.1254], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.3455, -7.3359, -7.3021, -7.3792, -7.3593, -7.3470, -7.3294, -7.3327,
        -7.3155, -7.2838, -7.2549, -7.2231, -7.1752, -7.1254], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1098, -0.1089, -0.1089, -0.1095, -0.1094, -0.1092, -0.1088, -0.1084,
        -0.1088, -0.1243, -0.2283, -0.1199, -0.1303, -0.1120, -0.1098],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.3954, -7.4002, -7.3836, -7.3737, -7.3683, -7.3586, -7.3440, -7.3153,
        -7.3002, -7.2567, -7.2002, -7.0402, -6.9941, -6.9455], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.3954, -7.4002, -7.3836, -7.3737, -7.3683, -7.3586, -7.3440, -7.3153,
        -7.3002, -7.2567, -7.2002, -7.0402, -6.9941, -6.9455], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.1576, -0.1860, -0.0995, -0.0858, -0.0865, -0.0869, -0.0865, -0.0868,
        -0.0862, -0.0873, -0.0862, -0.0869, -0.0873, -0.0862, -0.0859],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.5975, -7.5519, -7.4470, -7.4356, -7.4166, -7.3885, -7.3526, -7.3257,
        -7.2965, -7.2631, -7.2321, -7.1972, -7.1591, -7.1139], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.5975, -7.5519, -7.4470, -7.4356, -7.4166, -7.3885, -7.3526, -7.3257,
        -7.2965, -7.2631, -7.2321, -7.1972, -7.1591, -7.1139], device='cuda:0',
       grad_fn=<SelectBackward0>)
[GIF] Guardado en reconstruction.gif
Passed this part
Imagined rewards:  tensor([-0.0869, -0.0882, -0.0872, -0.0877, -0.0882, -0.0854, -0.0928, -0.2293,
        -0.1032, -0.0876, -0.0872, -0.0880, -0.0875, -0.0882, -0.0871],
       device='cuda:0', grad_fn=<SelectBackward0>)
Loss total:  tensor([-7.7845, -7.7828, -7.7730, -7.7562, -7.7363, -7.7132, -7.6832, -7.6514,
        -7.4821, -7.4396, -7.4217, -7.3874, -7.3527, -7.3168], device='cuda:0',
       grad_fn=<SelectBackward0>)
Discounted loss total:  tensor([-7.7845, -7.7828, -7.7730, -7.7562, -7.7363, -7.7132, -7.6832, -7.6514,
        -7.4821, -7.4396, -7.4217, -7.3874, -7.3527, -7.3168], device='cuda:0',
       grad_fn=<SelectBackward0>)
Current action distribution:
tensor([0.1084, 0.0668, 0.0566, 0.0347, 0.1313, 0.0852, 0.0393, 0.1455, 0.0905,
        0.0823, 0.0664, 0.0930], device='cuda:0')
